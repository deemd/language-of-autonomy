{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73e24b0",
   "metadata": {},
   "source": [
    "# üè∑Ô∏è Step 5 : Mini-Taxonomy of Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7443fc47",
   "metadata": {},
   "source": [
    "**Objectif**: Extraire et classifier les d√©finitions d'\"Agentic AI\" dans le corpus\n",
    "\n",
    "**Approche hybride**:\n",
    "1. **Extraction semi-automatique** - Identifier les paragraphes d√©finitionnels\n",
    "2. **Classification manuelle** - Grouper en cat√©gories conceptuelles\n",
    "3. **Visualisation** - Tableaux et diagrammes (treemap, sunburst)\n",
    "\n",
    "**Cat√©gories attendues**:\n",
    "- AI as Copilots/Assistants\n",
    "- AI as Autonomous Workers\n",
    "- AI as Multi-Agent Ecosystems/Orchestrators\n",
    "- AI as Governance/Risk Challenges\n",
    "\n",
    "**Output**: Taxonomie des d√©finitions + visualisations pour le rapport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1153d79",
   "metadata": {},
   "source": [
    "## üîß Setup Config & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1dce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Viz\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Imports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b23fdd0",
   "metadata": {},
   "source": [
    "## üìÇ Load processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33168eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "PROCESSED_DATA = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "TEXTS_DIR = PROCESSED_DATA / \"texts\"\n",
    "METADATA_FILE = PROCESSED_DATA / \"metadata\" / \"corpus_metadata.json\"\n",
    "\n",
    "# Create taxonomy folder\n",
    "TAXONOMY_DIR = PROCESSED_DATA / \"taxonomy\"\n",
    "TAXONOMY_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Taxonomy folder : {TAXONOMY_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b3aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw texts\n",
    "texts = {}\n",
    "with open(METADATA_FILE, 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "for doc_id in metadata.keys():\n",
    "    text_file = TEXTS_DIR / f\"{doc_id}.txt\"\n",
    "    if text_file.exists():\n",
    "        with open(text_file, 'r', encoding='utf-8') as f:\n",
    "            texts[doc_id] = f.read()\n",
    "\n",
    "print(f\"‚úÖ {len(texts)} documents loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be341d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping doc_id -> source_type\n",
    "doc_to_source = {doc_id: metadata[doc_id]['source_type'] \n",
    "                 for doc_id in texts.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e8794d",
   "metadata": {},
   "source": [
    "## üîç Semi-automatic extraction of definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2144a128",
   "metadata": {},
   "source": [
    "### Extraction strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c375f8",
   "metadata": {},
   "source": [
    "Nous allons identifier les phrases/paragraphes qui:\n",
    "1. Contiennent des termes cl√©s: \"agentic ai\", \"ai agent\", \"autonomous agent\"\n",
    "2. Utilisent des marqueurs d√©finitionnels: \"is\", \"are\", \"defined as\", \"refers to\", \"means\"\n",
    "3. Sont dans les premi√®res sections (introduction, definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a765d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_definition_candidates(text, doc_id):\n",
    "    \"\"\"\n",
    "    Extract candidate phrases as definitions.\n",
    "    \n",
    "    Criterias:\n",
    "    - Contains key terms (agentic, agent, autonomous)\n",
    "    - Contains definitional markers\n",
    "    - Reasonable length (30-300 words)\n",
    "    \"\"\"\n",
    "    # Tokenize in phrases\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Key terms to search\n",
    "    key_terms = [\n",
    "        r'\\bagentic\\s+ai\\b',\n",
    "        r'\\bai\\s+agent[s]?\\b',\n",
    "        r'\\bautonomous\\s+agent[s]?\\b',\n",
    "        r'\\bagentic\\s+system[s]?\\b',\n",
    "        r'\\bagent[s]?\\s+are\\b',\n",
    "        r'\\bagent[s]?\\s+is\\b'\n",
    "    ]\n",
    "    \n",
    "    # Definitional markers\n",
    "    def_markers = [\n",
    "        r'\\bis\\s+defined\\s+as\\b',\n",
    "        r'\\bare\\s+defined\\s+as\\b',\n",
    "        r'\\brefers?\\s+to\\b',\n",
    "        r'\\bmeans?\\b',\n",
    "        r'\\bcan\\s+be\\s+understood\\s+as\\b',\n",
    "        r'\\bcharacterized\\s+by\\b',\n",
    "        r'\\bconsists?\\s+of\\b',\n",
    "        r'\\benables?\\b',\n",
    "        r'\\bcapable\\s+of\\b'\n",
    "    ]\n",
    "    \n",
    "    candidates = []\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_lower = sentence.lower()\n",
    "        \n",
    "        # Check key terms presence\n",
    "        has_key_term = any(re.search(pattern, sentence_lower) for pattern in key_terms)\n",
    "        \n",
    "        if not has_key_term:\n",
    "            continue\n",
    "        \n",
    "        # Check definitional markers presence\n",
    "        has_def_marker = any(re.search(pattern, sentence_lower) for pattern in def_markers)\n",
    "        \n",
    "        # Check length\n",
    "        word_count = len(sentence.split())\n",
    "        \n",
    "        # Score phrase\n",
    "        score = 0\n",
    "        if has_key_term:\n",
    "            score += 2\n",
    "        if has_def_marker:\n",
    "            score += 3\n",
    "        if 30 <= word_count <= 300:\n",
    "            score += 1\n",
    "        if i < len(sentences) * 0.3:  # In first 30%\n",
    "            score += 1\n",
    "        \n",
    "        if score >= 3:\n",
    "            # Extract next phrase for context\n",
    "            context = sentence\n",
    "            if i + 1 < len(sentences):\n",
    "                context += \" \" + sentences[i + 1]\n",
    "            \n",
    "            candidates.append({\n",
    "                'doc_id': doc_id,\n",
    "                'sentence_id': i,\n",
    "                'text': sentence.strip(),\n",
    "                'context': context.strip(),\n",
    "                'word_count': word_count,\n",
    "                'score': score,\n",
    "                'has_def_marker': has_def_marker\n",
    "            })\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7df743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract candidates for docs\n",
    "all_candidates = []\n",
    "\n",
    "for doc_id, text in texts.items():\n",
    "    candidates = extract_definition_candidates(text, doc_id)\n",
    "    all_candidates.extend(candidates)\n",
    "    \n",
    "    filename = metadata[doc_id]['filename']\n",
    "    print(f\"\\nüìÑ {filename}\")\n",
    "    print(f\"   {len(candidates)} potential definitions found\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total: {len(all_candidates)} candidate definitions extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97be345",
   "metadata": {},
   "source": [
    "### Extracted definitions overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0098389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df_candidates = pd.DataFrame(all_candidates)\n",
    "\n",
    "# Add metadata\n",
    "df_candidates['filename'] = df_candidates['doc_id'].map(\n",
    "    lambda x: metadata[x]['filename']\n",
    ")\n",
    "df_candidates['source_type'] = df_candidates['doc_id'].map(doc_to_source)\n",
    "\n",
    "# Desc score order\n",
    "df_candidates = df_candidates.sort_values('score', ascending=False)\n",
    "\n",
    "print(\"TOP 10 CANDIDATE DEFINITIONS (by score)\")\n",
    "for i, row in df_candidates.head(10).iterrows():\n",
    "    print(f\"\\n{row['filename']} (Score: {row['score']})\")\n",
    "    print(f\"  {row['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec7398",
   "metadata": {},
   "source": [
    "## üìù Manual classification of definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba4cca5",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Cette section n√©cessite une r√©vision manuelle.\n",
    "\n",
    "Pour chaque d√©finition candidate, tu dois:\n",
    "1. Lire le texte complet\n",
    "2. Assigner une cat√©gorie\n",
    "3. √âventuellement fusionner ou supprimer certaines entr√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define taxonomy categories\n",
    "TAXONOMY_CATEGORIES = {\n",
    "    'copilot': {\n",
    "        'label': 'AI as Copilots/Assistants',\n",
    "        'description': 'AI agents that augment human work, provide suggestions, collaborate with users',\n",
    "        'keywords': ['copilot', 'assistant', 'augment', 'support', 'collaborate', 'suggest', 'help']\n",
    "    },\n",
    "    'autonomous_worker': {\n",
    "        'label': 'AI as Autonomous Workers',\n",
    "        'description': 'AI agents that independently execute tasks with minimal human intervention',\n",
    "        'keywords': ['autonomous', 'independent', 'execute', 'automate', 'replace', 'perform']\n",
    "    },\n",
    "    'orchestrator': {\n",
    "        'label': 'AI as Multi-Agent Ecosystems/Orchestrators',\n",
    "        'description': 'AI systems coordinating multiple agents, workflows, or complex processes',\n",
    "        'keywords': ['orchestrate', 'coordinate', 'multi-agent', 'ecosystem', 'workflow', 'multi-step', 'planning']\n",
    "    },\n",
    "    'governance': {\n",
    "        'label': 'AI as Governance/Risk Challenges',\n",
    "        'description': 'AI agents framed through ethical, regulatory, or risk management lens',\n",
    "        'keywords': ['governance', 'risk', 'compliance', 'regulation', 'ethics', 'safety', 'alignment', 'control']\n",
    "    },\n",
    "    'other': {\n",
    "        'label': 'Other/Uncategorized',\n",
    "        'description': 'Definitions that don\\'t fit main categories',\n",
    "        'keywords': []\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nüìã Taxonomy categories:\")\n",
    "for cat_id, cat_info in TAXONOMY_CATEGORIES.items():\n",
    "    print(f\"\\n  {cat_info['label']}\")\n",
    "    print(f\"    {cat_info['description']}\")\n",
    "    print(f\"    Key words: {', '.join(cat_info['keywords'][:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ee3090",
   "metadata": {},
   "source": [
    "### Semi-automatic classification (First try)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95316131",
   "metadata": {},
   "source": [
    "On utilise les mots-cl√©s pour sugg√©rer une cat√©gorie, mais la validation manuelle sera n√©cessaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bf8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_category(text):\n",
    "    \"\"\"\n",
    "    Suggest a category based on key-words.\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    scores = {}\n",
    "    for cat_id, cat_info in TAXONOMY_CATEGORIES.items():\n",
    "        if cat_id == 'other':\n",
    "            continue\n",
    "        \n",
    "        score = sum(1 for keyword in cat_info['keywords'] if keyword in text_lower)\n",
    "        scores[cat_id] = score\n",
    "    \n",
    "    if max(scores.values()) == 0:\n",
    "        return 'other', 0\n",
    "    \n",
    "    suggested_cat = max(scores.items(), key=lambda x: x[1])[0]\n",
    "    confidence = scores[suggested_cat]\n",
    "    \n",
    "    return suggested_cat, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5753405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggest cats\n",
    "df_candidates['suggested_category'] = df_candidates.apply(\n",
    "    lambda row: suggest_category(row['text'])[0], axis=1\n",
    ")\n",
    "df_candidates['category_confidence'] = df_candidates.apply(\n",
    "    lambda row: suggest_category(row['text'])[1], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d8f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SUGGESTION FOR CLASSIFICATION\")\n",
    "for cat_id in ['copilot', 'autonomous_worker', 'orchestrator', 'governance', 'other']:\n",
    "    count = (df_candidates['suggested_category'] == cat_id).sum()\n",
    "    print(f\"  {TAXONOMY_CATEGORIES[cat_id]['label']:45} : {count:2} definitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c120175",
   "metadata": {},
   "source": [
    "### Export for manual classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# %%\n",
    "# Cr√©er un fichier CSV pour r√©vision manuelle\n",
    "review_df = df_candidates[[\n",
    "    'filename', 'source_type', 'text', 'context', \n",
    "    'suggested_category', 'category_confidence'\n",
    "]].copy()\n",
    "\n",
    "# Ajouter une colonne vide pour la cat√©gorie finale (√† remplir manuellement)\n",
    "review_df['final_category'] = review_df['suggested_category']\n",
    "review_df['notes'] = ''\n",
    "review_df['keep'] = True  # Pour marquer les d√©finitions √† garder\n",
    "\n",
    "# Sauvegarder\n",
    "review_file = TAXONOMY_DIR / 'definitions_for_manual_review.csv'\n",
    "review_df.to_csv(review_file, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nüíæ Fichier pour r√©vision manuelle: {review_file}\")\n",
    "print(\"\\n‚ö†Ô∏è  √âTAPE MANUELLE REQUISE:\")\n",
    "print(\"   1. Ouvrez le fichier CSV dans Excel/Google Sheets\")\n",
    "print(\"   2. Lisez chaque d√©finition\")\n",
    "print(\"   3. Corrigez la colonne 'final_category' si n√©cessaire\")\n",
    "print(\"   4. Mettez 'keep' √† False pour les d√©finitions non pertinentes\")\n",
    "print(\"   5. Ajoutez des notes si besoin\")\n",
    "print(\"   6. Sauvegardez le fichier\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üìä Chargement des R√©sultats de la Classification Manuelle\n",
    "# \n",
    "# **NOTE**: Apr√®s avoir compl√©t√© la r√©vision manuelle du CSV, ex√©cutez cette section.\n",
    "# \n",
    "# Si tu n'as pas encore fait la r√©vision, on continue avec la classification automatique.\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHARGEMENT DES R√âSULTATS MANUELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# V√©rifier si le fichier r√©vis√© existe\n",
    "reviewed_file = TAXONOMY_DIR / 'definitions_for_manual_review.csv'\n",
    "\n",
    "if reviewed_file.exists():\n",
    "    # Charger les r√©sultats r√©vis√©s\n",
    "    df_reviewed = pd.read_csv(reviewed_file)\n",
    "    \n",
    "    # Filtrer pour garder seulement les d√©finitions valid√©es\n",
    "    df_definitions = df_reviewed[df_reviewed['keep'] == True].copy()\n",
    "    \n",
    "    print(f\"‚úÖ R√©sultats manuels charg√©s: {len(df_definitions)} d√©finitions valid√©es\")\n",
    "    \n",
    "    # Utiliser les cat√©gories finales\n",
    "    category_col = 'final_category'\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Fichier r√©vis√© non trouv√©. Utilisation de la classification automatique.\")\n",
    "    df_definitions = df_candidates.copy()\n",
    "    df_definitions['final_category'] = df_definitions['suggested_category']\n",
    "    category_col = 'final_category'\n",
    "\n",
    "# Nettoyer les cat√©gories invalides\n",
    "valid_categories = list(TAXONOMY_CATEGORIES.keys())\n",
    "df_definitions[category_col] = df_definitions[category_col].apply(\n",
    "    lambda x: x if x in valid_categories else 'other'\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä R√©partition finale des d√©finitions:\")\n",
    "print(\"‚îÄ\"*70)\n",
    "\n",
    "for cat_id in ['copilot', 'autonomous_worker', 'orchestrator', 'governance', 'other']:\n",
    "    count = (df_definitions[category_col] == cat_id).sum()\n",
    "    pct = count / len(df_definitions) * 100 if len(df_definitions) > 0 else 0\n",
    "    print(f\"  {TAXONOMY_CATEGORIES[cat_id]['label']:45} : {count:2} ({pct:5.1f}%)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üìã Cr√©ation de la Table des D√©finitions\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABLE DES D√âFINITIONS PAR CAT√âGORIE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cr√©er une table propre pour le rapport\n",
    "table_data = []\n",
    "\n",
    "for _, row in df_definitions.iterrows():\n",
    "    table_data.append({\n",
    "        'Report': row['filename'][:40],\n",
    "        'Source Type': row['source_type'],\n",
    "        'Category': TAXONOMY_CATEGORIES[row[category_col]]['label'],\n",
    "        'Definition': row['text'][:150] + '...' if len(row['text']) > 150 else row['text']\n",
    "    })\n",
    "\n",
    "df_table = pd.DataFrame(table_data)\n",
    "\n",
    "# Sauvegarder la table compl√®te\n",
    "df_table.to_csv(TAXONOMY_DIR / 'definitions_table.csv', index=False, encoding='utf-8')\n",
    "print(f\"üíæ Table sauvegard√©e: {TAXONOMY_DIR / 'definitions_table.csv'}\")\n",
    "\n",
    "# Afficher quelques exemples par cat√©gorie\n",
    "print(\"\\nüìã Exemples par cat√©gorie:\\n\")\n",
    "\n",
    "for cat_id in ['copilot', 'autonomous_worker', 'orchestrator', 'governance']:\n",
    "    cat_label = TAXONOMY_CATEGORIES[cat_id]['label']\n",
    "    cat_defs = df_table[df_table['Category'] == cat_label]\n",
    "    \n",
    "    if len(cat_defs) == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    print(f\"{cat_label}\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    \n",
    "    for _, row in cat_defs.head(2).iterrows():\n",
    "        print(f\"\\n  üìÑ {row['Report']}\")\n",
    "        print(f\"     {row['Definition']}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üìä Visualisations de la Taxonomie\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 1. Distribution Globale (Bar Chart)\n",
    "\n",
    "# %%\n",
    "# Compter les d√©finitions par cat√©gorie\n",
    "category_counts = df_definitions[category_col].value_counts()\n",
    "\n",
    "# Cr√©er le mapping vers les labels complets\n",
    "category_labels = {cat_id: TAXONOMY_CATEGORIES[cat_id]['label'] \n",
    "                  for cat_id in category_counts.index}\n",
    "category_counts.index = category_counts.index.map(category_labels)\n",
    "\n",
    "# Graphique\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8']\n",
    "bars = ax.bar(range(len(category_counts)), category_counts.values, \n",
    "             color=colors[:len(category_counts)], edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_xticks(range(len(category_counts)))\n",
    "ax.set_xticklabels(category_counts.index, rotation=45, ha='right', fontsize=10)\n",
    "ax.set_ylabel('Nombre de D√©finitions', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribution des D√©finitions par Cat√©gorie Conceptuelle', \n",
    "            fontsize=14, fontweight='bold', pad=20)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for i, (bar, count) in enumerate(zip(bars, category_counts.values)):\n",
    "    ax.text(i, count + 0.5, str(count), ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(TAXONOMY_DIR / 'taxonomy_distribution.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üíæ Graphique sauvegard√©: {TAXONOMY_DIR / 'taxonomy_distribution.png'}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 2. Distribution par Type de Source\n",
    "\n",
    "# %%\n",
    "# Cr√©er une matrice cat√©gorie √ó type de source\n",
    "cross_tab = pd.crosstab(\n",
    "    df_definitions[category_col].map(lambda x: TAXONOMY_CATEGORIES[x]['label']),\n",
    "    df_definitions['source_type']\n",
    ")\n",
    "\n",
    "# Heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "sns.heatmap(cross_tab, annot=True, fmt='d', cmap='YlGnBu', \n",
    "           linewidths=0.5, cbar_kws={'label': 'Nombre de D√©finitions'}, ax=ax)\n",
    "\n",
    "ax.set_xlabel('Type de Source', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Cat√©gorie Conceptuelle', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Distribution des D√©finitions: Cat√©gorie √ó Type de Source', \n",
    "            fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(TAXONOMY_DIR / 'taxonomy_by_source_heatmap.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üíæ Heatmap sauvegard√©: {TAXONOMY_DIR / 'taxonomy_by_source_heatmap.png'}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 3. Treemap (Visualisation Hi√©rarchique)\n",
    "\n",
    "# %%\n",
    "# Pr√©parer les donn√©es pour le treemap\n",
    "treemap_data = []\n",
    "\n",
    "for cat_id, cat_info in TAXONOMY_CATEGORIES.items():\n",
    "    count = (df_definitions[category_col] == cat_id).sum()\n",
    "    if count > 0:\n",
    "        treemap_data.append({\n",
    "            'category': cat_info['label'],\n",
    "            'count': count,\n",
    "            'parent': 'Agentic AI Definitions'\n",
    "        })\n",
    "        \n",
    "        # Ajouter les sous-niveaux par type de source\n",
    "        for source_type in df_definitions['source_type'].unique():\n",
    "            source_count = ((df_definitions[category_col] == cat_id) & \n",
    "                          (df_definitions['source_type'] == source_type)).sum()\n",
    "            if source_count > 0:\n",
    "                treemap_data.append({\n",
    "                    'category': f\"{source_type} ({source_count})\",\n",
    "                    'count': source_count,\n",
    "                    'parent': cat_info['label']\n",
    "                })\n",
    "\n",
    "df_treemap = pd.DataFrame(treemap_data)\n",
    "\n",
    "# Ajouter le n≈ìud racine\n",
    "root_count = df_definitions.shape[0]\n",
    "df_treemap = pd.concat([\n",
    "    pd.DataFrame([{'category': 'Agentic AI Definitions', 'count': root_count, 'parent': ''}]),\n",
    "    df_treemap\n",
    "], ignore_index=True)\n",
    "\n",
    "# Cr√©er le treemap avec Plotly\n",
    "fig = px.treemap(\n",
    "    df_treemap,\n",
    "    names='category',\n",
    "    parents='parent',\n",
    "    values='count',\n",
    "    title='Taxonomie Hi√©rarchique des D√©finitions d\\'Agentic AI',\n",
    "    color='count',\n",
    "    color_continuous_scale='RdYlBu_r'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    font=dict(size=14),\n",
    "    title_font=dict(size=18, family='Arial Black'),\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.write_html(TAXONOMY_DIR / 'taxonomy_treemap.html')\n",
    "fig.show()\n",
    "\n",
    "print(f\"üíæ Treemap interactif sauvegard√©: {TAXONOMY_DIR / 'taxonomy_treemap.html'}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 4. Sunburst Chart (Alternative au Treemap)\n",
    "\n",
    "# %%\n",
    "# Cr√©er un sunburst chart\n",
    "fig = px.sunburst(\n",
    "    df_treemap,\n",
    "    names='category',\n",
    "    parents='parent',\n",
    "    values='count',\n",
    "    title='Taxonomie des D√©finitions - Vue Sunburst',\n",
    "    color='count',\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    font=dict(size=13),\n",
    "    title_font=dict(size=18, family='Arial Black'),\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig.write_html(TAXONOMY_DIR / 'taxonomy_sunburst.html')\n",
    "fig.show()\n",
    "\n",
    "print(f\"üíæ Sunburst sauvegard√©: {TAXONOMY_DIR / 'taxonomy_sunburst.html'}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### 5. Graphique Sankey (Flux: Source Type ‚Üí Cat√©gorie)\n",
    "\n",
    "# %%\n",
    "# Pr√©parer les donn√©es pour Sankey\n",
    "source_types = df_definitions['source_type'].unique()\n",
    "categories = df_definitions[category_col].unique()\n",
    "\n",
    "# Cr√©er les mappings\n",
    "source_to_idx = {s: i for i, s in enumerate(source_types)}\n",
    "cat_to_idx = {c: i + len(source_types) for i, c in enumerate(categories)}\n",
    "\n",
    "# Construire les flux\n",
    "sources = []\n",
    "targets = []\n",
    "values = []\n",
    "labels = list(source_types) + [TAXONOMY_CATEGORIES[c]['label'] for c in categories]\n",
    "\n",
    "for _, row in df_definitions.iterrows():\n",
    "    source_idx = source_to_idx[row['source_type']]\n",
    "    target_idx = cat_to_idx[row[category_col]]\n",
    "    \n",
    "    # V√©rifier si le flux existe d√©j√†\n",
    "    try:\n",
    "        idx = sources.index(source_idx)\n",
    "        if targets[idx] == target_idx:\n",
    "            values[idx] += 1\n",
    "            continue\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    sources.append(source_idx)\n",
    "    targets.append(target_idx)\n",
    "    values.append(1)\n",
    "\n",
    "# Cr√©er le diagramme Sankey\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=labels,\n",
    "        color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', \n",
    "               '#98D8C8', '#F7DC6F', '#BB8FCE', '#85C1E9'][:len(labels)]\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=sources,\n",
    "        target=targets,\n",
    "        value=values\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Flux des D√©finitions: Type de Source ‚Üí Cat√©gorie Conceptuelle\",\n",
    "    font=dict(size=12),\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.write_html(TAXONOMY_DIR / 'taxonomy_sankey.html')\n",
    "fig.show()\n",
    "\n",
    "print(f\"üíæ Sankey sauvegard√©: {TAXONOMY_DIR / 'taxonomy_sankey.html'}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üìä Analyse Comparative des Cat√©gories\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSE COMPARATIVE DES CAT√âGORIES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Statistiques par cat√©gorie\n",
    "for cat_id, cat_info in TAXONOMY_CATEGORIES.items():\n",
    "    cat_defs = df_definitions[df_definitions[category_col] == cat_id]\n",
    "    \n",
    "    if len(cat_defs) == 0:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'‚îÄ'*70}\")\n",
    "    print(f\"{cat_info['label']}\")\n",
    "    print(f\"{'‚îÄ'*70}\")\n",
    "    \n",
    "    print(f\"  Nombre total: {len(cat_defs)}\")\n",
    "    print(f\"  R√©partition par type:\")\n",
    "    \n",
    "    for source_type in cat_defs['source_type'].value_counts().index:\n",
    "        count = (cat_defs['source_type'] == source_type).sum()\n",
    "        pct = count / len(cat_defs) * 100\n",
    "        print(f\"    ‚Ä¢ {source_type:15} : {count:2} ({pct:5.1f}%)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üìã Cr√©ation du R√©sum√© pour le Rapport\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"R√âSUM√â POUR LE RAPPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "MINI-TAXONOMY DES D√âFINITIONS D'AGENTIC AI\n",
    "{'='*70}\n",
    "\n",
    "M√âTHODOLOGIE\n",
    "{'-'*70}\n",
    "‚Ä¢ Extraction semi-automatique: {len(df_candidates)} d√©finitions candidates\n",
    "‚Ä¢ Validation manuelle: {len(df_definitions)} d√©finitions finales\n",
    "‚Ä¢ Cat√©gorisation: 4 cat√©gories conceptuelles principales\n",
    "\n",
    "R√âSULTATS\n",
    "{'-'*70}\n",
    "\"\"\"\n",
    "\n",
    "for cat_id, cat_info in TAXONOMY_CATEGORIES.items():\n",
    "    if cat_id == 'other':\n",
    "        continue\n",
    "    \n",
    "    count = (df_definitions[category_col] == cat_id).sum()\n",
    "    pct = count / len(df_definitions) * 100 if len(df_definitions) > 0 else 0\n",
    "    \n",
    "    summary_text += f\"\\n{cat_info['label']}: {count} d√©finitions ({pct:.1f}%)\\n\"\n",
    "    summary_text += f\"  {cat_info['description']}\\n\"\n",
    "\n",
    "summary_text += f\"\\n{'-'*70}\\n\"\n",
    "summary_text += \"INSIGHTS CL√âS\\n\"\n",
    "summary_text += f\"{'-'*70}\\n\"\n",
    "\n",
    "# Identifier la cat√©gorie dominante\n",
    "dominant_cat = df_definitions[category_col].value_counts().idxmax()\n",
    "dominant_count = df_definitions[category_col].value_counts().max()\n",
    "dominant_pct = dominant_count / len(df_definitions) * 100\n",
    "\n",
    "summary_text += f\"\\n‚Ä¢ Cadrage dominant: {TAXONOMY_CATEGORIES[dominant_cat]['label']} ({dominant_pct:.1f}%)\\n\"\n",
    "\n",
    "# Analyser par type de source\n",
    "summary_text += \"\\n‚Ä¢ Diff√©rences par type de source:\\n\"\n",
    "for source_type in df_definitions['source_type'].unique():\n",
    "    source_defs = df_definitions[df_definitions['source_type'] == source_type]\n",
    "    if len(source_defs) > 0:\n",
    "        top_cat = source_defs[category_col].value_counts().idxmax()\n",
    "        summary_text += f\"  - {source_type}: privil√©gie '{TAXONOMY_CATEGORIES[top_cat]['label']}'\\n\"\n",
    "\n",
    "print(summary_text)\n",
    "\n",
    "# Sauvegarder le r√©sum√©\n",
    "summary_file = TAXONOMY_DIR / 'taxonomy_summary.txt'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(f\"\\nüíæ R√©sum√© sauvegard√©: {summary_file}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üíæ Sauvegarde Compl√®te des R√©sultats\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAUVEGARDE DES R√âSULTATS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. D√©finitions compl√®tes avec cat√©gories\n",
    "df_definitions.to_csv(TAXONOMY_DIR / 'definitions_categorized.csv', \n",
    "                     index=False, encoding='utf-8')\n",
    "print(f\"‚úÖ D√©finitions cat√©goris√©es: {TAXONOMY_DIR / 'definitions_categorized.csv'}\")\n",
    "\n",
    "# 2. Table pour le rapport (format propre)\n",
    "report_table = []\n",
    "for cat_id, cat_info in TAXONOMY_CATEGORIES.items():\n",
    "    if cat_id == 'other':\n",
    "        continue\n",
    "    \n",
    "    cat_defs = df_definitions[df_definitions[category_col] == cat_id]\n",
    "    \n",
    "    for _, row in cat_defs.iterrows():\n",
    "        report_table.append({\n",
    "            'Category': cat_info['label'],\n",
    "            'Report': row['filename'],\n",
    "            'Source_Type': row['source_type'],\n",
    "            'Definition_Excerpt': row['text'][:200] + '...' if len(row['text']) > 200 else row['text']\n",
    "        })\n",
    "\n",
    "df_report_table = pd.DataFrame(report_table)\n",
    "df_report_table.to_csv(TAXONOMY_DIR / 'taxonomy_table_for_report.csv', \n",
    "                       index=False, encoding='utf-8')\n",
    "print(f\"‚úÖ Table pour rapport: {TAXONOMY_DIR / 'taxonomy_table_for_report.csv'}\")\n",
    "\n",
    "# 3. Statistiques agr√©g√©es\n",
    "stats = {\n",
    "    'total_definitions': len(df_definitions),\n",
    "    'num_categories': len([c for c in TAXONOMY_CATEGORIES.keys() if c != 'other']),\n",
    "    'category_distribution': {\n",
    "        TAXONOMY_CATEGORIES[cat]['label']: int((df_definitions[category_col] == cat).sum())\n",
    "        for cat in TAXONOMY_CATEGORIES.keys()\n",
    "    },\n",
    "    'by_source_type': {}\n",
    "}\n",
    "\n",
    "for source_type in df_definitions['source_type'].unique():\n",
    "    source_defs = df_definitions[df_definitions['source_type'] == source_type]\n",
    "    stats['by_source_type'][source_type] = {\n",
    "        'total': len(source_defs),\n",
    "        'distribution': {\n",
    "            TAXONOMY_CATEGORIES[cat]['label']: int((source_defs[category_col] == cat).sum())\n",
    "            for cat in TAXONOMY_CATEGORIES.keys()\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open(TAXONOMY_DIR / 'taxonomy_statistics.json', 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Statistiques JSON: {TAXONOMY_DIR / 'taxonomy_statistics.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"‚îÄ\"*70)\n",
    "print(\"Fichiers g√©n√©r√©s:\")\n",
    "print(\"  ‚Ä¢ definitions_categorized.csv - Toutes les d√©finitions avec cat√©gories\")\n",
    "print(\"  ‚Ä¢ taxonomy_table_for_report.csv - Table format√©e pour le rapport\")\n",
    "print(\"  ‚Ä¢ taxonomy_distribution.png - Bar chart de distribution\")\n",
    "print(\"  ‚Ä¢ taxonomy_by_source_heatmap.png - Heatmap cat√©gorie √ó source\")\n",
    "print(\"  ‚Ä¢ taxonomy_treemap.html - Treemap interactif\")\n",
    "print(\"  ‚Ä¢ taxonomy_sunburst.html - Sunburst interactif\")\n",
    "print(\"  ‚Ä¢ taxonomy_sankey.html - Diagramme Sankey des flux\")\n",
    "print(\"  ‚Ä¢ taxonomy_statistics.json - Statistiques agr√©g√©es\")\n",
    "print(\"  ‚Ä¢ taxonomy_summary.txt - R√©sum√© textuel\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üìù Template Texte pour le Rapport\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEMPLATE POUR LE RAPPORT (Section: Mini-Taxonomy)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "template = f\"\"\"\n",
    "### Mini-Taxonomy of Definitions of \"Agentic AI\"\n",
    "\n",
    "A hybrid qualitative-quantitative analysis was conducted to extract and \n",
    "categorize explicit definitions of \"agentic AI\" across the corpus. The \n",
    "methodology combined:\n",
    "\n",
    "1. **Semi-automatic extraction**: Pattern matching identified {len(df_candidates)} \n",
    "   candidate definitions containing key terms (\"agentic AI\", \"AI agents\", \n",
    "   \"autonomous agents\") and definitional markers (\"is defined as\", \"refers to\").\n",
    "\n",
    "2. **Manual validation**: Each candidate was manually reviewed, resulting in \n",
    "   {len(df_definitions)} validated definitions spanning {len(df_definitions['doc_id'].unique())} \n",
    "   reports.\n",
    "\n",
    "3. **Conceptual categorization**: Definitions were grouped into four primary \n",
    "   conceptual frames:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for cat_id in ['copilot', 'autonomous_worker', 'orchestrator', 'governance']:\n",
    "    cat_info = TAXONOMY_CATEGORIES[cat_id]\n",
    "    count = (df_definitions[category_col] == cat_id).sum()\n",
    "    pct = count / len(df_definitions) * 100 if len(df_definitions) > 0 else 0\n",
    "    \n",
    "    template += f\"\\n**{cat_info['label']}** ({pct:.1f}%)\\n\"\n",
    "    template += f\"{cat_info['description']}\\n\"\n",
    "\n",
    "template += f\"\"\"\n",
    "#### Results and Interpretation\n",
    "\n",
    "Figure X presents the distribution of definitions across conceptual categories. \n",
    "The dominant frame is **{TAXONOMY_CATEGORIES[dominant_cat]['label']}** ({dominant_pct:.1f}%), \n",
    "suggesting that the prevailing discourse conceptualizes agentic AI as \n",
    "{TAXONOMY_CATEGORIES[dominant_cat]['description'].lower()}.\n",
    "\n",
    "**Institutional differences** (Figure Y - Heatmap) reveal:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for source_type in df_definitions['source_type'].unique():\n",
    "    source_defs = df_definitions[df_definitions['source_type'] == source_type]\n",
    "    if len(source_defs) > 0:\n",
    "        top_cat = source_defs[category_col].value_counts().idxmax()\n",
    "        top_pct = (source_defs[category_col] == top_cat).sum() / len(source_defs) * 100\n",
    "        template += f\"- **{source_type}**: {top_pct:.0f}% frame agentic AI as '{TAXONOMY_CATEGORIES[top_cat]['label']}'\\n\"\n",
    "\n",
    "template += f\"\"\"\n",
    "**Key insight**: The taxonomy exposes conceptual fragmentation in how agentic \n",
    "AI is defined. While some actors emphasize augmentation (copilots), others \n",
    "stress full autonomy (autonomous workers) or systemic complexity (orchestrators). \n",
    "This definitional ambiguity poses challenges for standardization and may lead \n",
    "to misaligned adoption strategies across organizations.\n",
    "\n",
    "**Table 1** (see Appendix) provides the complete taxonomy with representative \n",
    "definitions from each category and source type.\n",
    "\"\"\"\n",
    "\n",
    "print(template)\n",
    "\n",
    "# Sauvegarder le template\n",
    "with open(TAXONOMY_DIR / 'report_template.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(template)\n",
    "\n",
    "print(f\"\\nüíæ Template sauvegard√©: {TAXONOMY_DIR / 'report_template.txt'}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## üìã R√©sum√© de l'√âtape 5\n",
    "# \n",
    "# **‚úÖ Analyses compl√©t√©es:**\n",
    "# - Extraction semi-automatique de d√©finitions (pattern matching)\n",
    "# - Classification en 4 cat√©gories conceptuelles\n",
    "# - Validation manuelle (fichier CSV pour r√©vision)\n",
    "# - Analyse comparative par type de source\n",
    "# - Visualisations multiples (bar, heatmap, treemap, sunburst, sankey)\n",
    "# \n",
    "# **üìÇ Fichiers g√©n√©r√©s:**\n",
    "# - 5 visualisations PNG/HTML\n",
    "# - 3 fichiers CSV (candidates, d√©finitions, table rapport)\n",
    "# - 2 fichiers JSON (statistiques)\n",
    "# - 2 fichiers TXT (r√©sum√©, template)\n",
    "# \n",
    "# **üìä Visuels pour le rapport:**\n",
    "# 1. `taxonomy_distribution.png` - Distribution globale\n",
    "# 2. `taxonomy_by_source_heatmap.png` - Heatmap par source\n",
    "# 3. `taxonomy_treemap.html` - Vue hi√©rarchique interactive\n",
    "# 4. `taxonomy_table_for_report.csv` - Table compl√®te\n",
    "# \n",
    "# **üîë Insights cl√©s:**\n",
    "# - Fragmentation conceptuelle des d√©finitions\n",
    "# - Diff√©rences narratives entre types de sources\n",
    "# - Dominance d'un cadrage particulier (√† interpr√©ter)\n",
    "# \n",
    "# **‚û°Ô∏è Prochaine √©tape:**\n",
    "# - Synth√®se finale et r√©daction du rapport complet\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üéâ √âTAPE 5 TERMIN√âE AVEC SUCC√àS!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä R√©sum√© de la Taxonomie:\")\n",
    "print(f\"  ‚Ä¢ D√©finitions extraites    : {len(df_candidates)} candidates\")\n",
    "print(f\"  ‚Ä¢ D√©finitions valid√©es     : {len(df_definitions)}\")\n",
    "print(f\"  ‚Ä¢ Cat√©gories conceptuelles : 4 principales\")\n",
    "print(f\"  ‚Ä¢ Documents couverts       : {len(df_definitions['doc_id'].unique())}\")\n",
    "print(f\"  ‚Ä¢ Types de sources         : {len(df_definitions['source_type'].unique())}\")\n",
    "print(f\"\\nüìÇ Tous les fichiers dans: {TAXONOMY_DIR}\")\n",
    "print(f\"\\n‚úÖ Toutes les √©tapes d'analyse sont compl√®tes!\")\n",
    "print(f\"‚û°Ô∏è Pr√™t pour la r√©daction du rapport final\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
