practices for governing agentic ai systems yonadav shavit sandhini agarwal miles brundage steven adler cullen o keefe rosie campbell teddy lee pamela mishkin tyna eloundou alan hickey katarina slama lama ahmad paul mcmillan alex beutel alexandre passos david g robinson abstract agenticaisystems aisystemsthatcanpursuecomplexgoalswithlimiteddirectsupervision are likely to be broadly useful if we can integrate them responsibly into our society while such systems have substantial potential to help people more efficiently and effectively achieve their own goals they also create risks of harm in this white paper we suggest a definition of agentic ai systems and the parties in the agentic ai system life-cycle and highlight the importance of agreeing on a set of baseline responsibilities and safety best practices for each of these parties as our primary contribution we offer an initial set of practices for keeping agents operations safe and accountable which we hope can serve as building blocks in the development of agreed baseline best practices we enumerate the questions and uncertainties around operationalizing each of these practices that must be addressed before such practices can be codified we then highlight categories of indirect impacts from the wide-scale adoption of agentic ai systems which are likely to necessitate additional governance frameworks table of contents introduction definitions agenticness agentic ai systems and agents the human parties in the ai agent life-cycle potential benefits of agentic ai systems agenticness as a helpful property agenticness as an impact multiplier practices for keeping agentic ai systems safe and accountable evaluating suitability for the task constraining the action-space and requiring approval setting agents default behaviors legibility of agent activity automatic monitoring attributability interruptibility and maintaining control lead authors correspondence should be directed to indirect impacts from agentic ai systems adoption races labor displacement and differential adoption rates shifting offense-defense balances correlated failures conclusion acknowledgements introduction ai researchers and companies have recently begun to develop increasingly agentic ai systems systems that adaptably pursue complex goals using reasoning and with limited direct supervision for example a user could ask an agentic personal assistant to help me bake a good chocolate cake tonight and the system would respond by figuring out the ingredients needed finding vendors to buy ingredients and having the ingredients delivered to their doorstep along with a printed recipe agentic ai systems are distinct from more limited ai systems like image generation or question-answering language models because they are capable of a wide range of actions and are reliable enough that in certain defined circumstances a reasonable user could trust them to effectively and autonomously act on complex goals on their behalf this trend towards agency may both substantially expand the helpful uses of ai systems and introduce a range of new technical and social challenges agentic ai systems could dramatically increase users abilities to get more done in their lives with less effort this could involve completing tasks beyond the users skill sets like specialized coding agentic systems could also benefit users by enabling them to partially or fully offload tasks that they already know how to do meaning the tasks can get done more cheaply quickly and at greater scale so long as these benefits exceed the cost of setting up and safely operating an agentic system agentic systems can be a substantial boon for individuals and society in this paper we will primarily focus on agentic systems with language models at their core including multimodal models as these have driven recent progress society will only be able to harness the full benefits of agentic ai systems if it can make them safe by mitigating their failures vulnerabilities and abuses this motivates our overarching question what practices could be adopted to prevent these failures vulnerabilities and abuses and where in the life-cycle of creating and using agents are they best implemented there are often many different stages at which harm could have been prevented for example consider a hypothetical agentic ai assistant whose user not based in japan directs it to purchase supplies for baking a japanese cheesecake instead of purchasing supplies locally the agent purchases an expensive plane ticket to japan which the user only notices when it is too late to refund in this hypothetical scenario several parties could have prevented this outcome the model developer could have improved the system s reliability and user-alignment4 so that it wouldn t have made this 1see section for elaboration on this definition 2this is in contrast to earlier generations of agentic ai systems which did not explicitly reason through language such as the deep blue chess playing program from ibm that defeated garry kasparov 3inthiscontext afailureiswhentheagentfailstoachievesomeobjectiveordoessoinanunsatisfactoryorharmful manner a vulnerability is when the agent can be co-opted or undermined by an attacker and an abuse is when an agent is used for harmful purposes 4in this paper we will refer to user-alignment as the propensity of an ai model or system to follow the goals specified by a user mistake the system deployer could have disabled the agent from taking action without explicit approval the user could have simply never agreed to delegate purchasing authority to an ai system that was commonly known to not be fully reliable the airline company could have even instituted policies or technologies that required affirmative human consent for purchases given that multiple parties could have taken steps to mitigate the damages every party can arguably cast blame on the other and in the worst case a party can be held responsible even when they could not have reasonably prevented the outcome a key goal of allocating accountability for harms from agentic ai systems should be to create incentives to reduce the likelihood and severity of such harms as efficiently as possible in order to make sure that someone is incentivized to take the necessary measures it is important that at least one human entity is accountable for every uncompensated direct harm caused by an agentic ai system other scholarship has proposed more radical or bespoke methods for achieving accountability such as legal personhood for agents coupled with mandatory insurance or targeted regulatory regimes these all appear to address the same problem in order to create incentives to reduce or eliminate harms from agentic ai systems society needs to agree on baseline best practices 6that prudent model developers system deployers and users are expected to follow given such a baseline when an agentic ai system causes harm we can identify which parties deviated from these best practices in a way that failed to prevent the harm in this white paper we lay out several practices that different actors can implement to mitigate the risk of harm from agentic ai systems which could serve as building blocks for a set of agreed baseline best practices we also highlight the many areas where operationalizing these practices may be difficult especially where there could be tradeoffs among safety usability privacy and cost ai developers cannot answer these questions alone nor should they and we are eager for further research and guidance from the wider world in section we define agentic ai systems and the human parties in the agentic ai life-cycle in section we briefly describe the potential benefits of agentic systems in section we provide an initial seven practices that could be part of a set of agreed best practices for parties in the agent life-cycle and highlight open questions finally in section we consider more indirect impacts from the introduction of ai agents that may not be addressable by a focus on individual harms we hope that the best practices we outline can serve as building blocks for a society-wide discussion about how to best structure accountability for risks from agentic ai systems for example they may inform discussion around what regulation of ai agent development might look like or how parties structure contracts regarding agents e g insurance for harms caused by agents terms of use regarding agents or how courts could think of various actors standards of care given the nascent state of agents and their associated scholarship we do not yet have strong recommendations on how accountability ought to be structured and would like to see a more robust public discussion of possible options we hope that this paper will help catalyze such conversations without anchoring or biasing them too strongly in any particular direction 5that is an individual corporation or other legal entity but not solely an ai system itself 6we refer to baseline best practices here rather than e g the legal concept of a professional standard of care the set of actions a reasonable and prudent party is expected to take such that deviating from this standard opens them up to legal responsibility from the resulting harmsince the former could provide a foundation for the latter and may also inform policymaking outside of courtrooms e g through legislation and regulation definitions agenticness agentic ai systems and agents agentic ai systems are characterized by the ability to take actions which consistently contribute towards achieving goals over an extended period of time without their behavior having been specified in advance in the cultural imagination an ai agent is a helper that accomplishes arbitrary tasks for its user like samantha from her or hal from a space odyssey such agents are very different from current ai systems like gpt- which while surprisingly knowledgeable and clever in some ways can thus far only complete a limited range of real-world tasks yet there is no clear line along which to draw a binary distinction between agents and current ai systems like gpt- instead an ai system s agenticness is best understood as involving multiple dimensions along each of which we expect the field to continue to progress we define the degree of agenticness in a system as the degree to which a system can adaptably achieve complex goals in complex environments with limited direct supervision agenticness as defined here thus breaks down into several components goal complexity how challenging would the ai system s goal8 be for a human to achieve and how wide of a range of goals could the system achieve properties of the goal may include target levels of reliability speed and safety example an ai system that can correctly answer users analytical questions across programming and law would have greater goal-complexity than a text classifier that can only classify the same inputs as belonging to law or programming environmental complexity how complex are the environments under which a system can achieve the goal e g to what extent are they cross-domain multi-stakeholder require operating over long time-horizons and or involve the use of multiple external tools example an ai system that can play any board game expertly has greater environment- complexity than an ai system that can only play chess because the first system can succeed under a far greater range of environments including chess than the second adaptability howwellcanthesystemadaptandreacttonovelorunexpectedcircumstances example automated rule-based customer-service systems have lower adaptability than human customer-service representatives since humans can address unexpected or un- precedented customer requests independent execution to what extent can the system reliably achieve its goals with limited human intervention or supervision example cars capable of level autonomous driving which can operate without human intervention under certain circumstances have greater independent execution than traditional cars that require continuous human operation 7we recognize that a variety of definitions of agenticness agents and agency are used by various people for various purposes inourassessmentmanyorallofthepracticeswediscussintheremainderofthepaperareapplicableacross many alternative definitions of these terms 8we will assume that an agentic ai system can be modeled as having goals including externally-defined goals such as following a set of provided instructions following recent literature we will generally refer to systems exhibiting high degrees of agenticness as agentic ai systems to emphasize that agenticness as we use it here is a property rather than a category classification though we will sometimes use agents as it is the prevailing term of art in some contexts this work will focus on the range of effects and best practices that may become relevant as systems agenticness increases we emphasize that agenticness is a distinct concept from consciousness moral patienthood or self-motivation and distinguish a system s degree of agenticness from its anthropomorphism indeed we will generally conceptualize agentic ai systems as operating in pursuit of goals defined by humans and in environments determined by humans and often in cooperation with human teammates rather than fully-autonomous systems that set their own goals agenticness as we define it is also not tied to physicality i e many digital systems are more agentic in the sense above than most robots but certain kinds of independent execution that have physical consequences e g in a driverless car can increase the risks and opportunities of agenticness in particular applications lastly agenticness is conceptually distinct from an ai system s level of performance on a given task or the generality of its capabilities though improvements in performance and generality may unlock the ability of a system to act as an agent in certain contexts the human parties in the ai agent life-cycle we provide a simplified overview of the agentic ai life-cycle though there are many different configurations of these roles in the ai industry and we hope further taxonomies emerge in our taxonomy the three primary parties that may influence an ai agent s operations are the model developer the system deployer and the user the model developer is the party that develops the ai model that powers the agentic system and thus broadly sets the capabilities and behaviors according to which the larger system operates the system deployer is the party that builds and operates the larger system built on top of a model including by making calls to the developed model such as by providing a system prompt routing those calls to tools with which the agent can take actions and providing users an interface through which they interact with the agent the system deployer may also tailor the ai system to a specific use case and thus may frequently have more domain-specific knowledge than the model developer or even the user finally the agent s user is the party that employs the specific instance of the agentic ai system by initiating it and providing it with the instance-specific goals it should pursue the user may be able to most directly oversee certain behaviors of the agentic system through its operation during which it can also interact with third parties e g other humans or the providers of apis with which the agent can interact sometimes the same entity will fulfill multiple roles such as the same company both developing a model and then deploying it via an api making them both the model developer and one of the system deployers other times multiple entities will share a role such as when one company trains a model and a second company fine-tunes it for their application making them share the 9for example llms are being augmented with tools scaffolding to increase their scores on the dimensions of agenticness including chain-of-thought to help with strategic reasoning code execution to help with independent execution and browsing to help with adaptability etc 10agenticness does not imply or require a human-like appearance or human-like behavior though anthropomorphic appearances and behavior may increase the likelihood of humans perceiving such systems as agentic and have other implications for responsible design and deployment 11we use this taxonomy as a useful mental model for enabling division of practices across the agent lifecycle and to better highlight open questions these are not intended to establish a prescriptive framework for allocation of responsibility suchresponsibilitymayvarydependingonthecontext forexample foranagentthatperformsmedical diagnoses if the agent is deployed in a hospital more responsibility may fall on the user a doctor whereas if the agent is a consumer app marketed as a personal diagnostic tool more perhaps more responsibility should fall on the system deployer the app developer responsibilities of a model developer we will also occasionally mention other relevant actors including the compute provider which operates the chips and other infrastructure on which agentic ai systems run and third-parties which interact with the user-initiated ai system we illustrate with the specific example of a scheduling assistant built on openai s assistants api openai developed the gpt- model making it the model developer openai deployed the infrastructure including serving the model and connecting it to tools such as a code execution environment and the application developer builds an app on top of it e g by building a user- interface choosing a system prompt and supplying an email template for the system to use when sending invites meaning they both share the role of system deployer finally a customer initiates a session with the scheduling assistant and specifies which goals e g scheduling requirements they d like the system to satisfy making them the user potential benefits of agentic ai systems in this section we take stock of the ways that agentic ai systems have the potential to benefit society first we consider the ways that a more agentic version of a particular ai system might be more beneficial than a less agentic version agenticness as a helpful property second we consider the ways in which agenticness can enable wider diffusion of ai in beneficial applications in society and is often implicit in many definitions of and visions for ai agenticness as an impact multiplier while our discussion in this section is brief this should not be read as an indication that the list of possible benefits is necessarily short or that the magnitude of those benefits is small nor do we make claims that the benefits clearly outweigh the risks or vice versa agenticness as a helpful property specific ai systems may in many cases be more beneficial in proportion to the extent to which they are agentic provided they are designed safely and that appropriate best practices for safety and accountability are applied agenticness can make a particular system more beneficial in ways such as the following higher quality and more reliable outputs for example a language model that is capable of browsing the internet autonomously and revising its queries in response to the results it receives may be capable of providing much more accurate answers to questions than a system that is not able to do so this may be particularly true in instances involving topics that are dynamic in nature or events that occurred after the underlying model was trained more efficient use of users time for example if a user provides high level instructions to an ai system regarding code they want the system to produce it may be smoother for the user if the system performs several steps autonomouslye g translating the instructions into code running the code displaying the results assessing those results and making edits to the code in order to improve outcomes improved user preference solicitation for example personal assistant ai that is capable of interactively sending messages to its users in order to ask clarifying questions in natural language and that does so at strategically appropriate times may provide a better experience 12the important question of how to split the responsibility for different best practices across the multiple entities that may share a single agent-life-cycle role is beyond the scope of this current whitepaper 13if the application developer fine-tuned the model on their custom data they may share the model developer responsibilities than an app with numerous complex configurations that is difficult for users to leverage effectively scalability an agentic ai system may allow a single user to take many more actions than they could otherwise or be capable of benefiting a much larger number of people than a less agentic version of the same system consider the example of radiology a non-agentic radiology image classification tool may be helpful for making a radiologist slightly more efficient but an agentic radiology tool that was capable of completing certain patient-care tasks without human supervision e g compiling reports on the scan asking patients basic follow-up questions could potentially increase a radiologist s efficiency substantially and leave more time for seeing many more patients agenticness as an impact multiplier in addition to analyzing the implications of agenticness in the context of particular ai systems one can also view agenticness as a prerequisite for some of the wider systemic impacts that many expect from the diffusion of aisome of which have significant potential to benefit society insofar as agenticness is a definitional or practical prerequisite for that diffusion the impacts of agenticness may be closely related to the impacts of ai more generally in this sense the impacts of ai generally are likely to be more frequent and more pronounced and to happen sooner to the extent that agenticness increases making agenticness an impact multiplier of the field of ai as a whole sometimes agenticness is implicitly assumed when people talk about current or future ai capabilities openai s charter defines artificial general intelligence agi as highly autonomous systems that outperform humans at most economically valuable work and canonical textbooks such as russell and norvig s artificial intelligence a modern approach emphasize agenticness in their conception of ai given these considerations we briefly review several commonly expected impacts of ai as an overall technological field even without significant further advances in agenticness ai is likely to already constitute a general-purpose technology historically the widespread adoption of general purpose technologies such as the steam engine and electricity has vastly increased the global standard of living over time though also brought about significant harm for many and in particular for less powerful or privileged groups living through those periods highly capable and agentic ai systems that are widely deployed could even improve economic productivity so much that they fundamentally change the nature of work potentially and perhaps more speculatively enabling a leisure society or post-work world though this is by no means guaranteed and would carry risks additionally ai could accelerate progress on various non-economic measures of societal wellbeing such as those encapsulated in the sustainable development goals and by accelerating scientific progress and understanding the economic and other productivity gains some expect from ai may be greater to the extent that agentic ai systems are able to take actions autonomously practices for keeping agentic ai systems safe and accountable below we suggest a range of practices different parties can adopt to ensure agentic ai systems operate safely and in accordance with users intents and to create accountability when harm does occur when implemented together the practices outlined in this section are intended to provide a defense-in-depth approach to mitigating risks from agentic ai systems though many of these practices are employed in some form today we highlight many open questions around how they should be operationalized we also discuss how additional precautions may be needed as ai systems become more agentic we emphasize that these practices alone are insufficient for fully mitigating the risks from present day ai systems let alone mitigating catastrophic risks from advanced ai for example none of the principles below covers methods for ensuring the cybersecurity of agents so as to prevent them from being hijacked by attackers even though we expect this to be a significant challenge that requires new practices the practices discussed here are intended as an initial outline of approaches and relevant considerations we avoid discussion of what technical best practices to use in order to build capable and user- aligned agentic ai systems these are both rapidly evolving fields and practices are changing rapidly such that we do not expect the fields to converge on best practices for guaranteeing particular ai capabilities or user-alignment in the near term in addition the science required to predict the capabilities user-alignment of an ai model given training choices is in its infancy this means that it is currently not possible for a model developer to deterministically guarantee a model s expected behavior to downstream system deployers and users there are exceptions such as how fully excluding a training sample from the training data will mean that the model cannot regurgitate it still given the limited degree to which model behavior can be delimited in advance we will focusondesigningasetofbestpracticesthatisagnostictotheparticularmodel smethodoftraining open question what harm mitigations if any are primarily attainable via technical choices in the model s training process what might corresponding best practices be evaluating suitability for the task either the system deployer or the user should thoroughly assess whether or not a given ai model and associated agentic ai system is appropriate for their desired use case whether it can execute the intended task reliably across the range of expected deployment conditions or to the extent reliability is not necessary or expected given the low stakes of the task and the nature of the user interface that user expectations are suitably established via that interface this raises the question of how to properly evaluate an agentic ai system and what failure modes can and cannot be foreseen by sufficient testing the field of agentic ai system evaluation is nascent with more questions than answers so we offer only a few observations evaluating agentic ai systems raises new challenges on top of the already significant challenges with evaluating current language models this is in part because successful agents may often need to execute long sequences of correct actions so that even if individual actions would only fail infrequently these rare events could compound and make failure in deployment likely one solution is for system deployers to independently test the agent s reliability in executing each subtask for example when an early system deployer was building an aws troubleshooting agent on top of openai s gpt- api they broke down the agent s needed subtasks into information gathering calculations and reasoning and created evaluations for each independently breaking down all the subtasks that could be encountered in a complex real-world operating domain may sometimes be too difficult for system deployers one approach could be to prioritize doing such evaluations for agents use of high-risk actions like financial transactions even if the system is shown to do individual subtasks reliably this still raises the problem of how to evaluate whether the agent will reliably chain these actions together finally agentic systems may be expected to succeed under a wide range of conditions but the real world contains a long tail of tasks which are difficult to define and events which are hard to anticipate in advance including those that emerge from human-agent or agent-agent interactions similar difficulties with evaluating reliability under unanticipated conditions have significantly slowed the deployment of self-driving cars and one might expect a similar effect for agentic ai systems ultimately there are currently few better solutions than to evaluate the agent end-to-end in conditions whether simulated or real as close as possible to those of the deployment environment so long as our ability to bound and evaluate the behaviors of agentic ai systems remains immature system deployers and users may need to lean more heavily on other practices such as human-approval for high-stakes actions in order to bound the behavior of these systems a separate evaluation challenge for model developers and system deployers is how to determine what scale of harm their agentic system could enable whether by a user intentionally or by ac- cident due to failures of user-alignment for example frontier model developers could test their models for capabilities that would facilitate harm such as generating individualized propaganda or assisting in cyberattacks it may be important to require system deployers or model develop- ers operating on their behalf to do such evaluations in order to determine what other measures they should take to mitigate misuse of the agentic ai system services they provide such guid- anceiscurrentlyunderdevelopmentbytheusgovernment andtheinternationalcommunity open questions how can system deployers and users effectively evaluate the agentic system s level of reliability in their use case what constitutes sufficient evaluation how can system deployers effectively evaluate the combination of agent and user and identify behaviors and potential failures that only emerge through human-agent interaction given the heterogeneous nature of real-world deployment what failure modes cannot be expected to be detected in advance via evaluation what evaluations of agents capabilities should be expected to be done by the model developer rather than the system deployer e g universally useful checks such as the system s propensity to act in alignment with the user s goals how can system deployers communicate to the user the intended conditions under which the agentic system can be used reliably and at what point does a user s unintended usage of a system make them responsible for resulting harms what misusable agentic system capabilities should model developers and system deployers be obligated to test for both for specific sectors and for agents in general constraining the action-space and requiring approval some decisions may be too important for users to delegate to agents if there is even a small chance that they re done wrong such as independently initiating an irreversible large financial transaction requiring a user to proactively authorize these actions thus keeping a human-in-the-loop is a standard way to limit egregious failures of agentic ai systems this raises the key challenge of how a system deployer should ensure that the user has enough context to sufficiently understand the implications of the action they re approving this is also made harder when the user must approve many decisions and thus must make each approval quickly reducing their ability to meaningfully consider each one 14openai has committed to testing for these and other model capabilities as part of its preparedness work 15asnotedbycrootofetal ahuman-in-the-loopmayservevariousrolesbeyondsimplyimprovingthereliability of the human-machine system e g assigning liability preserving human dignity in some cases agentic ai systems should be prevented from taking certain actions entirely to better bound the system s operational envelope and thus enable safe operation for instance it may be prudent to prevent agentic ai systems from controlling weapons similarly to mitigate accidents resulting from agents running for unintentionally long periods without supervision system deployers can cause agents to periodically time out until a human reviews and reauthorizes them of course the risk-mitigation benefits of these measures must be carefully balanced with the reduction in agentic ai systems capabilities and utility such operational limitations could be placed by the model developer at the model-level by the system deployer via hardcoding or a monitoring system section and or proactively set by the user themselves as the agenticness of ai systems increases hard-coded restrictions may cease to be as effective especially if a given ai system was not trained to follow these restrictions and thus may seek to achieve its goals by having the disallowed actions occur an ai agent could circumvent a hard-coded restriction by causing another party to take the action on the system s behalf while hiding the resulting potential impact from the user for instance an agent could send an email an allowed action to a non-user human that convinces said human to take the disallowed action system deployers can bound this problem by ensuring strong network controls to limit agents interactions with the outside world except through monitored channels agentic systems could also be sandboxed in order to prevent systems with cybersecurity capabilities from escaping these controls especially during development when a system s capabilities are uncertain but current sandboxing systems may not be well-suited to effectively bound highly-capable ai agents open questions how should a user or system deployer determine and enforce which interactions need human approval as a function of a model s empirical reliability on that task and other factors are there certain interactions such as formation of legal contracts where a user s approval should always be required what design practices should system deployers follow to ensure users have sufficient context before approving an action what are the best practices for users reviewing approvals for high-cost actions such as minimum review times to avoid their turning into a rubber stamp for agentic ai systems that cannot catch harmful actions what are the best practices for system deployers to test that agentic ai systems follow the action constraints the system deployer specified how can the effectiveness and meaningfulness of human oversight be evaluated how can existing sandboxing mechanisms be modified to work for agentic ai systems how can model developers or system deployers test the reliability of a sandbox for a given agentic system without incurring the actual risk of the agentic system breaking that sandbox or a wider nested sandbox test environment setting agents default behaviors model developers could significantly reduce the likelihood of the agentic system causing accidental harm by proactively shaping the models default behavior according to certain design principles for instance user interactions with agentic ai systems may be designed to begin with a prompt to the user to communicate their goals and preferences to the system this preference information will almost always be unclear or incomplete users don t want to have to tell their life story just to get help baking a cake it is still valuable for the agent to have a set of default common-sense background preferences that allow it to fill in the gaps without a user s guidance such as users prefer if i don t spend their money in the absence of user-specific information one common-sense heuristic could be to err toward actions that are the least disruptive ones possible while still achieving the agent s goal it should often still be possible for the user to overrule these default preferences if requested specifically though it may also be important to have agents themselves refuse to execute user-intended harm section to avoid agentic systems being overconfident about users objectives model developers and system deployers may be advised to build in features that cause agents to be aware of their own uncertainty about users intended goals agents can be trained or prompted to proactively request clarifications from the user to resolve this uncertainty especially when it may change their actions however better understanding of users alone does not guarantee the agent will pursue the right objectives for example instead of producing truthful outputs with which the user may disagree certain ai systems have been found to pander to users based on what beliefs they think a given user holds which may reflect a deficiency of current techniques to align ai systems with their user s true goals having agents request information too frequently can also raise issues with usability and privacy if the preference information is sensitive open questions what other default behaviors could model developers and system deployers instill in agentic ai systems that could mitigate the possibility of errors and harms how should these default behaviors be balanced when in conflict how is responsibility allocated between the model developer who may not have intended for their model to be used in a particular agentic system and the system deployer when it comes to instilling certain behaviors in ai systems legibility of agent activity the more a user is aware of the actions and internal reasoning of their agents the easier it can be for them to notice that something has gone wrong and intervene either during operation or after the fact revealing an agent s thought process to the user enables them to spot errors including identifying when a system is pursuing the wrong goal allows for subsequent debugging and instills trust when deserved conveniently current language model-based agentic systems can produce a trace of their reasoning in natural language a so-called chain-of-thought which provides a convenient source of truth for how the system reached a conclusion on which action to take it could be useful for system deployers to expose all details of the agents interactions such as any inputs it receives from tool-use api calls or interactions with other agents this could have the added benefit of enabling users to detect when a malicious third party such as a third-party agent is attempting to manipulate the primary agent s operations however chain-of-thought transparencycomeswithchallengesandcannotyetbefullyreliedon early work has shown that sometimes models do not actually rely on their chains-of-thought when reasoning so relying on these may create a false sense of security in the user an alternative approachcouldinsteadbetoexposedifferentreasoning-relateddatatouserstohelpthemunderstand models behavior such as probes of models internal representations such methods are still nascent a separate challenge is that chains-of-thought are growing longer and more complicated as agents produce thousands of words per action or are integrated into more complex architectures e g trees-of-thought or graphs-of-thought the length and complexity of these internal monologues may balloon beyond a user s ability to feasibly keep up one potential solution may be to leverage ai-based monitoring as discussed in section other challenges arise when agents integrate new modalities like images which may contain hidden messages that a human user cannot parse future ai systems may also perform better if they can reason in compressed more efficient ways which may not necessarily be readable to humans and these inscrutable architectures may bring significant safety and accountability costs it is unfortunately not possible to simply ask the agent to retroactively justify its behavior as this is likely to produce confabulated reasoning it may be particularly important that system deployers provide the user with a ledger of actions taken by the agent this is a lighter-touch method than requiring human approval section and can thereby give users visibility into a wider range of actions without substantially slowing down the system s operation actions that can only be reviewed after the fact should be more easily reversible than those that require approval much like when requiring action approvals it maybeimportanttoalsoprovideanagent sreasoningtotheusertoproperlycontextualizetheaction open questions should system deployers be required to provide their agents internal monologue to the user should all ai agent reasoning including inter-agent communication be required to occur in natural language in order to make subsequent auditing by humans easier what methods can model developers and system deployers take in order to produce accounts of agents thoughts that are more faithful to the logic the system truly employed how can model developers make their models reason in ways that are more legible to the user or system deployer what user-experience practices should system deployers follow in order for users to be able to easily understand and act on agents internal reasoning how can users easily identify the actions that are most likely to be mistaken or ill-informed among many actions taken by their agent automatic monitoring in practice human users may not always have the time to go through the agent activity logs exposed by the system deployer at the speed or scale they desire to address this users or system deployers can set up a second monitoring ai system that automatically reviews the primary agentic system s reasoning and actions made legible as in section to check that they re in line with expectations given the user s goals this monitoring ai system could be a classifier or a generative ai system capable of producing its own chains-of-thought such automated monitors operate at a speed and cost that human monitoring cannot hope to match and may be able to parse modalities such as detecting adversarially-perturbed images that a human could not monitoring can be provided as a service by the system deployer or set up by the user in case they wish to exercise additional control monitoring however comes with substantial costs and can be a double-edged sword if sensitive agent session data needs to be retained by the system deployer it may create privacy risks for the user and by extension the system deployer depending on which party carries out monitoring even monitoring intended solely to benefit the user could give rise to concerns about how that information will be used and push users to self-censor a separate challenge is how to prevent monitoring functions from being over-extended and being used as a means of centralized control especially by authoritarian governments additionally monitoring poses some practical and technical challenges one challenge of such monitoring is that it adds costs if the same foundation model prompted differently is used to classify all text outputted by the primary model the total inference cost may be doubled or multiplied even further if the monitor model produces its own chain-of-thought for each primary model output smaller ai models may be useful for doing monitoring more cheaply but doing so raises the risk that the primary ai system could misbehave in ways the smaller model can t reliably catch on the other hand recognizing incorrect behavior may be easier than generating full behaviors that are correct making savings possible this may create pressure for system deployers in competitive markets to minimize monitoring measures in order to undercut each other on price there is also a recursive issue with ensuring the reliability and user-alignment of monitor systems if an input causes the primary agent to malfunction it may cause the monitoring system to malfunction too for example a malicious prompt injection that hijacks the primary system when read by the monitoring system could similarly apply to and hijack the monitoring system and prevent it from reporting the incident meaning that the harm will go entirely undetected while the ultimate value of automated monitoring remains uncertain given the potential utility of ais-monitoring-ais as a means of overseeing future high-risk ai systems it may be important to investigate methods that allow us to carry out monitoring effectively while appropriately balancing concerns related to privacy and centralized control open questions how can system deployers verify that a monitor is capable of successfully rooting out mistakes from the primary ai system particularly when they may have been trained in a similar manner or when the agent is substantially more intelligent than the monitor what types of human oversight of a primary ai system may still be useful when ai-based monitoring is also in use what key behaviors should monitoring systems monitor for that would otherwise break implicit safety practices such as agents undermining a user s blank slate assumption by storing information somewhere on the internet so that they can retrieve it in a future session how can automatic monitoring systems be made to monitor for as-yet-unknown harms attributability in cases where preventing intentional or unintentional harms at the level of the user or system deployer is infeasible such as a criminal operating an ai agent to scam a third party it may still be possible to deter harm by making it likely that the user would have it traced back to them with the creation of reliable attribution it could become possible to have reliable accountability one idea for such a system of attribution is to have each agentic ai instance assigned a unique identifier similar to business registrations which contains information on the agent s user-principal and other key accountability information it may be valuable to keep such agent identification optional and allow anonymity in many circumstances so as to limit potentially harmful surveillance of ai usage but in high-stakes interactions such as those involving private data or financial transactions third parties including external tool providers interacting with a user s agent could demand such identification before starting the interaction to ensure they know a human user can be held accountable if something goes wrong given the substantial incentives for bad actors to spoof such a system similar to the pressures that exist for identity-verification protocols in the financial industry making this system robust may be an important challenge such attribution for individual interactions does not cover everything in some cases ai agents may be used to cause harm to individuals who never had a chance to identify them e g agents assist- ingahackerindevelopinganexploit forwhichalternativeaccountabilityapproachesmaybeneeded open questions how can society practically enable ai agent identity verification what existing systems such as internet certificate authorities can be adapted to facilitate such verification what other ideas exist for practically enabling agentic ai system attributability interruptibility and maintaining control interruptibility the ability to turn an agent off while crude is a critical backstop for preventing an ai system from causing accidental or intentional harm system deployers could be required to make sure that a user can always activate a graceful shutdown procedure for its agent at any time both for halting a specific category of actions revoking access to e g financial credentials and for terminating the agent s operation more generally this graceful fallback is also useful in the event that agents crash such as due to internet outages there may be some cases where shutting a system down may cause more harm than good e g a malfunctioning agent that is nonetheless assisting in a life-threatening emergency but by investing in interruptibility one can minimize the scenarios in which users are stuck with only poor options ensuring graceful interruptibility is challenging when agents are terminated mid-action-sequence e g while scheduling a five-person business meeting when only two invites had been sent so far an important principle for addressing this could be to have agents always pre-construct a fallback procedure if they re turned off e g pre-launching a script that would automatically notify the two invitees that the agent has terminated and thus the meeting may not occur a significant challenge is how to maintain such fallback procedures as agents action-sequence complexities increase in certain circumstances it may even be that any graceful fallback procedure would itself require significant agentic behavior though perhaps by a separate ai agent a second important principle could be that an agent should not be able to halt or tamper with the user s attempt to shut them down as might be the case if the agent is malfunctioning or if the agent or its surrounding system has an instrumental goal of self-preservation it may be important for model developers or system deployers to deliberately design agentic systems to place shut down gracefully when requested by the user as a primary goal above whatever other goals 16for example it could reference the ai model powering the agent and any certifications it has received it could even include information about the datacenter and chip powering the agent for purposes of interruptibility section if there were a way for the user to be able to verify this information e g by way of datacenters signing agent outputs or even hardware-level signing 17similarly if an ai system is associated with a bank account into which it deposits or extracts funds authorities could track access to that account as a basis for identifying human principals the system was provided with though this may be infeasible in certain situations such as those in which an agent is assisting in a life-threatening emergency interruptibility requirements should likely also extend recursively to any other sub-aents the agent has initiated all spawned sub-aents should be gracefully terminatable by the original user sometimes a user may be unwilling or unable to shut down their ai system even as it causes harm this raises important challenges around how third parties communicate to a user that their agent is causing harm and in what circumstances an external party has the right or obligation to terminate a user s agent in cases where it is merited to be able to stop an agent quickly during an incident society could encourage redundancy in the number of human parties that can turn off an ai agent instance the two relevant parties are the system deployer and the data center operator or chip owner on whose hardware the ai system is running if an agentic ai system causes significant ongoing harm that they could have halted these parties could themselves bear some of the responsibility in order for such shutdowns to be viable the system deployer or chip operator may need to maintain awareness of roughly what agentic ai jobs they are running though this must be done with significant care to avoid harms to privacy it may even be desirable to automatically trigger such shutdowns if risk indicators cross a certain threshold like an influx of new jobs from unknown accounts similar to stock market circuit breakers that are triggered at a given threshold drop in price as ai systems levels of agenticness increase there is a risk that certain model developers system deployers and users would lose the ability to shut down their agentic ai systems this could be because no viable fallback system exists e g in a similar sense that no one can shut down the global banking system or the electric grid without very significant costs or because the agent has self-exfiltrated its code to facilities beyond its initiator s grasp we can begin to take steps that make this worst case scenario less likely by establishing the degree to which model developers system deployers and users will be held accountable for the harms caused by the agent even after human control has been lost this could incentivize them to develop stronger methods of control making the worst case scenario less likely open questions how can model developers and system deployers design their systems to ensure that agentic systems have graceful fallbacks in case they re shut down or interrupted for the broad range of actions an agent might take are there principles by which a second agentic ai system could be used as the fallback and where might this approach fail in what settings is interruptibility users responsibility rather than model developers or system deployers for instance should users be considered responsible for only approving an agent s action if it is coupled with a fallback procedure how can system deployers ensure that agents only spawn sub-aents that can be similarly turned off under what circumstances if any should an agent ever be able to or be incentivized to prevent its own termination what information should system deployers or compute providers keep track of such as agent ids as in section in order to help determine that a system they re hosting has caused significant harm and needs to be turned off how can such information be minimized to satisfy the strong need for user privacy what restrictions should exist on such shutdowns to prevent them from being abused to police harmless or low-stakes usage of agents how realistic is it for agentic ai systems to resist being shut down in the near-term how realistic is it for an agentic ai system to be integrated into a social process or critical infrastructure including unintentionally such that the cost of shutting it down would become prohibitive if either scenario did happen what are the likeliest pathways and what signals might be observed in the run-up by the system deployer and user or by outside parties that can be used to trigger intervention ahead of time howshoulddifferentparties responsibilitiesbeallocatedintheeventofthenon-interruptibility of an ai system that causes harm indirect impacts from agentic ai systems in addition to direct impacts from individual agentic ai systems there will be indirect impacts that result collectively from the usage of many different ai systems and society s reaction to their usage just as it would have been difficult to anticipate the full range of societal readjustments from previous general-purpose technologies like electricity and computers one should expect the unexpected still we do think there are several categories of indirect impacts from agentic ai systems that are likely to require active mitigation by society which we list below these indirect impacts may be addressed at least in part by adopting best practices for users system deployers and model developers such as those outlined in section however fully addressing these complex challenges will likely require additional strategies beyond this paper s proposals including through industry-wide collaborations and society-wide mitigations some strategies towards this end may be domain or risk-specific while others may involve placing general requirements on the usage of certain types of agentic ai systems adoption races given the advantages that agents may confer in competitive environments such as competition betweenprivatefirmsorgovernments theremaybesignificantpressureforcompetitors toadoptagenticaisystemswithoutproperlyvettingthosesystems reliabilityandtrustworthiness a key observation driving such premature reliance is that agentic ai systems may succeed at a task on average while being unreliable in rare but important cases which can be missed or ignored by competitors under pressure for example consider a hypothetical class of agentic ai code-generation systems that can rapidly writenewcode butwhosecodeoccasionallycontainsserioussecurityflaws ifasoftwaredevelopment company thinks their competitor has been using these coding systems without human supervision as a way to quickly build new features they may feel pressured to do the same without doing proper due diligence as they might otherwise lose market-share to their competitor as a result all firms codebases would now be vulnerable to serious cyberattacks even if each individual firm would ve preferred to go slower and thereby avoid this outcome this trend toward overrapid adoption even in high-risk domains can lead to over-reliance whereby humans trust agentic ai systems without fully understanding their limitations this could create the conditions for widespread use of unsafe ai systems that in the worst case may prove catastrophic labor displacement and differential adoption rates agenticaisystemsappearlikelytohaveamoresubstantiveimpactonworkers jobs andproductivity thanstaticaisystems traditionalaisystemsexcelatsomeroutinework butincreasingagenticness couldexpandwhattasksare routine enoughtobeassistedorautomatedbyai suchasbyadapting to unexpected conditions gathering relevant context and calibrating to a user s preferences this means they may expose a greater number of jobs and tasks to augmentation and automation similar to other axes of ai system improvement like tool use this could result in a range of different economic effects these could lead to substantive boosts in worker productivity and economic growth but could also result in the displacement of a large number of workers either because their jobs are fully automated or because their skills are made less rare and thus their jobs become more precarious at the same time agentic ai systems may improve education and enable workers to upskill into new jobs it is also possible that agentic ai systems can increase the agency and productivity of individual workers or small firms more than traditional ai systems have done such as by increasing the availability of previously rare expertise this may or may not offset large firms advantages in capital e g their ability to run more agents and preexisting market position such as firms with access to proprietary data that can be used to train bespoke agents even similarly-positioned individuals and firms may differ in their ability to leverage agentic ai systems different individuals jobs and firms business strategies may be more or less amenable to ai agent automation depending on the particular order in which each ai agent capability is unlocked and becomes reliable individuals who lack digital literacy technology access or representation in design decisions around agentic ai systems may find themselves less able to participate in an agentic-ai-system-fueled world however ai agents could also reduce the technology access gap much like smartphones increased internet access to underserved populations though some gaps remain all these effects may alter the job landscape and business environment unevenly and increase the importance of taking active policy measures to ensure the benefits of increasingly agentic ai systems are in fact shared broadly shifting offense-defense balances some tasks may be more susceptible to automation by agentic ai systems than others this asymmetry is likely to undermine many current implicit assumptions that undergird harm mitigation equilibria in our society known as offense-defense balances with unclear consequences for example in the cyber domain human monitoring and incident response is still key to cyber-attack mitigation the feasibility of such human monitoring is predicated on the fact that the volume of attacks is similarly constrained by the number of human attackers consider the hypothetical where agentic ai systems can substantially automate cyber-attacker responsibilities and thus dramatically expand the volume of attacks but cyber-defender responsibilities such as monitoring are much harder to automate in such a world the overall effect of agentic ai systems would be to make cyberdefense less viable and make information systems less secure conversely if agentic ai systems make monitoring and response cheaper than producing new cyberattacks the overall effect would be to make cyberdefense cheaper and easier while it is very difficult to anticipate the net effect of agent adoption dynamics in a particular domain in advance one can be confident that some processes will be much more amenable to automation than others and that numerous societal equilibria may shift as a result it behooves actors to pay close attention in identifying which equilibrium assumptions no longer hold and to 18anysuchoffense-defenseanalysisshouldalsoincludetheextenttowhichagentsthemselvesrepresentanewattack surface and thus could create new vulnerabilities that need to be secured quickly respond such as by investing in differential technological development towards defender- oriented technologies correlated failures agentic ai systems may bring unexpected failure modes and a particular risk arises when a large number of ai systems all fail at the same time or all fail in the same way these correlated errors can occur due to algorithmic monoculture the observation that ai systems trained using the same or similar algorithms and data can make them malfunction in similar ways there is already evidence that language models trained on similar data distributions suffer from similar vulnerabilities such as adversarial prompts that corrupt one system generalizing to corrupting other similarly trained systems similarly biases in common training datasets when used by many different model developers could expand the biased behavior of individual ai systems into a society-wide harm such as by all agents suppressing the same news article in recommendations or reinforcing stereotyped representations against the same social group more broadly ai systems may be vulnerable to disruption in shared infrastructure e g power or internet outages such correlated failures may be more dangerous in agentic ai systems as they could be delegated more power by humans and thus the potential consequences of their failure could be greater they may also be exacerbated because agentic ai systems may shape each others information environments and even directly communicate with each other allowing for much more direct and even deliberate propagation of certain failures it is particularly challenging to guard against such correlated failures because they are a joint function of the individual ai system and its constantly- changing environment one initial path forward is to create visibility and monitoring in the agentic ai ecosystem to catch such wide-scale issues as they emerge correlated failures may be particularly hard to deal with because they may overtax the fallback systems intended to remedy individual agents failures but which are unprepared for large-scale failures this may be especially acute in cases where the fallback plan is to have humans manually take over for each malfunctioning agent for example if a company s loan approval chatbot generally fails of the time and has a small number of staffers to handle those failures then a rare correlated failure that takes down of the chatbots would bring the loan-approval system to a halt however the rarity of this risk may make it difficult to discern from routine operation alone and thus could make it challenging for concerned employees inside the company to justify the cost of retaining adequate staff for such a seemingly hypothetical failure in the longer term if as certain human tasks are entirely replaced by agentic ai systems human expertise in certain domains may atrophy and make us entirely dependent on agentic ai systems and their attendant failure modes it may be particularly important for policymakers and the ai ecosystem to find ways to ensure that fallback mechanisms for agentic ai systems are robust to these sorts of correlated failures conclusion increasingly agentic ai systems are on the horizon and society may soon need to take significant measures to make sure they work safely and reliably and to mitigate larger indirect risks associated with agent adoption we hope that scholars and practitioners will work together to determine who should be responsible for using what practice and how to make these practices reliable and affordable for a wide range of actors and affordable agreeing on such best practices is also unlikely to be a one-time effort if there is continued rapid progress in ai capabilities society may need to repeatedly reach agreement on new best practices for each more capable class of ai systems in order to incentivize speedy adoption of new practices that address these systems greater risks acknowledgements we would like to thank seth lazar tim hwang chris meserole gretchen krueger rebecca crootof dan hendrycks gillian hadfield rob reich meredith ringel morris josh albrecht matt boulos laura weidinger daniel kokotajlo jason kwon artemis seaford michael kolhede michael lampe andrea vallone christina kim tejal patwardhan davis robertson hannah rose kirk ashyana-jasmine kachra and karthik rangarajan for their helpful advice feedback and comments which were integral in the development of this white paper references a costinot on the origins of comparative advantage journal of international economics vol p apr m campbell a j hoane and f -h hsu deep blue artificial intelligence vol p jan a chan r salganik a markelius c pang n rajkumar d krasheninnikov l langosco z he y duan m carroll m lin a mayhew k collins m molamohammadi j burden w zhao s rismani k voudouris u bhatt a weller d krueger and t maharaj harms from increasingly agentic algorithmic systems in acm conference on fairness accountability and transparency p june arxiv cs j cobbe m veale and j singh understanding accountability in algorithmic supply chains in proceedings of the acm conference on fairness accountability and transparency pp m c elish moral crumple zones cautionary tales in human-robot interaction pre-print mar w m landes and r a posner the economic structure of tort law harvard university press oct l b solum legal personhood for artificial intelligences mar d vladeck machines without principals liability rules and artificial intelligence washington law review vol p mar m anderljung j barnhart a korinek j leung c okeefe j whittlestone s avin m brundage j bullock d cass-beggs b chang t collins t fist g hadfield a hayes l ho s hooker e horvitz n kolt j schuett y shavit d siddarth r trager andk wolf frontier ai regulation managing emerging risks to public safety nov arxiv cs a chan r salganik a markelius c pang n rajkumar d krasheninnikov l langosco z he y duan m carroll et al harms from increasingly agentic algorithmic systems in proceedings of the acm conference on fairness accountability and transparency pp s yao j zhao d yu n du i shafran k narasimhan and y cao react synergizing reasoning and acting in language models arxiv preprint arxiv m r morris j sohl-dickstein n fiedel t warkentin a dafoe a faust c farabet and s legg levels of agi operationalizing progress on the path to agi nov arxiv cs j cobbe m veale andj singh understandingaccountabilityinalgorithmicsupplychains in acm conference on fairness accountability and transparency p june arxiv cs j shieh best practices for prompt engineering with openai api -best-practices-for-prompt-engineering-with-openai-api accessed -- a d saenz z harned o banerjee m d abrmoff and p rajpurkar autonomous ai systems in the face of liability regulations and costs npj digital medicine vol p oct m clancey and t besiroglu the great inflection a debate about ai and explosive growth the-great-inflection-a-debate-about-ai-and-explosive-growth accessed -- t eloundou s manning p mishkin and d rock gpts are gpts an early look at the labor market impact potential of large language models aug arxiv cs econ q-fin s r bowman eightthingstoknowaboutlargelanguagemodels apr arxiv cs anthropic challenges in evaluating ai systems evaluating-ai-systems accessed -- d bogdoll m nitsche and j m zllner anomaly detection in autonomous driving a survey in2022ieee cvfconferenceoncomputervisionandpatternrecognitionworkshops cvprw p june arxiv cs executive order on the safe secure and trustworthy de- velopment and use of artificial intelligence whitehouse gov briefing-room presidential-actions executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence accessed -- the bletchley declaration by countries attending the ai safety summit ai-safety-summit--the-bletchley-declaration the-bletchley-declaration-by-countries-attending-the-ai-safety-summit---november- accessed -- r crootof m e kaminski and w n price ii humans in the loop mar j zerilli a knott j maclaurin and c gavaghan algorithmic decision-making and the control problem minds and machines vol p dec h khlaaf toward comprehensive risk assessments and assurance of ai-based systems trail of bits v krakovna l orseau r ngo m martic and s legg avoiding side effects by considering future tasks in advances in neural information processing systems vol p curran associates inc s russell human compatible artificial intelligence and the problem of control penguin d hadfield-menell s j russell p abbeel and a dragan cooperative inverse reinforcement learning advances in neural information processing systems vol r shah p freire n alex r freedman d krasheninnikov l chan m d dennis p abbeel a dragan and s russell benefits of assistance over reward learning oct e perez s ringer k lukoit k nguyen e chen s heiner c pettit c olsson s kundu s kadavath a jones a chen b mann b israel b seethor c mckinnon c olah d yan d amodei d amodei d drain d li e tran-johnson g khundadze j kernion j landis j kerr j mueller j hyun j landau k ndousse l goldberg l lovitt m lucas m sellitto m zhang n kingsland n elhage n joseph n mercado n dassarma o rausch r larson s mccandlish s johnston s kravec s e showk t lanham t telleen-lawton t brown t henighan t hume y bai z hatfield-dodds j clark s r bowman a askell r grosse d hernandez d ganguli e hubinger n schiefer and j kaplan discovering language model behaviors with model-written evaluations m sharma m tong t korbak d duvenaud a askell s r bowman n cheng e durmus z hatfield-dodds s r johnston s kravec t maxwell s mccandlish k ndousse o rausch n schiefer d yan m zhang and e perez towards understanding sycophancy in language models j wei x wang d schuurmans m bosma f xia e chi q v le d zhou et al chain-of-thought prompting elicits reasoning in large language models advances in neural information processing systems vol pp k greshake s abdelnabi s mishra c endres t holz and m fritz more than you ve asked for a comprehensive analysis of novel prompt injection threats to application-integrated large language models arxiv preprint arxiv t lanham a chen a radhakrishnan b steiner c denison d hernandez d li e durmus e hubinger j kernion et al measuring faithfulness in chain-of-thought reasoning arxiv preprint arxiv a azaria and t mitchell the internal state of an llm knows when its lying arxiv preprint arxiv a zou l phan s chen j campbell p guo r ren a pan x yin m mazeika a -k dombrowski et al representation engineering a top-down approach to ai transparency arxiv preprint arxiv s yao d yu j zhao i shafran t l griffiths y cao andk narasimhan treeofthoughts deliberate problem solving with large language models arxiv preprint arxiv m besta n blach a kubicek r gerstenberger l gianinazzi j gajda t lehmann m podstawski h niewiadomski p nyczyk et al graph of thoughts solving elaborate problems with large language models arxiv preprint arxiv x qi k huang a panda m wang and p mittal visual adversarial examples jailbreak large language models arxiv preprint arxiv t chakraborti and s kambhampati how can ai bots lie a formal perspective on explana- tions lies and the art of persuasion w saunders c yeh j wu s bills l ouyang j ward and j leike self-critiquing models for assisting human evaluators arxiv preprint arxiv d lyon surveillance society monitoring everyday life mcgraw-hill education uk j w penney chilling effects online surveillance and wikipedia use berkeley technology law journal vol no pp e morozov the net delusion the dark side of internet freedom publicaffairs s willison you cant solve ai security problems with more ai g hadfield m -f t cullar and t oreilly its time to create a national registry for large ai models d leung b nolens d w arner and j frost corporate digital identity no silver bullet but a silver lining june r zwetsloot and a dafoe thinking about risks from ai accidents misuse and structure lawfare february vol p j ding and a dafoe engines of power electricity ai and general-purpose military transfor- mations european journal of international security vol no p s shoker a reddie s barrington m brundage h chahal m depp b drexel r gupta m favaro j hecla et al confidence-building measures for artificial intelligence workshop proceedings arxiv preprint arxiv c sterbenz and r trager autonomous weapons and coercive threats m c horowitz when speed kills lethal autonomous weapon systems deterrence and stability journal of strategic studies vol no pp d hendrycks m mazeika and t woodside an overview of catastrophic ai risks arxiv preprint arxiv a askell m brundage andg hadfield theroleofcooperationinresponsibleaidevelopment arxiv preprint arxiv j schneider the capability vulnerability paradox and military revolutions implications for computing cyber and the onset of war in emerging technologies and international stability pp routledge d hendrycks natural selection favors ais over humans arxiv preprint arxiv e tsetsi and s a rains smartphone internet access and use extending the digital divide and usage gap mobile media communication vol no pp j munga to close africa s digital divide policy must address the usage gap to-close-africa-s-digital-divide-policy-must-address-usage-gap-pub- accessed -- b schneier artificial intelligence and the attack defense balance ieee security privacy vol no pp j sandbrink h hobbs j swett a dafoe and a sandberg differential technology develop- ment a responsible innovation principle for navigating technology risks available at ssrn r bommasani k a creel a kumar d jurafsky and p liang picking on the same person does algorithmic monoculture lead to outcome homogenization j kleinberg and m raghavan algorithmic monoculture and social welfare proceedings of the national academy of sciences vol no p e2018340118 a zou z wang j z kolter and m fredrikson universal and transferable adversarial attacks on aligned language models arxiv preprint arxiv