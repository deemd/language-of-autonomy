In collaboration with Capgemini AI Agents in Action: Foundations for Evaluation and Governance W H I T E P A P E R N O V E M B E R 2 0 2 5 Images: Adobe Stock Contents Foreword 4 Executive summary 5 Introduction 6 1 Evolving technical foundations of AI agents 8 1.1 The software architecture of an AI agent 8 1.2 Communication protocols and interoperability 10 1.3 Cybersecurity considerations 12 2 Foundations for AI agent evaluation and governance 13 2.1 Classification 14 2.2 Evaluation 19 2.3 Risk assessment 22 2.4 G overnance considerations for AI agents: a progressive approach 25 3 Looking ahead: multi-agent ecosystems 29 Conclusion 30 Contributors 31 Endnotes 34 Disclaimer This document is published by the World Economic Forum as a contribution to a project, insight area or interaction. The findings, interpretations and conclusions expressed herein are a result of a collaborative process facilitated and endorsed by the World Economic Forum but whose results do not necessarily represent the views of the World Economic Forum, nor the entirety of its Members, Partners or other stakeholders. © 2025 World Economic Forum. All rights reserved. No part of this publication may be reproduced or transmitted in any form or by any means, including photocopying and recording, or by any information storage and retrieval system. AI Agents in Action: Foundations for Evaluation and Governance 2 November 2025 AI Agents in Action: Foundations for Evaluation and Governance Foreword Cathy Li Roshan Gya Head, Centre for AI Chief Executive Officer, Excellence, Member of Capgemini Invent the Executive Committee, World Economic Forum In recent years, organizations have moved beyond Through the AI Governance Alliance, the World predictive models and chat interfaces to experiment Economic Forum and Capgemini are advancing with artificial intelligence (AI) in more transformative this subject in collaboration with the AI community, ways. AI agents are now emerging as integrated signalling that now is the time to prepare for an collaborators in business, public services and agentic future. If adopters start small, iterate everyday life. The adoption of AI agents could carefully and apply proportionate safeguards, bring significant gains in efficiency, altered kinds of agents can be deployed in ways that amplify human-machine interaction and the advent of novel human capabilities, unlock productivity and digital ecosystems. establish a foundation for more complex multi-agent ecosystems to emerge over time. Unless a careful This transition faces multiple obstacles that need to be and deliberate approach to adoption is adopted, addressed. Moving from models to agents represents untested use cases could outpace oversight and more than a technical milestone and requires lead to misaligned incentives, emergent risks and organizations to rethink how they design, evaluate and loss of public trust. govern advanced agentic systems. Many companies are now questioning what agents can accomplish As with any transformative technology, the alongside the practical steps needed to adopt and opportunities presented by AI agents must be deploy them safely, responsibly and effectively. accompanied by a responsibility to guide their development and deployment with care. Through This paper was developed to help answer those cross-functional efforts and collaborative questions. By mapping the evolving foundations governance, AI agents can be integrated in ways of agentic systems, classifying their roles, that amplify human ingenuity, promote innovation identifying new ways to evaluate them and outlining and improve overall quality of life. This paper is a progressive governance approaches, the paper step in that direction, offering guidance to help early offers practical guidance for leaders navigating adopters navigate the complex and often uneven adoption in real-world contexts. path of AI agent adoption. AI Agents in Action: Foundations for Evaluation and Governance 3 Executive summary This paper explores the emergence of AI agents, outlining their technical foundations, classification, evaluation and governance to support safe and effective adoption. This report has been tailored mainly for adopters classification that differentiates agents by their role, of AI agents, including decision-makers, technical autonomy, authority, predictability and operational leaders and practitioners seeking to integrate AI context. Thirdly, it suggests a progressive governance agents into organizational workflows and services. approach that directly connects evaluation and safeguards to an agent’s task scope and While AI agents are gaining traction, there remains deployment environment. limited guidance on how to design, test and oversee them responsibly. This paper aims to help Together, these elements guide adopters with a fill that gap by providing a structured foundation for conceptual blueprint for moving from experimentation the safe and effective deployment of these systems. to deployment. The report highlights the importance of aligning adoption with evaluation and governance The paper makes three key contributions. Firstly, practices to ensure that AI agents are successfully it covers the technical foundations of AI agents, deployed while trust, safety and accountability including their architectures, protocols and security are maintained. considerations. Secondly, it offers a functional AI Agents in Action: Foundations for Evaluation and Governance 4 Introduction AI agents are shifting from prototypes to deployment, bringing both transformative opportunities and novel governance challenges. AI agents are gradually becoming embedded in today is the rise of data-driven models, particularly an increasing number of tasks, workflows and generative artificial intelligence (AI) and large language use cases that span cloud and edge computing, models (LLMs), which are enabling the emergence leading the way to more widespread adoption. of a new generation of LLM-based agents. These As the transition from prototyping to deployment systems can generate plans, simulate reasoning accelerates, current adoption remains concentrated and adapt their behaviour through feedback among early adopters. According to a recent global mechanisms in ways that were previously not survey of executives, 82% of organizations plan to possible. This evolution has sparked a new integrate agents within the next one to three years, wave of experimentation, with researchers and indicating that most efforts are still in the planning or companies rapidly creating prototypes of agents pilot phase,1 while moving towards wider adoption. in various fields. This report focuses mainly on LLM-based agents (“AI agents” is sometimes The concept of software agents has been studied used in short), whose growing capabilities create for decades in fields such as robotics, autonomous both significant opportunities for adoption and a systems and distributed computing. What is different new set of challenges in governance and safety. FIGURE 1 Foundations for the responsible adoption of AI agents 1 Technical foundations Lay the groundwork 2 3 Functional Evaluation and classification governance Define the Scale with agent’s role confidence AI Agents in Action: Foundations for Evaluation and Governance 5 LLM-based AI agents, for example, introduce new pillars across classification, evaluation, risk assessment risks such as goal misalignment, behavioural drift, and governance, which together form the foundation tool misuse and emergent coordination failures for a progressive approach to adoption and that traditional software governance models are deployment. Figure 1 presents the general content unable to manage. Unlike conventional software, of this report, which helps guide the responsible agents are increasingly assuming roles that adoption and deployment of AI agents. resemble those of human decision-makers rather than static tools. This means that governance The goal is to equip adopters, providers, technical models designed solely for access control and leaders, organizational decision-makers and other system reliability are no longer sufficient. A more stakeholders with a shared understanding of the useful comparison is the governance applied current state of agentic systems and emerging to human users, who must earn permissions, oversight practices. Building on established accountability and trust by demonstrating performance AI governance principles and frameworks, over time. Similarly, trust in AI agents can be such as those developed by the Organisation established by testing their behaviour against for Economic Co-operation and Development validated cases, running them in human-in-the- (OECD),2 National Institute of Standards and loop configurations and gradually expanding Technology (NIST),3 International Organization for autonomy only once reliability has been sufficiently Standardization (ISO)/International Electrotechnical demonstrated. In both cases, the principle of least Commission (IEC)4 and others, this paper privilege remains essential, with access limited to introduces additional principles addressing information and actions necessary for the task. autonomy, authority, operational context and systemic risk that extend existing governance This report aims to provide a forward-looking analysis guidance from an agent-focused lens. The of the evolving landscape of AI agents, focusing insights have been informed by working group on the capabilities, infrastructure, classification and meetings, workshops and extensive interviews with safeguards necessary for responsible deployment. members of the Safe Systems and Technologies To this end, it is structured around four foundational working group of the AI Governance Alliance. AI Agents in Action: Foundations for Evaluation and Governance 6 Evolving technical 1 foundations of AI agents The architecture, protocols and security models of AI agents dictate how they integrate into organizations and interact with the world. While the core architecture of AI agents is beginning with the same level of rigour as onboarding a new to take shape, practices for agent deployment, employee, including clearly defined roles, safeguards integration and governance remain nascent. As and structured oversight mechanisms. This section organizations begin to “hire” AI agents to support or outlines the technical foundations that enable agentic augment human teams, or perform tasks that impact systems and the architecture decisions that shape the physical world, adoption should be treated how they are built, deployed and governed. 1.1 T he software architecture of an AI agent Building agents The adoption of LLM-based agents by industry between models, tools, data sources and humans. requires not just marks a broader shift in software development from This layered setup introduces new complexity engineering but rigid, rules-based systems to more flexible, intent- in how agents behave, generalize and interact also orchestration driven interactions. For instance, in call centres, with their environment, reinforcing the need for and coordination early chatbots that followed scripted decision trees structured scaffolding. are now giving way to agentic systems capable between models, of understanding intent, managing context, and Today, AI agent architectures are organized into tools, data sources escalating decisions more dynamically. This evolution three interconnected layers, consisting and humans. towards agentic AI represents a fundamental change of application, orchestration and reasoning, in control and autonomy, where tasks traditionally which collectively enable intelligent, context- performed by humans are delegated to machines. aware and business-aligned automation. At a high level, agent architectures are designed to interface To enable this shift, AI agents draw on four with users and systems, coordinate complex tasks technological paradigms: using external tools and application programming interfaces (APIs), and support decision-making – Classical software: deterministic logic and rule- through a combination of language models, based execution reasoning modules and control logic. Together, these layers provide the technical foundation that – Neural networks: pattern recognition and underpins how agents operate. statistical learning The application layer, along with protocols such – Foundation models: general-purpose, adaptive as Model Context Protocol (MCP) and agent-to- systems that interpret instructions and act agent protocol (A2A), integrates the agent into contextually specific processes or user workflows. It receives input through user interfaces or APIs and translates it into – Autonomous control: mechanisms that enable structured signals. Application logic applies domain- systems to plan, coordinate and act with minimal specific rules and constraints to ensure the agent’s human oversight output (i.e. forecast, decisions, actions, messages, etc.) is aligned with user expectations and business As a result, building agents requires not just requirements. This layer can run in the cloud or on- engineering but also orchestration and coordination prem in edge computing equipment. AI Agents in Action: Foundations for Evaluation and Governance 7 Understanding The orchestration layer (framework layer) operate beyond traditional network boundaries, this architecture is governs how the agent interprets inputs, invokes introducing novel cybersecurity concerns. key to anticipating tools and coordinates tasks. While some LLM how agents will providers5 have integrated tools directly into their The reasoning layer underpins the agent’s ability solutions, this can create rigid and vendor-locked to generate, predict, classify or apply rules in engage with users systems. Agentic frameworks overcome this pursuit of its goals. Depending on the task, the and systems, by standardizing tools and systems integration, reasoning layer can draw on a range of models, coordinate remaining LLM-agnostic and spanning multiple including deterministic, rule-based approaches workflows and workloads across cloud and edge. This enables AI and classical machine learning, as well as small make context- agents to employ a range of reasoning strategies or large language models and other generative aware decisions. and support features, such as code execution or architectures. The choice of model shapes how search, and use protocols like MCP to connect the agent processes information, adapts to context with enterprise resources, including databases and ultimately carries out its assigned role. and customer relationship management (CRM) systems. Most agents also include specialized Figure 2 illustrates this layered architecture, showing sub-agents that handle distinct tasks, which makes how internal components across application, them functionally part of a multi-agent system. orchestration and reasoning work together to The orchestration layer is critical in this regard, as support dynamic agent behaviour while maintaining it coordinates sub-agents, assigns responsibilities secure boundaries across organizational systems. and manages dependencies between them. It also enables model switching, allowing organizations to In combination, these layers constitute the assign different models to various tasks based on technical backbone that governs agent their complexity, cost or performance. Importantly, functionality. For organizations implementing AI agents have a unique architecture that can AI agents, understanding this architecture is be extended beyond the organization’s security key to anticipating how agents will engage perimeter. Their ability to invoke external tools and with users and systems, coordinate workflows communicate with other agents enables them to and make context-aware decisions. FIGURE 2 Software architecture of an AI agent Internal organization resources Third-party resources Environment Event User Al agent boundary IT applications Percepts Actions Al agent CRM Messaging Input/output Application UI API Code Database Agentic framework MCP Al agent Orchestration Planning Memory Tools Workflow A2A Application Orchestration Models Reasoning Generative Non-generative Mechanistic Reasoning AI Agents in Action: Foundations for Evaluation and Governance 8 1.2 Communication protocols and interoperability MCP has gained The landscape of advanced LLM-based agents external data sources, APIs and enterprise widespread support is supported by new protocols that enable more systems through a standardized protocol. across leading seamless integration and collaboration. The MCP, Rather than developing bespoke integrations agent frameworks for example, aims to standardize the connection for each agent-task pairing, MCP allows agents between enterprise software systems, external to act as clients that request access to services and is increasingly data sources and agents, while protocols such via MCP-compliant servers. For example, an viewed as a core as A2A and the AGNTCY architecture’s agent agent using MCP can check a calendar, retrieve mechanism. connect protocol (ACP) offer tools to facilitate emails, update database content or update interaction between varying AI agents, forming the CRM records through a shared interface. interoperability layer for multi-agent systems (MAS). This significantly reduces friction, speeds up As these protocols are implemented across cloud deployment and supports modular plug-and-play platforms, enterprise networks and edge devices, capabilities across tools and environments. they are necessary for running agentic code while connecting with real-world sensor data and systems. MCP has gained widespread support across leading agent frameworks and is increasingly Introduced by Anthropic in late 2024, MCP6 viewed as a core mechanism for connecting enables agents to connect with internal or agents to the broader enterprise infrastructure. FIGURE 3 Illustration of MCP-based agent communication Overview of MCP AI agent 1 AI agent 2 MCP MCP MCP 1 client client client messaging database database User updates a record Send an email Read a record MCP MCP server 4 server messaging database Acknowledge update 3 2 MCP server updates the database Database update confirmed Messaging Database AI Agents in Action: Foundations for Evaluation and Governance 9 Where MCP focuses on communication between Released by Google in April 2025,7 A2A operates agents and external or internal systems, protocols through a common communication interface and like A2A enable agents to discover each other, introduces the concept of agent cards (similar to interact, collaborate and delegate tasks, whether model cards8), which are structured descriptions operating within an organization’s security perimeter of an agent’s identity, along with its capabilities or outside it. These protocols address a growing and skills. This allows for automatic discovery need in complex environments where multiple and coordination between agents and systems. agents work together across organizational or technical boundaries, enabling agents from different vendors to communicate effectively. FIGURE 4 Illustration of agent-to-agent communication protocol AI agent 1 AI agent 2 A2A protocol Agents Agents Agents card LLM LLM Agent framework Agent framework Task manager MCP A2A Artefact handler APIs and enterprise APIs and enterprise applications applications Beyond communication and discovery, new between agents. Strategy, privacy and security standards are also emerging that address how considerations often shape how and whether agents transact and exchange value. Released systems should be integrated and are important by Google in September 2025, the Agent for enterprises to carefully consider. Payments Protocol (AP2)9 enables secure, auditable transactions under user-defined For example, communication between different agents limits. Unlike MCP and A2A, which focus on could raise concerns about access control, data data exchange and task coordination, AP2 confidentiality or compliance across jurisdictions. addresses complex financial operations. Choosing whether to expose a capability to other agents becomes a governance decision as much Despite this progress, interoperability remains as a technical choice. a key challenge. Technical compatibility alone does not guarantee successful coordination AI Agents in Action: Foundations for Evaluation and Governance 10 1.3 Cybersecurity considerations Security As AI agents move into enterprise and consumer- While protocols such as MCP and A2A can strategies have facing environments, they extend rather than streamline integration, they also expand the attack evolved from replace existing security challenges. Security surface11 by introducing new external dependencies perimeter defences strategies have evolved from perimeter defences and interfaces, as illustrated in Figure 2. The very to layered “defence in depth,” and more recently interoperability that enhances agent capabilities to layered “defence to the zero-trust model.10 These changes reflect also exposes enterprises to unpredictable inputs in depth” and more broader transformations such as cloud adoption, and vulnerabilities from third parties. For adopters, recently to the distributed workforces and interconnected this means that every agent interaction should be zero-trust model. ecosystems, all of which have already weakened treated as untrusted by default, and that verifying the notion of a clear boundary between internal identity, permissions and context is necessary and external networks. Agents build on this before granting access. trajectory but add additional layers of risk that must be managed proactively. Finally, agents can be misused.12 They might be exploited through design flaws or prompt injections, By autonomously invoking tools and communicating or even intentionally deployed for malicious purposes, across organizational lines (e.g. via MCP and such as accessing private data or spreading A2A), agents embed external services, databases misinformation. Unlike traditional attacks, autonomous and peer agents into enterprise workflows. This agents can act with speed and persistence, making multiplication of identities and connections makes attribution and accountability harder. Organizations identity management, micro-segmentation and should prepare for this by implementing strong ongoing verification of agent activity essential. audit trails, incident response plans and clear accountability structures. AI Agents in Action: Foundations for Evaluation and Governance 11 Foundations for AI agent 2 evaluation and governance A structured foundation for evaluating and governing AI agents enables consistent assessment and oversight across contexts. Systematic As AI agents mature and adoption increases, a performance, identifying risks and establishing classification is functional understanding of their roles and properties governance mechanisms that scale with an important because is beginning to take shape. Rather than classifying agent’s autonomy, authority and function. it provides a agents solely by modality (e.g. text, speech, vision) or domain (e.g. customer service, decision support, To address classification, evaluation, risk assessment common basis for workflow orchestration), it is more effective to and governance, it is useful to distinguish between comparing agents, evaluate them according to their intended purpose, two main stakeholder perspectives:13 anticipating risks core properties and operating context. This approach and linking creates a clearer foundation for assessing impacts – Provider: Refers to organizations or individuals evaluation and and designing safeguards that are proportionate to that supply AI systems, platforms or tools. Their governance. an agent’s role. Systematic classification is important responsibilities include ensuring that products because it provides a common basis for comparing are developed and maintained in accordance agents, anticipating risks and linking evaluation with responsible and ethical guidelines, and and governance decisions to the realities of how that the necessary documentation and support an agent operates. Without it, oversight risks may are provided. become inconsistent, reactive or disconnected from an agent’s actual capabilities and environment. – Adopter: Refers to individuals within an organization who use AI systems, encompassing To establish this foundation, this report responsibilities such as procurement and introduces four foundational pillars which, deployment. Procurement involves the in combination, provide a structured responsibility of acquiring AI solutions for approach to assessment and adoption: organizational use by conducting due diligence and ensuring that all AI agent solutions comply – Classification: Establish the agent’s with organizational policies and regulatory characteristics and operational context requirements. Deployment is the responsibility to inform downstream assessment. for implementing AI systems in accordance with documented requirements and plans, while – Evaluation: Generate evidence of performance ensuring that risks and impacts of the AI agent and limitations in representative settings. are properly assessed and managed. – Risk assessment: Analyse potential harm The adopter depends on the provider for using classification and evaluation as inputs. transparent documentation, model and system specifications, and sufficient performance and risk – Governance: Translate classification, information to support responsible deployment and evaluation and risk assessment results oversight throughout the system life cycle. into safeguards and accountability proportionate to the agent’s profile. The four pillars form a continuous and parallel progression in which classification provides These foundations apply to diverse AI agents, structure, evaluation establishes evidence, risk encompassing both virtual and embodied assessment identifies and mitigates potential systems in different operational contexts. harms, and governance translates those insights They provide a consistent basis for assessing into safeguards and accountability. AI Agents in Action: Foundations for Evaluation and Governance 12 FIGURE 5 Foundations for AI agent evaluation and governance Classification dimensions Evalution criteria Risk assessment life cycle Progressive governance practices 2.1 C lassification Classification defines an agent’s characteristics Role reflects the breadth of tasks an agent and operating context to guide evaluation, risk can perform. Specialized agents are narrowly assessment and governance. focused and optimized for specific domains, while generalized agents can adapt across domains to To support evaluation and risk assessment, agents address a broader range of tasks or challenges. can be described across a set of dimensions that For instance, a tax-filing agent designed only capture both their internal characteristics and the to prepare returns is specialized, whereas a external contexts in which they operate. These personal digital assistant that manages scheduling, dimensions provide a structured approach to email drafting and online search operates as a analyse and compare agents across applications, generalist agent. ensuring clarity about their design choices and real- world effects. Predictability describes the stability and repeatability of agent behaviour. Deterministic In combination, the proposed dimensions define agents produce consistent, identical outputs how an agent operates, what actions it is permitted when given the same inputs, which makes their to take and the complexity of the context it is performance highly predictable and easier to deployed in. The agent’s overall impact can be seen validate. Non-deterministic agents, by contrast, as a profile that emerges from the interaction of may evolve, learn or generate variable outputs these dimensions, reflecting the benefits or risks of over time.15 This variability can support creativity, its application in practice.14 adaptation and exploration, but it reduces the reliability of producing identical results under Function refers to the specific role, purpose or identical conditions. For adopters, predictability set of tasks the agent is designed to perform. determines how much confidence they can place It describes what the agent does in practice, in an agent’s outputs, how reproducible those independent of the environment it is deployed outputs are, and what level of oversight is required in. For example, a coding co-pilot that generates to manage variability in practice. software snippets and a triage assistant that prioritizes patients in an emergency department Autonomy captures the degree to which an agent have distinct functions, even though both operate in can define and pursue objectives. The spectrum digital workflows. ranges from simple command-response systems to AI Agents in Action: Foundations for Evaluation and Governance 13 Establishing agents capable of planning and executing actions Autonomy and authority can be combined in levels of independently across authorized environments. different ways, depending on an agent’s purpose autonomy can help Autonomy in this context refers to an agent’s capacity and design. They are not inherent system properties organizations set to decide when and how to act toward a goal, but design choices that can be made based on the adapting to changing conditions without human agents’ intended functions, risk considerations and clear expectations guidance. Automation, on the other hand, refers to oversight requirements. They can also be calibrated for functionality systems that execute predefined functions reliably during assessment or adjusted in real time. and implement under specified conditions without human intervention. proportionate The key distinction is that autonomy entails decision- Operational context refers to the use case and governance making flexibility (i.e. choosing what to do), whereas environment in which the agent operates. The mechanisms. automation emphasizes execution reliability (i.e. doing environment is especially critical, as it determines what the system is programmed to do). observability, predictability of outcomes, interaction with other agents and how conditions evolve In the automotive sector, SAE International’s over time.17 Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Use case defines the domain and environment Vehicles16 framework defines driving automation where the agent performs its distinct function from Level 0 (no automation) to Level 5 (full for stakeholders. For example, an autonomous automation). A similar spectrum can be applied cleaning agent in the residential sector performs to AI agents. This spectrum can be conceived household vacuuming and floor cleaning as part of as moving from no autonomy (for example, a of routine home maintenance. simple chatbot that only answers user queries) to full autonomy (for example, a customer service Environment represents the operating conditions agent that automates interactions, resolves queries the agent functions under, ranging from simple and personalizes responses using a company’s and predictable settings to complex, uncertain knowledge base). Establishing levels of autonomy and dynamic contexts. A complex environment can help organizations set clear expectations is one where the agent navigates and acts under for functionality and implement proportionate uncertainty, with incomplete or noisy information, governance mechanisms. unpredictable outcomes, changing conditions over time, continuous ranges of possible actions or states, Authority defines the actions an agent is permitted and interactions with other agents whose behaviour to take. It sets the boundaries of system access, also affects results. By contrast, a simple environment such as permissions to use tools, interact with is one where the agent operates with complete databases or execute transactions. Like autonomy, information, predictable and static outcomes, authority exists on a sliding scale, from read-only independent episodes, a finite set of states or access to full administrative control. actions, and no need to consider other actors. FIGURE 6 Classification dimensions Agent characteristics Operational context 1. Function 6. Use case What does the agent do? Application domain and environment where the agent performs its function 2. Role 7. Environment Specialist Generalist Simple Complex 3. Predictability Deterministic Non-deterministic 4. Autonomy Low High 5. Authority Low High AI Agents in Action: Foundations for Evaluation and Governance 14 An example of an operational context could mitigated by adjusting agent parameters, like be a fraud detection agent in online banking. autonomy and authority, and/or by constraining The agent accesses transaction data and user the context in which the agent operates. Examples history, but cannot fully observe external factors include limiting a robot to a controlled zone or like user intentions or hidden fraud tactics. It confining a software agent to a sandbox. functions stochastically, with outcomes influenced by unpredictable variables such as varying An AI agent’s role, autonomy, authority, predictability fraud methods or user behaviours, rather than and operational context collectively shape its For organizations guaranteed results. The setup is sequential, overall impact, defined as the degree of benefit refining risk assessments with each detection. or harm it may generate. Highly autonomous, adopting AI, Operating in a fast-changing environment, authorized and non-deterministic behaviour in a understanding it requires continuous monitoring by human complex operational context may deliver strong and clearly defining reviewers and other security systems. performance but also carry greater risks. the operational context is For organizations adopting AI, understanding The following example illustrates how these essential to ensure and clearly defining the operational context dimensions can be applied in practice through effectiveness in is essential to ensure effectiveness in actual the classification of a basic AI agent, a robot actual deployment. deployment settings. Potential issues can be vacuum cleaner. AI Agents in Action: Foundations for Evaluation and Governance 15 CASE STUDY 1 Robot vacuum cleaner – classification Robot vacuum cleaner Agent characteristics Operational context 1. Function 6. Use case Autonomous indoor navigation and cleaning floors A home vacuum robot operates in the household services domain, autonomously navigating a residential environment to clean floors for occupants. 2. Role Specialist Generalist 7. Environment 3. Predictability Simple Complex Deterministic Non-deterministic 4. Autonomy Low High 5. Authority Low High Robot vacuum cleaner – classification – Autonomy: Medium – it operates independently within mapped areas of the home’s floorplan. – Function: The primary function is autonomous indoor – Authority: Low – it is limited to sensing, movement navigation and cleaning, interpreting spatial layouts, avoiding and vacuuming. obstacles and adapting to changing floor conditions. While it operates autonomously within mapped areas and Operational context schedules, it does not make decisions that affect other systems or safety-critical outcomes. Use case: Home vacuuming – Role: Specialist – it only does one specific job, vacuuming the floors. Environment: Moderate – the environment is primarily household environments with occasional dynamic obstacles. – Predictability: Deterministic – it follows specific instructions and task planning but may follow unspecified routes. AI Agents in Action: Foundations for Evaluation and Governance 16 As agents become more embedded in tools, – Guide governance and oversight: Align platforms and workflows, the proposed dimensions safeguards, controls and monitoring can help organizations define specific agent roles mechanisms with the nature and complexity of and levels of integration while evaluating benefits the agent’s role. and limitations in context and implement oversight mechanisms that match their capabilities. Taking – Support interoperability and scaling: these dimensions into consideration can help Structure agent types in ways that facilitate providers and adopters to: coordination in multi-agent environments and integration across systems. – Clarify functional scope: Define what an agent is designed to do, under what conditions and Without clear classification, organizations may where its responsibilities begin and end. adopt AI agents without fully understanding what they are designed to do, how they operate, the – Support assessment: Evaluate the technical, impact they may have on their environment or organizational, safety and security implications the oversight mechanisms they require. This lack of deploying specific agents in their contexts. of clarity could result in gaps in safety, security, control, privacy, reliability and accountability. FIGURE 7 Foundations for AI agent evaluation and governance – classification dimensions Progressive governance practices Access Traceability & Legal & Long-term Testing & control identity compliance management validation Trustworthiness & Monitoring & Manual Human And more... explainability logging redundancy oversight Define Classification Evaluation Risk assessment the use dimensions criteria life cycle Tool call Function Predictability Capabilities Define context Evaluate risks success Task success Edge case Role Use case Identify risks Manage risks rate robustness Task completion Autonomy Environment Trust indicators Analyse risks time Authority Error types And more... AI Agents in Action: Foundations for Evaluation and Governance 17 2.2 E valuation Robust evaluation is crucial for assessing providing real-world measures of reasoning, agent performance and limitations across code modification and system integration22 diverse contexts. – HCAST: Compares agent performance to As organizations begin deploying agents with human developers in areas such as programming different functional roles, the need for structured tasks, offering calibrated insights into agent evaluation becomes more important. This section coding capabilities, for example23 explores how evaluation methodologies are evolving to reflect this growing complexity. Although these emerging benchmarks offer valuable signals, they are typically built for academic or Agent “evaluation” refers to the measurement of an research settings, where tasks are predefined, AI agent’s performance and operation in representative environments are static and outcomes are often contexts, generating evidence about how well it deterministic. They rarely capture operational achieves intended functions, under what conditions realities such as ambiguous success criteria or and with what limitations. This means that robust dynamic workflows. evaluation frameworks are essential for building trust in AI agents’ performance. By providing clear, Evaluation requires clear performance metrics that multidimensional assessments of agent capabilities capture both task-level and system-level outcomes. and limitations, evaluations can help organizations Examples include task success rate, completion develop appropriate expectations and confidence time, error types, tool call success, throughput, in agentic systems. robustness against edge cases and user trust indicators. These metrics help establish whether While the evaluation of foundation models such the system delivers its functions reliably and provide as LLMs is supported by a rich landscape of the operational evidence that later informs risk standardized benchmarks,18,19,20 agent evaluation assessment and governance decisions. remains nascent. Unlike static model testing, agents operate as orchestrated systems that combine tool Providers benchmark systems to assess technical use, memory, decision-making and user interaction, maturity, while procurers and deployers are which exceed the scope of traditional benchmarks. responsible for ensuring that agents operate In response, several agent-specific capability safely and compliantly within specific industry, benchmarks have begun to emerge: organizational and operational contexts. Therefore, deployment environments provide the most – AgentBench: Tests agents in interactive accurate ground truth, but deployers often lack the environments like web browsing and games, resources to design comprehensive benchmarks. In and is useful for evaluating real-time decision- many cases, this makes collaboration with providers making and adaptability21 essential to establishing meaningful metrics. – SWE-bench: Evaluates an agent’s ability to An effective provider-focused evaluation should begin resolve GitHub issues in open-source repositories, with a technical screening of baseline capabilities, AI Agents in Action: Foundations for Evaluation and Governance 18 An effective such as reasoning, planning and tool use. Once Emerging evaluation tools are increasingly applied provider-focused validated in sandbox environments that mirror in enterprise settings to support the continuous evaluation should real-world tasks, agents may progress to controlled assessment of agentic systems, helping to track begin with a deployment, where they are integrated into workflows reasoning, compare outcomes to expectations technical screening under close monitoring, with safeguards in place to and detect anomalies that are overlooked by confirm that they align with human or established traditional testing. Major cloud providers have also of baseline decisions. Full deployment should only follow once started embedding such frameworks into their AI capabilities, such as reliability has been demonstrated, with fallback platforms, highlighting the importance of deployer- reasoning, planning mechanisms and defined human oversight. Audit side evaluation for adoption. and tool use. logs are central throughout this life cycle, providing structured records of agent activity and the rationale By approaching evaluation as a structured, context- behind it. Audit logs also support governance aware and continuous process, organizations can by enabling oversight and accountability, aiding more effectively determine whether an agent is fit debugging by tracing errors and points of failure, for deployment. and helping inform evaluation. To illustrate how these principles apply in practice, The following principles support this life cycle of the following illustration examines a coding co-pilot agent evaluation: agent. The illustration applies the evaluation dimensions from a deployer’s perspective, showing – Contextualization: Reflect the tools, how task-level and system-level metrics can be workflows and edge cases the agent will used to assess reliability, safety and overall encounter in practice. performance in an operational setting. – Multidimensional assessment: Define success Effective evaluation depends on close collaboration across various factors, including accuracy, between providers and adopters, where transparent robustness, latency tolerance, compliance, documentation, model specifications and performance and user trust. reports from providers enable deployers to validate reliability, identify risks and apply safeguards – Temporal and behavioural monitoring: Track throughout the system life cycle. performance over time to detect regressions, shifts in behaviour, or failures to adapt to The results form an integrated performance evolving inputs. profile that informs subsequent risk assessment and governance. FIGURE 8 Foundations for AI agent evaluation and governance – evaluation criteria Progressive governance practices Access Traceability & Legal & Long-term Testing & control identity compliance management validation Trustworthiness & Monitoring & Manual Human And more... explainability logging redundancy oversight Define Classification Evaluation Risk assessment the use dimensions criteria life cycle Tool call Function Predictability Capabilities Define context Evaluate risks success Task success Edge case Role Use case Identify risks Manage risks rate robustness Task completion Autonomy Environment Trust indicators Analyse risks time Authority Error types And more... AI Agents in Action: Foundations for Evaluation and Governance 19 CASE STUDY 2 Coding co-pilot – evaluation Coding co-pilot Agent characteristics Operational context 1. Function 6. Use case Assists human developers with code generation and debugging A coding co-pilot operates in the software development domain, assisting programmers within their coding environment by generating, completing and debugging 2. Role code to improve productivity and reduce errors. Specialist Generalist 7. Environment 3. Predictability Simple Complex Deterministic Non-deterministic 4. Autonomy Low High 5. Authority Low High Coding co-pilot – evaluation – Robustness: Exposing the agent to ambiguous or conflicting code to assess recovery, error handling and adaptability Evaluation starts with controlled tests in development environments to verify productivity gains while ensuring – Human trust: Gathering user feedback on reliability safety, reliability and compliance. Evaluation follows several and usefulness key steps including: – Monitoring: Using continuous logging to detect performance – Contextualization: Testing across coding tasks such as drift, anomalous tool use or regressions after deployment code generation, debugging and documentation to reflect real workflows – Performance: Measuring task success rate, completion time and error frequency, along with system metrics like tool-call success AI Agents in Action: Foundations for Evaluation and Governance 20 2.3 R isk assessment Risk assessment identifies and analyses Risk assessment draws on an agent’s defined potential harms, linking evaluation results classification dimensions to identify and analyse to oversight. potential risks, considering factors such as cybersecurity threats, safety hazards, operational Evaluation establishes how the system performs, vulnerabilities, legal and regulatory requirements, whereas risk assessment determines whether the and stakeholder impacts. It also incorporates agent and its use present risks that need to be evidence from evaluation activities, such as understood, assessed and mitigated. Evaluation sandbox testing and pilot deployments, including provides evidence as to whether the set mitigations task success rates, error patterns and robustness. are effective and met in implementation. To make this process operational, organizations can The goal of risk24 assessment is to identify, analyse follow a five-step life cycle that can be scaled to the and prioritize the ways an agent could fail or be complexity of the use case. misused, estimate likelihood and severity, and determine whether it can operate within acceptable The life cycle outlined in Figure 9 links the boundaries with appropriate controls. This applies outputs of classification and evaluation directly to single agents and multi-agent systems, software- to risk management and progressive governance based and embodied deployments, and covers practices. The following, Table 1, provides an both technical and organizational vulnerabilities. example of how the risk assessment process can be structured in practice.25 FIGURE 9 Foundations for AI agent evaluation and governance – risk assessment life cycle Progressive governance practices Access Traceability & Legal & Long-term Testing & control identity compliance management validation Trustworthiness & Monitoring & Manual Human And more... explainability logging redundancy oversight Define Classification Evaluation Risk assessment the use dimensions criteria life cycle Tool call Function Predictability Capabilities Define context Evaluate risks success Task success Edge case Role Use case Identify risks Manage risks rate robustness Task completion Autonomy Environment Trust indicators Analyse risks time Authority Error types And more... AI Agents in Action: Foundations for Evaluation and Governance 21 TABLE 1 Risk assessment life cycle for AI agents Step Objective Example activities Example outputs 1. Define context Establish the scope of the – Determine internal and external context – Context definition assessment, system boundaries, (strategic goals, legal framework, – Risk management plan objectives and criteria for stakeholders) managing risk – Risk evaluation criteria – Define boundaries, intended use, assumptions – Establish risk criteria (likelihood, impact scales, acceptance threshold) 2. Identify risks Identify potential technical, Brainstorm, workshops, risk identification – Risk register listing risks, organizational and ecosystem (e.g. hazard identification, threat causes, impacts risks, harms and affected parties identification, etc.), identification of sources of risk, causes, failure mode analysis 3. Analyse risks Understand the nature, likelihood – Assess probability and impact – Risk analysis scores and consequence of each risk (considering, for example, characteristics showing likelihood impact and quantify them like autonomy and authority, ratings and rationale predictability and operational context) – Identify existing controls or guardrails; apply qualitative or quantitative methods for risk estimation; use evaluation results to inform likelihood and impact 4. Evaluate risks Compare analysis results with – Rank and prioritize risks – Risk ranking summary risk criteria to determine priority – Use evaluation results for quantifying – Risk acceptance and tolerability and prioritizing risks evaluations – Use performance metrics and test confidence to inform risk thresholds 5. Manage risks Implement risk response actions – Assign owners of preventive, detective – Control actions (avoid, mitigate, transfer, accept) and response controls – Implementation plan and monitor risks – Evidence these controls through – Residual risk profile evaluation results – Risk assessment report – Address emerging risks as systems evolve or context changes – Evidence logs – Integrate feedback loops for – Monitoring reports continuing monitoring – Revised frameworks – Coordinate incident response and impact mitigation – Improved processes – Update governance and controls based on lessons learned Defining clear risk criteria and tolerability thresholds, reliability, robustness and observed error rates. and applying them consistently to prioritize and This relationship establishes a clear connection evaluate risks, remains a central challenge in AI between how an agent is designed, how it performs risk management. and how risks are managed, providing the basis for proportionate governance and oversight. The identification, analysis and evaluation of risks are directly linked to the classification dimensions Applying this approach in practice helps introduced earlier, allowing organizations to demonstrate how structured risk assessment understand how factors such as autonomy, authority, translates classification and evaluation evidence predictability and environmental complexity shape into measurable controls. The following example overall risk levels for AI agents. Inherent risk illustrates the risk assessment process in the combines likelihood and impact, while residual risk context of an autonomous vehicle. reflects the effectiveness of applied mitigations, informed by evaluation evidence such as system AI Agents in Action: Foundations for Evaluation and Governance 22 CASE STUDY 3 Autonomous vehicle – risk assessment Autonomous vehicle Agent characteristics Operational context 1. Function 6. Use case Performs the complete driving task without human control An autonomous vehicle operates in the transportation domain, navigating public or private road environments to transport passengers or goods safely and efficiently 2. Role without direct human control. Specialist Generalist 7. Environment 3. Predictability Simple Complex Deterministic Non-deterministic 4. Autonomy Low High 5. Authority Low High Autonomous vehicle – risk assessment Quantitative scoring combines these factors and is weighted according to the vehicle’s autonomy and authority levels. Risk assessment focuses on identifying and mitigating possible Mitigation measures may include redundancy and diversity failures across perception, decision-making and control systems. in critical sensors, reduction of autonomy or authority Key risk areas include sensor malfunction, data drift, adversarial thresholds, anomaly detection mechanisms and real-time interference and coordination failures with other vehicles or incident reporting. Residual risk is evaluated after these infrastructure that could lead, for example, to loss of steering safeguards are applied, drawing on evidence from controlled or braking control and eventual collisions. testing, field trials and continuous monitoring. The results Each risk is analysed for its likelihood (for example, the frequency determine whether the system can safely progress to wider of sensor failure leading to braking failure) and its impact (for deployment or requires additional control layers. example, the severity of injury, fatality or legal consequence). AI Agents in Action: Foundations for Evaluation and Governance 23 Risk assessment should be treated as a continuous, a control plan with clear ownership and verification iterative process rather than a single checkpoint. and validation steps, operating limits and monitoring Ongoing monitoring, regression testing, periodic requirements, and a deployment status. These reassessment and incident reviews are essential to outputs feed directly into progressive governance, maintaining alignment as agentic systems evolve. The ensuring oversight scales in line with an agent’s outputs of this process should include a risk register, demonstrated risk profile and operating context. 2.4 G overnance considerations for AI agents: a progressive approach Governance Progressive governance approaches scale Across these levels, governance mechanisms levels are informed oversight and safeguards in proportion to the advance in both scope and sophistication. The by risk assessment autonomy, authority and complexity of the agent. focus shifts from operational safeguards to outcomes, ensuring comprehensive risk management, with early that controls scale Evaluation and risk assessment provide critical levels emphasizing reactive measures, while more insights into an agent’s capabilities, performance, advanced levels incorporate proactive monitoring, with demonstrated reliability, security, safety and alignment. Governance, accountability frameworks and systemic autonomy, authority however, determines whether those insights translate risk assessments. and contextual into effective oversight and responsible adoption. complexity. “Governance” refers to the structured application of This progression is evident across key areas such technical safeguards and operational, ethical and as monitoring, accountability, risk management, organizational processes intended to ensure agents transparency, adaptability and scope. Monitoring remain within acceptable risk boundaries over time. evolves from basic logging to real-time, AI- As agents become more capable and integrated into assisted oversight, incorporating the automated core workflows, governance must evolve from basic analysis of logs to detect anomalies and precautionary measures to dynamic, multi-layered deviations in system behaviour. In parallel, risk systems of control and accountability. Governance management advances from static checklists to levels are informed by risk assessment outcomes, dynamic, predictive modelling, while the scope ensuring that controls scale with demonstrated of governance expands from narrow, task- autonomy, authority and contextual complexity. specific oversight to consideration of broader ecosystem impacts. A progressive set of governance levels can be distinguished, ranging from baseline safeguards to Operational environments are dynamic, and enhanced controls and systemic risk management. effective governance often requires recalibrating These levels correspond to the agent’s classification autonomy and authority in real time. The profile, which is linked to its function, predictability, following example illustrates this through autonomy, authority and operational context. a personal assistant agent, whose level of Oversight, therefore, intensifies as agents move autonomy and authority is dynamically adjusted from narrow, low-risk applications to complex, high- to ensure ongoing compliance. impact environments. AI Agents in Action: Foundations for Evaluation and Governance 24 CASE STUDY 4 Personal assistant – governance considerations Agent characteristics Agent characteristics Operational context 1. Function 6. Use case Assists users by organizing schedules, managing communication It operates in the personal productivity domain, and coordinating managing tasks, communications and information across a user’s digital environment to support daily coordination 2. Role and decision-making. Specialist Generalist 7. Environment 3. Predictability Simple Complex Deterministic Non-deterministic 4. Autonomy Low High 5. Authority Low High Personal assistant – governance Key governance risks include data overreach, privacy considerations violations, prompt manipulation and unauthorized actions such as unintended communication. Governance focuses on scaling oversight in line with the Mitigation measures include least-privilege access, consent- personal assistant’s autonomy, authority and environmental based data sharing, input and output filtering, audit logging, complexity. Unlike narrow task agents, a personal assistant and human approval for sensitive actions. Adaptive controls operates across multiple platforms such as email, calendars, should reduce permissions upon detecting anomalies or messaging and enterprise tools, raising questions about the policy breaches, supported by continuous monitoring and extent of information it can access, interpret and act upon on incident reporting. behalf of the user. As integration deepens and authority expands (e.g. from drafting messages to sending them, or booking travel) governance mechanisms must increase. AI Agents in Action: Foundations for Evaluation and Governance 25 The example illustrates that an agent’s overall impact a human-in-the-loop (HITL) configuration ensures emerges from the interaction of multiple dimensions that agents can suggest or prepare actions, but final across function, role, predictability, autonomy, decisions remain subject to explicit human approval. authority and context. As these dimensions shift, In more stable or clearly defined environments, a so does the risk profile, reinforcing the need for human-on-the-loop (HOTL) configuration allows governance frameworks that are both progressive agents to act within defined boundaries, while and adaptive. humans monitor behaviour, receive alerts and retain the ability to intervene or override when necessary. Effective governance requires maintaining an Integrating these oversight models into governance appropriate level of human oversight in relation to structures helps maintain accountability and the agent’s autonomy, authority and operational human judgment as agents operate with greater context. In high-risk or less predictable settings, independence and scale. TABLE 2 Baseline governance mechanisms for AI agents Governance area Foundational mechanism Purpose Enforce least-privilege access; define Prevent each agent from accessing unnecessary task boundaries. data, systems, or tools; reduce risk of misuse or accidental harm. Access control Conduct a data protection impact assessment Ensure data handling and processing complies with (DPIA); perform privacy and regulation compliance relevant laws and regulations. checks, such as General Data Protection Regulation or the California Consumer Privacy Act (CCPA). Legal and compliance Perform sandbox runs or controlled pilots with Validate expected behaviour, detect errors and prevent non-production data; install input-output filters; untested code from affecting live systems, conduct perform third-party audits. audits (code, red teaming, etc.). Testing and validation Implement logging for all agent actions; set up Maintain traceability for accountability; enable anomaly alerts or dashboards. early detection, incident response and post- incident analysis. Monitoring and logging Define and assign oversight models, including Ensure accountable human control for material HITL/HOTL. Require policy review before decisions, keep behaviour aligned with organizational deployment and set supervisory triggers policies and provide escalation paths when the agent for exceptions. acts unexpectedly. Human oversight Assign unique agent identifiers; tag outputs to the Attribute actions and outcomes to specific agents; responsible agent instance. enable forensic review and performance tracking. Traceability and identity Establish protocols for ongoing monitoring, Ensure continued alignment, performance and updates and eventual decommissioning. relevance throughout the agent’s life cycle. Long-term management Implement explainability tools; establish Ensure agent behaviour is interpretable and trust metrics. measurable; build user confidence. Trustworthiness and explainability Establish manual redundancy procedures to Preserve data integrity and plan for human resources ensure the sustained continuity of critical to take over. business use cases. Manual redundancy AI Agents in Action: Foundations for Evaluation and Governance 26 FIGURE 10 Foundations for AI agent evaluation and governance: progressive governance practices Progressive governance practices Access Traceability & Legal & Long-term Testing & control identity compliance management validation Trustworthiness & Monitoring & Manual Human And more... explainability logging redundancy oversight Define Classification Evaluation Risk assessment the use dimensions criteria life cycle Tool call Function Predictability Capabilities Define context Evaluate risks success Task success Edge case Role Use case Identify risks Manage risks rate robustness Task completion Autonomy Environment Trust indicators Analyse risks time Authority Error types And more... Prior to For all agents, regardless of their level of autonomy, the detection of anomalies early, while balancing deployment, authority or the complexity of their operational concerns about privacy and surveillance risks agents should context, specific governance mechanisms should associated with monitoring at scale. Human undergo sandbox serve as a baseline for adoption. At a minimum, oversight, through policy reviews, audit log analysis every agent should operate under strict access and supervisory triggers, helps ensure alignment or controlled pilot control based on the principle of least privilege, with organizational priorities. Unique identifiers and testing using non- with clear task boundaries that prevent unnecessary output tagging support attribution, performance production data to system or data access. Basic legal and compliance tracking and post-incident analysis. In practice, validate expected checks, such as data protection impact the depth of safeguards should scale with the behaviour. assessments and privacy compliance reviews, agent’s autonomy, authority, complexity of context are necessary to ensure alignment with regulatory and overall impact. Higher-risk systems require obligations. In addition, technical controls such as proportionally greater investment in monitoring and input and output filters can help constrain agent oversight, with a deliberate balance between human behaviour by screening potentially harmful, irrelevant review and automated, continuous monitoring. or non-compliant interactions before they propagate through the system. By embedding these measures into the life cycle of all agents, organizations establish a governance Prior to deployment, agents should undergo baseline that can scale proportionally with complexity sandbox or controlled pilot testing using non- and risk. This foundation helps address immediate production data to validate expected behaviour operational safety and compliance needs, creating and mitigate unintended effects. All actions and the structures and practices upon which more planning should be recorded in an audit log for advanced, context-specific governance mechanisms traceability, supported by monitoring tools or alerts can be layered as agents become more autonomous, tailored to the agent’s overall profile. This enables integrated and capable. AI Agents in Action: Foundations for Evaluation and Governance 27 Looking ahead: multi- 3 agent ecosystems Future ecosystems of interacting agents introduce new risks that demand interoperable standards and oversight. As organizations Future ecosystems of interacting agents – Embodied agents: Embodied agents extend begin to deploy introduce new risks that demand interoperable governance challenges into the physical world, multiple agents standards and oversight. where oversight mechanisms must address across departments, both digital actions and consider physical safety, systems and The future of AI agents will happen in a much reliability and human interaction. broader space than enterprise automation and networks, a new will increasingly be defined by the emergence As organizations begin to deploy multiple agents class of failure of multi-agent ecosystems. In these ecosystems, across departments, systems and networks, a modes is emerging. agents are expected to interact, negotiate and new class of failure modes is emerging, linked to collaborate across organizational and technical potentially misaligned interactions between agents. boundaries. In many ways, the interconnectedness A few examples include: of these systems will redefine the future of AI, moving beyond traditional enterprise automation – Orchestration drift: When agents are plugged to allow agents to negotiate, collaborate and into other agents without shared context or coordinate autonomously. While this shift opens coordination logic, workflows can become brittle new opportunities for innovation, it also introduces or unpredictable. challenges around alignment, trust, emergent behaviours and system design. Given the complex – Semantic misalignment: When two agents nature of these systems, ensuring responsible interpret the same instruction differently, it behaviour and effective use requires robust can lead to conflicting actions or duplicated mechanisms for monitoring and assessing agent effort, with implications for safety, reliability interactions. A few examples of emerging multi- and coordination. agent ecosystems and their implications are: – Security and trust gaps: Without shared trust – Agent-to-agent commerce: Agents can initiate frameworks, agents may inadvertently expose transactions, request services or exchange sensitive data or interact with malicious actors, data with other agents, forming a new layer of exploiting vulnerabilities in the system. internet activity with considerable downstream economic implications. – Interconnectedness and cascading effects: Failures in tightly linked agents or systems can – Internet of agents: Beyond isolated interactions, propagate across networks, creating a chain large-scale networks of agents could form of disruptions. an “internet of agents,” raising questions of interoperability, standards, governance and – Systemic complexity: As the number and societal impact. diversity of interacting agents grow, the likelihood of emergent behaviours and cascading failures – Trust frameworks for inter-agent collaboration: increases, making them more difficult to anticipate, As agents begin operating autonomously trace or diagnose. across boundaries, establishing shared norms, credentialing systems and behavioural standards Although the widespread deployment of multi-agent is critical to verify identity, capabilities and reliability. ecosystems is still in its early stages, providers and adopters must now anticipate the associated risks. – Agent governance and oversight: As agent As organizations experiment and pilot agents, capabilities advance, dedicated “governor” or misaligned interactions are already creating new “auditor” agents will monitor, audit or regulate failure modes. Understanding possible challenges the actions of other agents, validating transactions, such as orchestration drift, semantic misalignment detecting anomalies and correcting unsafe or and cascading failures enables adopters to implement unintended behaviours. They enable scalable safeguards before scaling. A proactive approach oversight in complex ecosystems, but they risk ensures responsible growth, aligning governance overreliance on agents supervising other agents. with technical capabilities and defined boundaries. AI Agents in Action: Foundations for Evaluation and Governance 28 Conclusion Agents have already begun moving into production As the development of agents advances across various domains, including customer support, towards multi-agent ecosystems, the need for workflow automation, autonomous research and shared protocols, interoperability standards and more. As adoption advances and as early use coordinated oversight is only going to increase. cases move from single agents to more complex Cross-functional governance that links technical interconnected systems, expectations for scalable assurance with organizational accountability is oversight grow. considered key to preventing cascading failures and ensuring responsible oversight at scale. This report has outlined the foundations for AI agent evaluation and governance, presenting a At the core of this long-term transition is effective conceptual approach to classification, evaluation, human-AI collaboration. In evolving governance risk assessment and governance that supports practices, clear responsibility for objectives, responsible adoption. The proposed dimensions supervision, and outcomes must be supported by aim to help organizations better understand what novel tools and processes that maintain systems an agent does, how it operates and its place within as understandable, safe and secure in practice. the broader organization. Evaluation provides evidence of performance and reliability, while Ultimately, the responsible deployment of agentic risk assessment identifies potential harms and systems depends on a baseline of trust, transparency mitigations. Governance helps translate these and accountability that remains valid for all digital insights into safeguards and concrete accountability systems. With thoughtful design, careful evaluation mechanisms, which can then scale as the agent’s and proportionate governance, AI agents are likely capability is extended to more complex use-cases to amplify human capabilities, improve productivity and scenarios. and, over time, meaningfully contribute to both public and private value. AI Agents in Action: Foundations for Evaluation and Governance 29 Contributors The World Economic Forum’s AI Governance Capgemini Alliance Safe Systems and Technologies working group convenes chief science officers and AI producers to advance thought leadership Olivier Denti surrounding AI agents, from their architecture to Data Architect, AI, Capgemini Invent applications, social implications, guardrails and governance structures. This initiative promotes Jason DePerro the development of safety mechanisms and Human-AI Collaboration Director, Capgemini Invent encourages collaboration on best practices for the design and implementation of AI systems. Jeanne Heuré Vice-President, Digital Trust & Security, Capgemini Invent World Economic Forum Raymond Millward GenAI for R&D Technical Solution Lead, Capgemini Benjamin Cedric Larsen Engineering Initiatives Lead, AI Safety, Centre for AI Excellence Efi Raili Safety Authority, Technology and Innovation, Capgemini Engineering Acknowledgements Animashree (Anima) Anandkumar Kevin Chung Bren Professor of Computing and Mathematical Chief Strategy Officer, Writer Sciences, California Institute of Technology (Caltech) Cathy Cobey Mandanna Appanderanda Nanaiah Global Trusted AI Advisory Leader, EY Head, Infosys Responsible AI, North America, Infosys Ben Colman Nebahat Arslan Co-Founder and Chief Executive Officer, Reality Director, Group General Counsel and Partnership Defender Officer, Women in AI Sakyasingha Dasgupta Mennatallah El-Assady Founder and Chief Executive Officer, EdgeCortix Professor of Interactive Visualization and Intelligence Augmentation, ETH Zurich Umeshwar Dayal Senior Fellow and Senior Vice-President, Hitachi Ricardo Baeza-Yates America; Corporate Chief Scientist, Hitachi WASP Professor, KTH Royal Institute of Technology, Sweden Mona Diab Director, Language Technologies Institute, Amir Banifatemi Carnegie Mellon University Chief Responsible AI Officer, Cognizant Yawen Duan William Bartholomew AI Safety Research Manager, Concordia AI Director of Public Policy, Responsible AI, Microsoft Gilles Fayad Aaron Bawcom Adviser, Institute of Electrical and Electronics Field Chief Technology Officer, Invisible Technologies Engineers (IEEE) Pete Bernard Claudia Fischer Chief Executive Officer, EDGE AI FOUNDATION Public Policy Planning, Global Affairs, OpenAI Fabio Casati Jenn Gamble Lead, AI Trust and Governance Lab, ServiceNow Head, Data Science, Distyl AI AI Agents in Action: Foundations for Evaluation and Governance 30 Chen Goldberg Mao Matsumoto Senior Vice-President, Engineering, CoreWeave Head, NEC Fellow Office, NEC Tom Gruber Sean McGregor Founder, Humanistic AI Agentic Product Safety Lead, MLCommons Gillian Hadfield Risto Miikkulainen Professor of Law and Professor of Strategic Professor of Computer Science, The University of Management, University of Toronto Texas at Austin Peter Hallinan Satwik Mishra Director, Responsible Artificial Intelligence, Amazon Executive Director, Centre for Trustworthy Web Services (AWS) Technology (CTT) Bennet Hillenbrand Margaret Mitchell Agentic Product Safety Lead, MLCommons Researcher and Chief Ethics Scientist, Hugging Face Babak Hodjat Chief AI Officer, Cognizant Jessica Newman Director, AI Security Initiative, Centre for Long-Term Sean Kask Cybersecurity, UC Berkeley Chief AI Strategy Officer, SAP Mark Nitzberg Robert Katz Executive Director, Center for Human-Compatible Vice-President, Responsible AI and Tech, Salesforce AI, UC Berkeley Michael Kearns Henrik Ohlsson Founding Director, Warren Center for Network and Vice-President; Chief Data Scientist, C3 AI Data Sciences, University of Pennsylvania Dmytro Ovcharenko Steven Kelly AI Chief Technology Officer, Ministry of Digital Chief Trust Officer, Institute for Security and Technology Transformation of Ukraine Alex Lebrun Maria Pocovi Co-Founder and Chief Executive Officer, Nabla Global Head of Responsible AI, Uniphore Stefan Leichenauer Reza Rooholamini Vice-President, Engineering, SandboxAQ Chief Scientific, Artificial Intelligence and Innovation Officer, CCC Intelligent Solutions Tze Yun Leong Professor of Computer Science, National University Long Ruan of Singapore Chief Technology Officer, Astra Tech Scott Likens Jason Ruger Global AI and Innovation Technology Lead, PwC Chief Information Security Officer, Lenovo Ramana Lokanathan Daniela Rus Senior Vice-President, Engineering and AI, Director, Computer Science and Artificial Automation Anywhere Intelligence Laboratory (CSAIL), Massachusetts Institute of Technology (MIT) Nada Madkour Non-Resident Research Fellow, University of Jun Seita California, Berkeley Team Director, Medical Science Deep Learning Team, RIKEN Richard Mallah Principal AI Safety Strategist, Future of Life Institute Norihiro Suzuki Chairman of the Board, Hitachi Pilar Manchón Research Institute, Hitachi Senior Director, Engineering, Google Sumit Taneja Gaonyalelwe Maribe Senior Vice-President and Global Head, Artificial Head, Data Analytics and AI, Old Mutual Intelligence (AI) Consulting and Implementation, EXL Service Darko Matovski Founder and Chief Executive Officer, causaLens AI Agents in Action: Foundations for Evaluation and Governance 31 Fabian Theis Science Director, Helmholtz Association World Economic Forum Li Tieyan Chief AI Security Scientist, Huawei Technologies Abhi Balakrishnan Initiatives Lead, AI and Innovation, Lisa Titus Centre for AI Excellence AI Policy Manager, Meta Maria Basso Kush Varshney Head, AI Applications and Impact, IBM Fellow, IBM Centre for AI Excellence Anthony Vetro Daniel Dobrygowski President, Chief Executive Officer, IEEE Fellow, Head, Governance and Trust, Centre for AI Mitsubishi Electric Research Laboratories Excellence Tiffany Wang Xingyu Audrey Duet Founder, Stealth Head, Data and AI Innovation, Centre for AI Excellence Andrea Wong Global Head, Responsible AI Policy, Ginelle Greene Trust and Safety, Bytedance Initiatives Lead, Artificial Intelligence and Energy, Centre for AI Excellence Lauren Woodman Chief Executive Officer, DataKind Connie Kuang Initiatives Lead, Technology Convergence, Centre Michael Young for AI Excellence Vice-President of Products, Private AI Cathy Li Xiaohui Yuan Head, Centre for AI Excellence; Member of the Director, Innovation Research Center; Senior Executive Committee Expert, TRI, Tencent Holdings Hesham Zafar Andy Zhang Lead, Partner Engagement, Centre for AI Excellence Researcher, Stanford University Leonid Zhukov Production Vice-President of Data Science, Boston Consulting Group X (BCG X); Director, BCG Global AI Institute, Boston Consulting Group (BCG) Laurence Denmark Creative Director, Studio Miko Blake Elsey Designer, Studio Miko Will Liley Editor, Studio Miko AI Agents in Action: Foundations for Evaluation and Governance 32 Endnotes 1. Capgemini Research Institute. (2024). Harnessing the value of generative AI. https://www.capgemini.com/wp-content/ uploads/2024/05/Final-Web-Version-Report-Gen-AI-in-Organization-Refresh.pdf. 2. Organisation for Economic Co-operation and Development (OECD). (2024). Recommendation of the Council on Artificial Intelligence. https://legalinstruments.oecd.org/en/instruments/oecd-legal-0449. 3. National Institute of Standards and Technology (NIST). (2023). Artificial Intelligence Risk Management Framework (AI RMF 1.0). https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=936225. 4. International Organization for Standardization (ISO). (2023). ISO/IEC 23894:2023: Information technology — Artificial intelligence — Guidance on risk management. https://www.iso.org/standard/77304.html. 5. Claude Docs. (n.d.). Features overview. https://docs.claude.com/en/docs/build-with-claude/overview. 6. Anthropic. (2024). Introducing the Model Context Protocol. https://www.anthropic.com/news/model-context-protocol. 7. Surapaneni, R., M. Jha, M. Vakoc and T. Segal. (2025). Announcing the Agent2Agent Protocol (A2A). Google for Developers. https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/. 8. Mitchell, M., S. Wu, A. Zaldivar, P. Barnes, et al. (2019). Model Cards for Model Reporting. FAT* 19: Proceedings of the Conference on Fairness, Accountability, and Transparency, pp. 220-229. https://dl.acm.org/ doi/10.1145/3287560.3287596. 9. Parikh, S. and R. Surapaneni. (2025). Powering AI commerce with the new Agent Payments Protocol (AP2). Google Cloud. https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol. 10. Cloudflare. (n.d.). Zero Trust security | What is a Zero Trust network? https://www.cloudflare.com/en-gb/learning/security/ glossary/what-is-zero-trust/. 11. Hasan, M. M., L. Hao, E. Fallahzadeh, B. Adams, et al. (2025). Model Context Protocol (MCP) at First Glance: Studying the Security and Maintainability of MCP Servers. https://arxiv.org/abs/2506.13538v1. 12. Lynch, B. and R. Harang. (2025). From Prompts to Pwns: Exploiting and Securing AI Agents. https://i.blackhat.com/BH- USA-25/Presentations/US-25-Lynch-From-Prompts-to-Pwns.pdf. 13. Adapted from: International Organization for Standardization (ISO). (2023). ISO/IEC 42001:2023: Information Technology — Artificial intelligence — Management system. https://www.iso.org/standard/81230.html; National Institute of Standards and Technology (NIST). (2023). Artificial Intelligence Risk Management Framework (AI RMF 1.0). https://tsapps.nist.gov/ publication/get_pdf.cfm?pub_id=936225. 14. Ibid. 15. Capgemini. (n.d.). Business, meet agentic AI. https://www.capgemini.com/wp-content/uploads/2025/05/Confidence-in- autonomous-and-agentic-systems_19May.pdf. 16. SAE International. (2021). J3016_202104 - Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles. https://www.sae.org/standards/j3016_202104-taxonomy-definitions-terms-related-driving- automation-systems-road-motor-vehicles. 17. Russell, S. J. and P. Norvig. (2021). Artificial Intelligence: A Modern Approach. Pearson. 18. Hendrycks, D., C. Burns, S. Basart, A. Zou, et al. (2021). Measuring Massive Multitask Language Understanding. https://arxiv.org/abs/2009.03300. 19. Srivastava, A., A. Rastogi, A. Rao, A. A. Shoeb, et al. (2022). Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research (TMLR). https://arxiv.org/abs/2206.04615. 20. Liang, P., R. Bommasani, T. Lee, D. Tsipras, et al. (2022). Holistic Evaluation of Language Models. Transactions on Machine Learning Research (TMLR). https://arxiv.org/abs/2211.09110. 21. Liu, X., H. Yu, H. Zhang, Y. Xu, et al. (2024). AgentBench: Evaluating LLMs as Agents. International Conference on Learning Representations (ICLR). https://arxiv.org/abs/2308.03688. 22. Jimenez, C. E., J. Yang, A. Wettig, S. Yao, et al. (2023). SWE-bench: Can Language Models Resolve Real-World GitHub Issues? https://arxiv.org/abs/2310.06770. 23. Rein, D., J. Becker, A. Deng, S. Nix, et al. (2025). HCAST: Human-Calibrated Autonomy Software Tasks. https://arxiv.org/ abs/2503.17354. 24. “Risk” refers to the composite measure of an event’s probability (or likelihood) of occurring and the magnitude or degree of the consequences of the corresponding event; National Institute of Standards and Technology (NIST). (2024). Artificial Intelligence Risk Management Framework: Generative Artificial Intelligence Profile. https://nvlpubs.nist.gov/nistpubs/ai/NIST. AI.600-1.pdf. 25. Adapted from: International Organization for Standardization (ISO). (2023). ISO/IEC 42001:2023: Information Technology — Artificial intelligence — Management system. https://www.iso.org/standard/81230.html; National Institute of Standards and Technology (NIST). (2023). Artificial Intelligence Risk Management Framework (AI RMF 1.0). https://tsapps.nist.gov/ publication/get_pdf.cfm?pub_id=936225. AI Agents in Action: Foundations for Evaluation and Governance 33 The World Economic Forum, committed to improving the state of the world, is the International Organization for Public-Private Cooperation. The Forum engages the foremost political, business and other leaders of society to shape global, regional and industry agendas. World Economic Forum 91–93 route de la Capite CH-1223 Cologny/Geneva Switzerland Tel.: +41 (0) 22 869 1212 Fax: +41 (0) 22 786 2744 contact@weforum.org www.weforum.org