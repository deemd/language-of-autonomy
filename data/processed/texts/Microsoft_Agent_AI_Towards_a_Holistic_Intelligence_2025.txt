Agent AI Towards a Holistic Intelligence QiuyuanHuang⋆∁▶, NaokiWake⋆ℜ▶♢, BidiptaSarkar§†, ZaneDurante§†, RanGong♮†, RohanTaori§†, YusukeNoda⅁, DemetriTerzopoulos♮, NoboruKuno∢, AdeFamoti∢, AshleyLlorens∢, JohnLangford𭟋, HoiVo⅁‡, LiFei-Fei§‡, KatsuIkeuchiℜ‡, JianfengGao∁‡ ∁MicrosoftResearchCore,Redmond;ℜMicrosoftAppliedRoboticsResearch,Redmond; §StanfordUniversity;♮UniversityofCalifornia,LosAngeles; ⅁MicrosoftGamingUS;∢MSRAccelerator;𭟋MSRAIFrontiers,Newyork Figure1:OverviewofanAgentAIsystem.Thissystemisapplicableacrossmultipledomainsandprovidesafoundationmodel forinteractivemanipulationandembodiedoperations. AgentAIoperatesinbothphysicalandvirtualworldsbyleveraging cross-modaldatathatisacquiredthroughinteractionsbetweendiverseenvironments.AgentAIoffersapromisingapproachto unifyabroadrangeofapplicationsandcapabilitieswithininfrastructureandsystem.Furthermore,itisemergingasapromising pathwaytowardsHolisticIntelligence(HI). Abstract modeltoachieveembodiedintelligentbehav- ior, the Agent Foundation Model. On top of Recentadvancementsinlargefoundationmod- this idea, we discuss how agent AI exhibits elshaveremarkablyenhancedourunderstand- remarkablecapabilitiesacrossavarietyofdo- ingofsensoryinformationinopen-worlden- mains andtasks, challenging ourunderstand- vironments. Inleveragingthepoweroffoun- ing of learning and cognition. Furthermore, dationmodels,itiscrucialforAIresearchto wediscussthepotentialofAgentAIfroman pivot away from excessive reductionism and interdisciplinaryperspective,underscoringAI towardanemphasisonsystemsthatfunction cognitionandconsciousnesswithinscientific as cohesive wholes. Specifically, we empha- discourse. We believe that those discussions sizedevelopingAgentAI—anembodiedsys- serveasabasisforfutureresearchdirections temthatintegrateslargefoundationmodelsinto andencouragebroadersocietalengagement. agentactions. TheemergingfieldofAgentAI spansawiderangeofexistingembodiedand agent-basedmultimodalinteractions,including ⋆Equal Contribution. ▶Project Lead. ‡Equal Advisor. robotics,gaming,andhealthcaresystems,etc. ♢CorrespondingAuthor. †Workdonewhileinterningorre- Inthispaper,weproposeanovellargeaction searchingpart-timeatMicrosoftResearch,Redmond. 1 Introduction activelyleveragingaction-basedlargefoundation modelsmakesourapproachuniquefordeveloping Artificial Intelligence (AI) was historically de- integratedAIsystems. fined at the 1956 Dartmouth Conference as ar- BuildingupontheAgentAIframework,webe- tificial life forms capable of collecting informa- lievethattheAIcommunitywillsteadilyaccumu- tion from their environment and taking effective lateinsightsandknowledgeessentialfortransition- actions within it. Minsky’s group at MIT devel- ing from AI models used for passive, structured opedaroboticsystemin1970,knownasthe“Copy taskstothosecapableofdynamic,interactiveroles Demo,”thatobserved“blocksworld”scenesand in complex environments. This is a critical step successfully reconstructed the observed polyhe- towardsthedevelopmentofArtificialGeneralIn- dral block structures (Winston, 1972). The sys- telligence(AGI)(Fig.1). Inthispaper,weanalyze tem,comprisingobservation,planning,andmanip- a new architecture for Agent AI systems, along- ulationmodules,demonstratedthateachofthese side a review of recent literature in Agent AI do- subproblemswashighlychallengingandnecessi- mainsincludingrobotics,gaming,andhealthcare. tated further research. Consequently, the field of Furthermore,weexplorethecognitiveaspectsof AI fragmented into specialized subfields. While Agent AI and introduce research areas impacted thesesubfieldshavemadesignificantprogressin- by Agent AI to engage a broader community of dependently,thisover-reductionismhasblurredthe researchersandactivelypromoteitsdevelopment. overarchinggoalsofAIresearch. Finally, we discuss future research directions, in- To advance beyond the current state towards cluding the ethical challenges that need to be ad- more sophisticated AI, we emphasize the impor- dressed. Through these discussions, we aim to tanceofembracingtheholisticphilosophyofAris- illustratehowthedevelopmentofthesetechnolo- totle, which underscores the integration of com- gies is bringing AI agents closer to AGI, holistic ponents to surpass the sum of its parts. Recent intelligence. advancementsinLargeLanguageModels(LLMs) andVisualLanguageModels(VLMs)haveshown 2 AgentAIParadigm greatpotentialinrecognizinglanguageandimages 2.1 AgentAIfundamentals inanopen-worldcontext(OpenAI,2023). Forex- ample,theadvancedsemanticprocessingofLLMs WedefineAgentAIasanintelligentagentcapable has been utilized to decompose human instruc- of autonomously executing appropriate and con- tionsintohigh-leveltasksforrobots(Wakeetal., textuallyrelevantactionsbasedonsensoryinput, 2023c,d). However, these existing multimodal whetherinaphysical,virtual,ormixed-realityen- foundation models, even for GPT-4V(ision), still vironment. Agent AI represents a new paradigm faceachallengeinachievingfine-grainedmanip- thatshedslightonembodiedintelligence,empha- ulationthatnecessitatesactionprediction. There- sizingtheimportanceofanintegratedapproachfor fore, a new embodied Agent Foundation Model interactiveagentsincomplexdynamics. Thisap- was proposed (Durante et al., 2024b) which inte- proachismotivatedbythebeliefthatintelligence grateslanguageproficiency,visualcognition,con- arisesfromtheintricateinterplaybetweenlearning, textmemory,intuitivereasoning,andcanpredict memory, action, perception, planning, andcogni- theembodiedactionswithadaptability. Thisisthe tion(Fig.2). firststudythatpretrainsafoundationmodelforthe Learning. Agent AI can adapt to new environ- developmentofgeneral-purposeAIagentsbyus- mentsbyacquiringnewknowledgeandupdating ingembodieddatacollectedfromrobotics,gaming, itsskills. Tothisend,theagentneedstoobserveits andhealthcaretasks. environment,understandtheimpactofitsactions An embodied agent is conceptualized as an in- onthatenvironment,andlearnfromhumandemon- teractivesystemthatcommunicateswithhumans strations(Wakeetal.,2021). Forinstance,byem- andinteractswithenvironmentsthroughitspercep- ployingreinforcementlearning(RL)techniquesor tualcapabilities,employingactionsaligningwith supervised learning from human demonstrations humanintents. Thisisthereasonwhyweconsider (e.g.,imitationlearning(IL),behaviorcloning),the theadvanceoflargeembodiedfoundationmodels agentcanprogressivelyimproveitsbehavior. asasignificantcontributiontoAgentAI,enabling Memory. Long-termmemoryenablestheAgentto systemstoparseandinferhumanintentfromvari- rememberspecificoperationsadaptabletotheenvi- ousdomaininformation,actions,natural-language ronmentoruserpreference. Incontrast,short-term instructions and multimodal contexts. Moreover, memorypertainstothehistoryofactionstakenand Figure2:AnAgentAIparadigmforsupportingembodiedmulti-modalgeneralistagentsystems.Therearefivemainmodules asshown:(1)AgentinEnvironmentandPerceptionwithtask-planningandobservation,(2)AgentLearning,(3)Memory,(4) Action,and(5)CognitionandConsciousness. Webelievethatthecohesiveintegrationofthesecomponentsfacilitatesthe developmentofaholisticintelligence. Akeydistinctionofourapproachfromsomepriorinteractivestrategiesisthat,after training,theagent’sactionswilldirectlyinfluencetaskplanningwithouttheneedforreceivingfeedbackfromtheenvironment toplanitssubsequentactionsasthepreviousinteraciveparadigm. perceptions observed during an operation. Short- orchestrationfunctionalityisreferredtoasthecog- term memory enables the system to replan and nitiveaspectofAgentAI. considernext-stepactionsbasedonhistory. Action. TheactionsofAgentAIdonotnecessar- 2.2 AgentAIConsciousness ily have to be physical actions in the real world. Depending on the definition of the environment, Agent AI can go beyond a simple component or- actionsmayincludeinteractionsinvirtualreality chestration and potentially entail a type of “con- (VR)environmentsorspeechdirectedathumans. sciousness.” Inrecentchallengingattemptstofind A suitable action is selected through a cognitive consciousness in AI based on neuroscientific in- processfromlearnedskills,basedonmemory. Ad- sights,neuroscientistshavediscussedAgencyand ditionally, real-world operations often cannot be Embodimentasindicatorsofconsciousness(Butlin completedinoneshotandthusrequiremulti-round etal.,2023). Agencyreferstothecapacitytolearn interactions between humans or the environment from feedback, make decisions to pursue goals, andtheagent. Thisinteractionisalsoorchestrated and adapt to conflicting objectives. It indicates byacognitiveprocessandmemory(e.g.,conversa- asystem’scharacteristicofattemptingtoachieve tionhistory). goalsthroughinteractionwithitsenvironment. Em- Perception. Likehumans,robustandmultimodal bodimentinvolvesunderstandingandutilizingthe perceptioniscrucialforagentstounderstandtheir relationship between actions and feedback from environment. Visualperceptionisoneofthemost theenvironmenttoaffectperceptionorcontrol. It importantabilities,enablingtheagenttocompre- emphasizes comprehending how one’s body and hend the world, e.g., images, videos, gameplay. thesurroundingenvironmentcanbeleveragedin Similarly, audio perception is crucial for under- cognitiveprocesses. standinghumanintent. Planning. Planningisanimportantaspectoflong- OurAgentAIpredictsoptimalactionsbasedon rangetasks,suchasarobotmanipulatingobjects language(i.e.,textualinstructions),sensoryinputs, inanenvironmentforaspecificpurpose. Theplan- andactionhistory,fulfillingAgencybygenerating ningstrategytypicallydependsonthegoalofthe goal-directed actions. It also learns from the re- task. Goal-orientedplanningenablesflexibleopera- lationship between its actions and environmental tionthatadaptstouncertaintiesduetoanyexternal outcomes,fulfillingtheprincipleofEmbodiment. andinternaldisturbances. Thus,wecanpotentiallyquantifyaspectsofAgent AI’sconsciousness,suggestingitspotentialacross Cognitive Aspects. Agent AI focuses not only disciplineslikeneuroscience,biology,physics,bio- ontheperformanceofindividualcomponentsbut logicalphysics,cognitivescience,medicalhealth, alsoontheutilityofthesystemasawhole. Con- andmoralphilosophy. sider a scenario where a robot, right after being unboxed,beginstocommunicatewithanon-expert There are various approaches to developing userandswiftlyadaptstocarryoutdomestictasks Agent AI. In Section 4, we will introduce a spe- within the user’s home setting. Realizing such a cific example of Agent AI. In Section 6, we will system is challenging and requires a mechanism discussthemainchallengesandnecessaryactions, thatorchestrateseachAgentAIcomponents. This includingethicalconcernsinAgentAIresearch. 3 AgentFoundationModel diversehistoricaldataintothetransformermodel includingbutnotlimitedtopreviouslow-levelfine- Agent AI systems that interact with the environ- grainedactions(agentinformation),video/images, ment,withhumans,andamongstotheragents. We audio,language,orhigh-levelinstruction,ascon- consideragent-environmentinteractionsasencom- textduringpre-training. Asaresult,foranygiven passingabroaderscopethanembodiedagents. For timestep,itcanpredictlow-levelmanipulation(ac- instance,ambientintelligencesystems,whichde- tion) tokens, general agent types (e.g., TypeChat spitenotonlyhavingaphysicalembodiment,can ingaming),orhigh-levelinstructions(e.g.,agent beembeddedintoandinteractwiththeirenviron- intention). Moreover,theunifiedtransformercan ment. Theadvancementofagentsystemsthatin- alsoproducehigh-levelinstructionsbasedontext teract with humans is another area of keen inter- prompts,visualcontext,andpreviousactions. This est for this area. We strongly believe that mul- approach allows the model to take into account timodalinteractionsbetweenhumansandagents, boththecurrentcontextandthehistoryofinterac- extendingbeyondhigh-levelintentioninstructions, tions,makingitabletorespondmoreaccuratelyto is a promising area of research and future direc- thetaskathand. tion for low-level fine-grained actions manipula- tion with human-agent interactions. We are also 3.2 AgentLearningStrategy interestedindevelopingsystemsforeffectiveagent ReinforcementLearning(RL).Tolearntheopti- toagentcommunicationandefficientcollaboration malrelationshipbetweenstatesandactionsbased within multi-agent infrastructures and exploring onrewards(orpenalties)receivedasaresultofits newagentparadigmandagentlearningStrategy. actions, we can use reinforcement learning. RL Inthissection,weprovideanoverviewofAgent is a highly scalable framework that has been ap- AIsystemthatleveragesfoundationmodelswith pliedtonumerousapplications,includingrobotics. thelatestmachine-learningtechnologies. Thesys- Formanyapplications, itischallengingorcostly temiscomposedofthreecomponents: i)Interac- tocollecthumandemonstrations,suchaslearning tiveagenttransformer,ii)Agentfoundationmodel policiesinautomaticallygeneratedvirtualenviron- learningstrategywithRLandIL,andiii)selfim- ments. RL is particularly effective in these sce- provement. narios, exemplified by the actor-critic algorithm 3.1 AgentTransformer PPO (Schulman et al., 2017). Additionally, RL technology can be applied to model human-AI interactions, which is a crucial aspect of interac- tiveAgentAI.Forinstance,agentscanbetrained via RL from human feedback (RLHF) (Ouyang et al., 2022), allowing humans to choose desired responseswithouthand-engineeringrewards. Imitation Learning (IL). IL seeks to leverage demonstrationdatatomimictheactionsofhuman experts. For example, in robotics, one of the ma- Figure3:Overviewofaninteractiveagentfoundationmodel jorframeworksbasedonILisBehavioralCloning framework. The transformer is designed to process multi- modalinformationthatconveysvariouslevelsofabstraction. (BC).BCisanapproachwherearobotistrainedto Thisapproachfacilitatesacomprehensiveunderstandingofthe mimictheactionsofanhumanexpertbydirectly context,thusenhancingcoherentactions. Throughlearning copyingthem. Inthisapproach,theexpert’sactions acrossavarietyoftaskdomainsandapplications. inperformingspecifictasksarerecorded,andthe Weanalyzeatransformer-basedmultimodalen- robotistrainedtoreplicatetheseactionsinsimilar coder(Fig.3)thatenablesaninteractiveagentto situations. RecentBC-basedmethodsoftenincor- takeactionsbasedonmultimodalinformation. This porate technologies from LLM/VLMs, enabling modelisinitializedwiththreepre-trainedsubmod- more advanced end-to-end models. For example, ules, namely, the visual module, the agent action Brohanetal. proposedRT-1(Brohanetal.,2022) moduleandthelanguagemodule. andRT-2(Brohanetal.,2023),transformer-based To facilitate cross-modal information sharing, modelsthatoutputanactionsequenceforarobot’s (Duranteetal.,2024b)foundationmodelallowsthe base and arm, taking a series of images and lan- agenttopredictactions(oractiontokens)tocom- guageasinput. Thesemodelsarereportedtoshow pletetheembodiedtasksinrobot,gaming,andin- high generalization performance as the result of teractivehealthcaredomains. Themodelalsofeed trainingonalargeamountsofdemonstrationdata. Traditional RGB. Learning intelligent agent be- 3.4 SelfImprovementforTransformers haviorleveragingimageinputshasbeenofinterest Currently,foundationmodelbasedAIagentshave for many years (Mnih et al., 2015). The inherent the capacity to learn from multiple different data challengeofusingRGBinputisthecurseofdimen- sources,whichallowformoreflexiblesourcesfor sionality. Tosolvethisproblem,researcherseither datafortraining. Twokeyconsequencesofthisare use more data (Jang et al., 2022; Ha et al., 2023) that(1)userandhuman-basedinteractiondatacan orintroduceinductivebiasesintothemodeldesign beusedtofurtherrefineandimprovetheagentand toimprovesampleefficiency. Inparticular,authors (2)existingfoundationmodelsandmodelartifacts incorporate3Dstructuresintothemodelarchitec- canbeusedtogeneratetrainingdata. Wediscuss tureformanipulations(Zengetal.,2021;Shridhar each of these in more detail in the following sec- et al., 2023; Goyal et al., 2023; James and Davi- tions,butwenotethatsincecurrentAIAgentsare son,2022). Forrobotnavigation,authors(Chaplot largelytiedtoexistingpretrainedfoundationmod- etal.,2020a,b)leveragemapsasarepresentation. els, they generally do not learn from continuous Mapscaneitherbelearnedfromaneuralnetwork interactionwiththeirenvironments. Wethinkthis aggregatingallpreviousRGBinputsorthrough3D isanexcitingfuturedirection,andinitialworkby reconstruction methods such as Neural Radiance Bousmalis et al. has shown that self-improving Fields(Rosinoletal.,2022). agents for robotic control are able to continuous learnandimprovethroughenvironmentalinterac- 3.3 OptimizationintheAgentSystem tions without supervision (Durante et al., 2024b; Theoptimizationofagentsystemscanbedivided Bousmalisetal.,2023). intospatialandtemporalaspects. Spatialoptimiza- Furthermore,theiterativelearningprocesscan tion considers how agents operate within a phys- leveragehumanfeedback(Gongetal.,2023). For ical space to execute tasks. This includes inter- example,inthecontextofrobotteaching,AgentAI robotcoordination,resourceallocation,andkeep- understandswhatitneedstodofrommultimodalin- inganorganizedspace. Inordertoeffectivelyop- structionsprovidedbyhumans(Wakeetal.,2021). timizeagentAIsystems,especiallysystemswith Basedontheseinstructions,itgeneratesimagesor large numbers of agents acting in parallel, previ- scenesandmakesthemoperableinavirtualworld. ousworkshavefocusedonusinglargebatchrein- Thisprocessisrepeatedbyutilizinguserfeedback, forcementlearning(Shacklettetal.,2023). Since allowingAgentAItograduallyimproveandadapt datasets of multi-agent interactions for specific itselftotheenvironment. tasksarerare,self-playreinforcementlearningen- ablesateamofagentstoimproveovertime. How- 4 AgentAICategorization ever,thismayalsoleadtoverybrittleagentsthat canonlyworkunderself-playandnotwithhumans AgentAIaimstodevelopagentsthatcanadeptly orotherindependentagentssincetheyover-fitto navigateandinteractwithachangingworld. These the self-play training paradigm. To address this agents are designed to learn and solve complex issue,wecaninsteaddiscoveradiversesetofcon- tasks through direct engagement with their envi- ventions(Cuietal.,2023;Sarkaretal.,2023),and ronment. The field has been propelled forward trainanagentthatisawareofawiderangeofcon- by significant advancements in the development ventions. Foundation models can further help to ofgeneral-purposefoundationmodels,leadingto establish conventions with humans or other inde- superhumanachievementsinvariousAIdomains pendentagents,enablingsmoothcoordinationwith previously deemed challenging. These develop- newagents(Gongetal.,2023). ments have significantly boosted the capabilities Temporal optimization, on the other hand, fo- ofembodiedAI.Researchersarenowrapidlyad- cusesonhowagentsexecutetasksovertime. This vancingtowardscreatingintelligentagentsthatcan encompasses task scheduling, sequencing, and perceivetheirsurroundings,engageinnaturallan- timeline efficiency. For instance, optimizing the guagedialogue,understandandrespondtoauditory trajectory of a robot’s arm is an example of effi- inputs,navigateandmanipulatetheirenvironment cientlyoptimizingmovementbetweenconsecutive to achieve objectives, and reason about the long- tasks (Zhou et al., 2023b). At the level of task termoutcomesoftheiractions. Weareinterestedin scheduling, methods like LLM-DP (Dagan et al., particularwithsubmissionsthatfocusonthemulti- 2023)andReAct(Yaoetal.,2023)havebeenpro- modalaspectsofembodiedAIsystemsanddevelop posedtosolveefficienttaskplanningbyincorpo- novelmethodsforsynthesizingmeaningfulagent ratingenvironmentalfactorsinteractively. outputsfrommulti-sensoryinputs. 4.1 EmbodiedAgentCategorization controllersthataretrainedusingconventionalmeth- odsRT-1(Brohanetal.,2022)andRT-2(Brohan etal.,2023)andAgentfoundationmodel(Durante etal.,2024b). 4.1.2 Manipulationactioninvirtual environments Thistypeofagentutilizesavirtualsimulatedenvi- ronment. Intheroboticsdomain,themainobjec- tiveistotrainAgentAIthroughtrial-and-errorfor taskswherephysicaltrialsareimpracticalorrisky, including the ability to predict user actions and devise plans for tasks within specific constraints (Ahn et al., 2022b; Brohan et al., 2023; Durante et al., 2024b; Gong et al., 2023). In the case of gamingagents,thegoalisnottoeventuallytransi- tiontothephysicalworld,butthelearningwithin thesimulationenvironmentitselfisthemainobjec- tive(Parketal.,2023c;Wangetal.,2023b,e;Baker Figure4:Overviewofthetwoaxesforagentsspaces.Embod- etal.,2022). iedAgentAIisclassifiedaccordingtotheextenttowhichit involveslow-levelfineactionmanipulations,whichwerefer There have also been a number of works that to as “manipulation actions” (e.g., action prediction) in an demonstratetheabilityofgeneral-purposevisually- environment,whetherrealorvirtual. Incontrast,anagent’s aligned large language models trained on large- actions may primarily aim at high-level information trans- missionforarobotorhuman’sintentinstruction,whichwe scale text, image, and video data to serve as a refer to as “intention action” (e.g., general task planning). foundation for creating multi-modal agents that Anagent’senvironmentcanbebroadlycategorizedbasedon areembodiedandcanactinvariousenvironments whetheritisthephysicalworldoravirtualone.Accordingto this,wedivideembodiedandinteractiveAgentintomainfour (Bakeretal.,2022;Driessetal.,2023;Brohanetal., categories. 2023; Durante et al., 2024b). Typically, research Agent AI refers to AI systems that integrate ontheseagentsinvolvessimulationplatformsfor Largefoundationmodels. Consequently,anumber objectrecognition(Kolveetal.,2017;Wangetal., ofrecentAIsystemsthatarebasedonLLM/VLMs 2023d;Meesetal.,2022;Yangetal.,2023a;Ehsani can be associated with Agent AI subcategories. etal.,2021;Szotetal.,2021;Puigetal.,2018;Car- Specifically,wecategorizeAgentAIbasedbythe roll et al., 2019; Li et al., 2021; Srivastava et al., types of agent actions and their environments, as 2022; Mittal et al., 2023; Zhong et al., 2023; Liu illustrated in Fig. 4. Therefore, Agent AI can be andNegrut,2021;Saitoetal.,2023;Huangetal., broadlygroupedintofourcategories. Thissection 2022a). reviewsrelatedresearch(Duranteetal.,2024a)and 4.1.3 Intentionalactioninphysical organizesthemaccordingtothesecategories. We environment alsoexpandonsystemscombiningbothintention andmanipulationagentsinAppendixA. A typical example of interactive agents in this category is found in the healthcare domain, such 4.1.1 Manipulationactioninphysical as applications in diagnostics and knowledge re- environments trieval (Lee et al., 2023; Peng et al., 2023). In Agents in this category are intended to work in a similar context, several works have developed thephysicalworld,withroboticsapplicationsbe- empathy-awareagentsforengagingdialogueand ingthetypicalexample(Ahnetal.,2022a;Huang human-machine interactions (Chen et al., 2021; etal.,2022b;Liangetal.,2022;Driessetal.,2023; Maoetal.,2022;Wakeetal.,2023a;Savvaetal., Brohanetal.,2023). Trainingagentsforphysical 2019; Puig et al., 2023; Huang et al., 2018). In manipulationinanend-to-endmanneristypically other cases, Agent AI’s focus on knowledge and challengingduetothesignificantcostsassociated logicalreasoninginvolvesintegratingimplicitand withcollectingalargeamountofdatafortraining. explicit knowledge sources. This integration en- Consequently, recenttrendshaveshiftedtowards ables more accurate and contextually appropri- solvinghigher-ordertaskplanswithlargefounda- ateresponses(Brownetal.,2020;OpenAI,2023; tionmodelandintegratingthesewithlower-level Lewis et al., 2020; Peng et al., 2023; Gao et al., 2022; Marcus and Davis, 2019; Gao et al., 2020; etal.,2023b),butnotlimitedtoSimulationanden- Wang et al., 2023a; Chen et al., 2020; Park et al., vironmentsagents(Puigetal.,2018)),generative 2023a;Lietal.,2023b). agents(Huangetal.,2023b),knowledgeandlogi- calinferenceagents (Lewisetal.,2020;Pengetal., 4.1.4 Intentionalactioninvirtual 2023;Wangetal.,2023a;Guietal.,2022b),emo- environment tion agent (Chen et al., 2021), Neuro-symbolic Studies on Agent AI in this category have high- agents (Chen et al., 2020), and agents for tradi- lighted the utility for the creation of interactive tionalmultimodaltasks,multimodalagentsystems content in gaming and both VR and XR (Chen andinfrastructure,andapplicationsofmultimodal etal.,2021;Maoetal.,2022;Huangetal.,2023b). agents. Agent navigation following instruacion is also a 5 AgentAIApplicationTasks representativetaskthatfallsinthiscategory(Tsoi etal.,2022;Deitkeetal.,2020). Similartogaming In Section 4, we categorized existing research agents for intentional action, this type of Agent within the realm of Agent AI. To offer a tangi- AI has shown super-human performance in spe- bleunderstandingofitsapplications,weintroduce cificgames(MetaFundamentalAIResearchetal., fourmission-criticaldomainswhereAgentAIcan 2022;Yaoetal.,2023). Recentroboticsresearch haveamajorimpact. alsoleveragesLLMstoperformtaskplanning(Ahn et al., 2022a; Huang et al., 2022b; Liang et al., 5.1 Robotics 2022) by decomposing natural language instruc- Robots are representative agents that necessitate tionintoasequenceofsubtasks,eitherinthenatu- effectiveinteractionwiththeirenvironment. Inthis rallanguageformorinPythoncode,thenusinga section, we introduce key elements essential for low-levelcontrollertoexecutethesesubtasks. efficientroboticoperation,reviewresearchtopics wherethelatestlargefoundationmodelshavebeen 4.2 MultimodelAgentCategorization applied,andshareinsightsfromrecentstudies. (Non-Embodied) Multimodal Systems. Recent research focuses ThesecategoriesofAgentsemphasizetheimpor- on developing end-to-end systems incorporating tanceofusingmultimodalinformationtotakeben- large foundation model technologies as encoders eficialnon-embodiedfromtheirrespectiveaspects. forinputinformation,guidingroboticactionsbased This indicates the necessity for agents to possess onlinguistic instructionsand visualcues (Huang highrecognitioncapabilitiesforbothlanguageand etal.,2018;Jiangetal.,2022;Brohanetal.,2023, vision, thereby strongly suggesting the effective- 2022;Lietal.,2023f;Ahnetal.,2022b;Shahetal., nessofleveraginglargefondationmodels. MUlti- 2023b;Lietal.,2023c). modelAgenthaveshownsignificantutilityacross Task Planning and Skill Training. Advanced a variety of tasks. The advancements in large- languageprocessingabilitiesofLLMsinterpretin- scale foundational models and interactive artifi- structions and decompose them into robot action cialintelligencehaveopenedupnovelcapabilities steps, advancing task planning technologies (Ni for multimodel agent. A number of works lever- et al., 2023; Li et al., 2023a; Parakh et al., 2023; age multi-modelagents toperform task planning Wakeetal.,2023b). Forskilltraining,largefoun- (Huangetal.,2022a;Wangetal.,2023b;Yaoetal., dationmodelsareusedfordesigningrewardfunc- 2023;Lietal.,2023b),andleveragethelargemul- tions(Yuetal.,2023;Kataraetal.,2023;Maetal., timodels’ large internet-scale domain knowledge 2023),generatingdataforpolicylearning(Kumar andzero-shotplanningabilitiestoperformagentic etal.,2023;Duetal.,2023),oraspartofareward tasks like planning and reasoning. Additionally, function(Sontakkeetal.,2023). (Huang et al., 2022b), (Liang et al., 2022), and On-siteOptimization. Thisinvolvesdynamically (Wangetal.,2023e)alsoincorporateenvironmen- adaptingandrefiningroboticskillsbyintegrating talfeedbacktoimprovetaskperformance. taskplanswithreal-timeenvironmentaldata(Ahn Nevertheless,foragentAItobegenuinelybene- et al., 2022b; Zhou et al., 2023b; Raman et al., ficial,theymustofferintuitiveinteractionexperi- 2023;Chenetal.,2021). Strategiesseektoachieve ences and adapt to a wide array of environments, environment-groundedrobotexecutionbyadjust- contexts, andmodalities. Topromoteresearchin ingtherobot’sactionsatthetaskplanorcontroller thisarea,weproposedabroadrangeofcategoriza- level. tion relevant for multimodal agents without em- Conversation Agents. LLMs contribute to natu- bodied action including (Gui et al., 2022a; Park ral,context-sensitiveinteractionswithhumansin conversationalrobots(Yeetal.,2023;Wakeetal., immersion(Gongetal.,2023). 2023d). Theyprocessandgenerateresponsesthat Agent-basedAnalysisofGaming. Gamingisan mimic human conversation and estimate concep- integralpartofdailylife,estimatedtoengagehalf tual(Henseletal.,2023;Teshimaetal.,2022)and oftheworld’spopulation(Intelligence,2020)and emotionalattributes(Zhaoetal.,2023;Yangetal., exhibitsapositiveimpactonmentalhealth(Granic 2023b;Wakeetal.,2023a)ofutterances. et al., 2014). Contemporary game systems, how- NavigationAgents. Robotnavigationfocuseson ever,oftenexhibitdeficienciesininteractionswith coreaspectssuchasmap-basedpathplanningand human players due to primarily hand-crafted be- SLAM(Guimarãesetal.,2016). Recentworken- haviors by game developers. In such a context, ables robots to navigate in challenging environ- AgentAIprovesvaluableasasystemthatanalyzes ments using object names (Chaplot et al., 2020a; in-gametextdata,suchaschatlogsandplayerfeed- Batra et al., 2020; Gervet et al., 2023; Ramakr- back, to identify patterns of player behavior and ishnan et al., 2022; Zhang et al., 2021) or zero- preferences,aswellasanalyzesimageandvideo shotobjectnavigation(Gadreetal.,2023;Dorbala datafromgamingsessionstounderstanduserintent et al., 2023; Cai et al., 2023). Vision-Language andactions. Navigation (VLN) interprets sentences for navi- Scene Synthesis for Gaming. Scene synthesis gation in unseen environments (Anderson et al., isessentialforcreatingandenhancingimmersive 2018;Shahetal.,2023a;Zhouetal.,2023a;Dor- gaming environments, encompassing the genera- balaetal.,2022;Liangetal.,2023;Huangetal., tionofthree-dimensional(3D)scenes,terraincre- 2023a). VLNinterpretssentencesratherthanob- ation,objectplacement,realisticlighting,anddy- ject names, it requires a higher functionality to namicweathersystems(Huangetal.,2023b). In parseinputtext(Wangetal.,2019). modern games, providing vast open-world envi- ronmentsnecessitatestheuseofproceduralorAI- 5.2 Gaming driven techniques for automated terrain genera- Gamesprovideauniquesandboxtotesttheagen- tion. AgentAI,utilizinglargefoundationmodels, tic behavior of large foundation models, pushing aidsscenedesignersbyformulatingnon-repeating, theboundariesoftheircollaborativeanddecision- uniquelandscapedesignrulesbasedonthedesign- making abilities. We describe three areas in par- ers’desiresandthecurrentscene,ensuringseman- ticular that highlight agent’s abilities to interact ticconsistencyandvariabilityofthegeneratedas- with human players and other agents, as well as sets. Thesemodelsexpediteobjectplacementand their ability to take meaningful actions within an assistincontentgeneration,enhancingthedesign environment. process. NPC Behavior. In modern gaming systems, the 5.3 InteractiveHealthcare behaviorofNon-PlayerCharacters(NPCs)ispre- dominantlydictatedbypredefinedscriptscrafted Inhealthcare,AgentAIcanhelpbothpatientsand bydevelopers. Thesescriptsencompassarangeof physicians by utilizing large foundation models reactionsandinteractionsbasedonvarioustriggers inunderstandingtheintentoftheuser, retrieving orplayeractionswithinthegamingenvironment. clinical knowledge, and grasping the undergoing Inlightofthissituation,AgentAIisattheforefront human-to-human interaction, but not limited to ofrevolutionizingNPCtechnologies. Byleverag- theseareas. Examplesofapplicationinclude: inglargefoundationmodel,AgentAIcanprovide DiagnosticAgents. LLMsasmedicalchatbotsfor dynamicdialoguesandrefinebehaviorsbasedon patient diagnosis have gained attention for their player feedback and in-game data, significantly potentialtohelptriageanddiagnosepatients,pro- contributing to the evolution of NPC behavior in vidingequitablehealthcareaccesstodiversepopu- games. lations (Lee et al., 2023). They offer a pathway Human-NPCInteraction. AgentAIplaysacrit- to improve healthcare for millions, understand- icalroleinenhancingtheinteractionbetweenhu- ing various languages, cultures, and health con- manplayersandNPCs,offeringamoreimmersive ditions,withinitialresultsshowingpromiseusing gamingexperience. Theconventionalinteraction healthcare-knowledgeableLLMstrainedonlarge- paradigmisprimarilyone-dimensional,withNPCs scalewebdata(Duranteetal.,2024b,a). However, reactinginapresetmannertoplayerinputs. Agent riskssuchashallucinationwithinmedicalcontexts AI,utilizinglargefoundationmodels,cananalyze arenotablechallenges. and learn from human behavior, providing more KnowledgeRetrievalAgents. Inthemedicalcon- human-likeinteractionsandincreasingrealismand text, model hallucinations can be dangerous, po- tentially leading to serious patient harm or death. image understanding to include dynamic content Approaches using agents for reliable knowledge andrequiresagentstointeractwithvisual,textual, retrieval(Pengetal.,2023)orretrieval-basedtext andaudiomodalities. Keytasksincludecaptioning, generation(Guuetal.,2020)arepromising. Pairing question answering, and activity recognition, fo- diagnosticagentswithmedicalknowledgeretrieval cusingontemporalalignment,sequencehandling, agents can reduce hallucinations and improve re- and complex activity interpretation. Agents also sponsequalityandpreciseness. needtoprocessaudiocueslikespokenwordsand Telemedicine and Remote Monitoring. Agent- background sounds to grasp a video’s mood and basedAIinTelemedicineandRemoteMonitoring nuances. canenhancehealthcareaccess,improvecommuni- Parallel research explores generating scaled cationbetweenhealthcareprovidersandpatients, datasets from large models, then applying visual andincreasetheefficiencyofdoctor-patientinter- instructiontuning(Duranteetal.,2024b,a;Lietal., actions (Amjad et al., 2023). Agents can assist 2023d; Zhu et al., 2023) on the generated data. in triaging messages from doctors, patients, and Considerableaudio,speech,andvisualexpertper- healthcareproviders,highlightingimportantcom- ception models are subsequently used to verbal- munications,andrevolutionizingremotehealthcare ize videos. Speech is transcribed with automatic anddigitalhealthindustries. speech recognition tools, and video descriptions andrelateddataareproducedwithvarioustagging, 5.4 InteractiveMultimodalTasks grounding,andcaptioningmodels(Lietal.,2023e; Theintegrationofvisualandlinguisticunderstand- Maaz et al., 2023; Chen et al., 2023; Wang et al., ing is a fundamental of Agent AI. Therefore, the 2023c). Thesetechniquesdemonstratehowinstruc- development of Agent AI is closely linked to the tion tuning video-language models on generated performanceofmultimodaltasks,includingimage datasetsmayleadtoenhancedvideo-reasoningand captioning, visual question answering, video lan- communicationabilities. guagegeneration,andvideounderstanding. Here Suchagentswouldbeabletounderstandthecon- aresometasksthathaverecentlygarneredsignifi- text of the video, identify the key steps, and gen- cantinterest: erateacoherentsummaryoftheprocedure. This ImageandLanguageUnderstandingandGen- wouldnotonlyenhancetheinterpretabilityofthe eration. Image-language understanding is a task modelbutalsoenableittoprovideusefulfeedback thatinvolvestheinterpretationofvisualcontentin orguidancetotheuser. a given image with language and the generation Weexpanduponmorecross-modalityandMix- of associated linguistic descriptions. This task is reality topic discussion in Appendix B.1, Ap- criticaltothedevelopmentofAIagentsthatcanin- pendixB.2andB.3. teractwiththeworldinamorehuman-likemanner. Someofmostpopularonesareimagecaptioning 6 DeployingAgentAI (Linetal.,2014;Sharmaetal.,2018;Youngetal., 2014; Krishna et al., 2016), referring expression We believe that in order to develop a system that (Yuetal.,2016;Karpathyetal.,2014),andvisual incorporates these elements, it is necessary to in- questionanswering(Antoletal.,2015;Renetal., volveawiderangeofexpertsandpractitioners. For 2015; Singh et al., 2019). This demands capabil- instance,therearethefollowingimportantresearch ities beyond object recognition, encompassing a areas: deepunderstandingofspatialrelationships,visual Exploringnewparadigms. Thedevelopmentof semantics, and integrating world knowledge for agentparadigmswithintegratedmodalities(audio, accuratedescriptiveandreasoningabilities. image, text, sensor inputs) may address common Video-Language Understanding and Genera- issuesinlarge-scalemodels,suchashallucinations tion. Video captioning and storytelling involve and biases in their outputs, which will enhance generating coherent sentences for video frames, their recognition and response capabilities for a challengingduetotheneedforacomprehensiveun- widevarietyofapplications. derstandingofeachframeandtheirinterrelations. General-purposeend-to-endsystems. Versatile Recentadvancesleveragelargefoundationmodels and adaptable AI solutions can be driven by the forimprovedvideo-languagegeneration,emphasiz- developmentofend-to-endmodelsthataretrained ingthedevelopmentofagent-awaretextsynthesis withlarge-scaledata. modelsforencodingsequencesandgeneratingco- Methodologiesforgroundingmodalities. Byin- hesiveparagraphs. Videounderstandingbroadens tegratinginformationacrossvariousmodalities,we canenhancethecoherenceandefficacyofdatapro- agentcopilotsbyusingadvancedhardware,diverse cessing. WeexpandonthistopicinAppendixB.1. datasources,andpowerfulsoftwarelibraries(Gong Intuitivehumaninterface. Developingintuitive et al., 2023). The rising prevalence of Agent AI humaninterfacescanfacilitateeffectiveandmean- underscores the need for robust infrastructure to ingfulinteractionsbetweenhumansandagents. facilitatetheirtraining,evaluation,anddeployment. TamingLLM/VLMs. Exploringnewapproaches Inresponsetothisneed,weareintroducingaded- canaddresscommonissuesinlarge-scalefounda- icatedtrackforagentresearchfocusingonthein- tion models, such as hallucinations and biases in frastructureandmethodologiespertinenttothede- theiroutputs. velopment, evaluation, and deployment of Agent Bridgingthegapbetweensimulationandreal. AI. We expect this track will attract a significant The"sim-to-real"problemhighlightsthechallenge numberofsubmissionscenteredontheefficiency ofdeployingAIagentstrainedinsimulationstothe and optimization of agent systems. Agent AI in- realworld,wherediscrepanciesinconditionslike frastructureisintendedtoensurethatthebroader disturbancesandphysicalpropertiescandegrade community can readily access and benefit from performance. Totackletheseissues,strategiesin- these contributions, thereby fostering further ad- clude: vancementsinthefield. WeexpandonbiasesandhallucinationsinAp- • DomainrandomizationIntroducingvariabil- pendixCandDrespectively. ityinthesimulatedenvironmenttobetterpre- pare the model for real-world unpredictabil- 7 ChallengesforAgentAI ity(Tobinetal.,2017;Saitoetal.,2022). Inthispaper,weputspecialemphasisondiscover- • DomainadaptationBridgingsim-to-realgap ingthecurrentagentAIlimitation,andwediscuss bytrainingonbothsimulatedandreal-world thechallengesaheadforadvancingtowardsdeeper data(Zhuetal.,2017a;Raoetal.,2020;Ho andmorecomprehensiveversionsofAGI,includ- etal.,2021). ingthepossibleneedforpursuinganewparadigm thatmovesbeyondnext-wordprediction. • ImprovementofsimulationEnhancingsim- ulation fidelity through better replication of Achievement of the Agent AI still have some real-worldconditions(Zhuetal.,2017b;Al- challenges,especiallyconsideringthedynamicsys- levatoetal.,2020;Martinez-Gonzalezetal., temwithhighmodalityobservationsinthephysical 2020; Müller et al., 2018; Shah et al., 2018; world. Therestillexistanumberofchallengesthat Sasabuchietal.,2023). need to be addressed, including but not limited to: 1) unstructured environments, where current Multi-Agent. Agent AI interaction is currently visualinputsaffectbothhigh-levelintentsandlow- still a complex process that requires a combina- levelactionsoftheembodiedagentgiventhesame tionofmultipleskills. Thecurrenthuman-machine goalinstruction;2)empathyforagent,whenopen interactionsystemsinsidemulti-agentsareprimar- setsofobjects,whichrequiretheagent’sdecision- ilyeffectivenessofcooperationrule-based. They makingmoduletousecommonsenseknowledge do have intelligent behaviors in response to hu- that is hard to encode manually; 3) multi-agent man/user actions and possess web knowledge to interactionsandcollaborations,whichrequirethe some extent (Gong et al., 2023). The kind multi agenttounderstandandoperateonmorethanjust agentsinteractionsareveryimportantintheagent template-based commands, but also a context of development to enable specific behaviors in the goals,constraints,andpartialplansexpressedinev- agentsystemdesign. erydaylanguage. Toenableamorecomprehensive AgentInfrastructureandSystem. Agent-based approach to these complex challenges, the inclu- AIisalargeandfast-growingcommunitywithin sionofresearchersandpractitionersfromabroader thedomainsofentertainment,research,andindus- range of fields is critical. 4) Emergent ability for try. The development of large foundation mod- embodied large agent foundation model. We as- elshassignificantlyimprovedtheperformanceof pire to broaden our collective understanding of agent AI systems. However, creating agents in the potential and limitations of Agent Paradigm this vein is limited by the increasing effort nec- byleveragingouruniqueanddiverseperspectives. essary to create high-quality datasets and overall Westronglybelievethatthisproposednewagent cost. In industry, building high-quality agent in- paradigmwillnotonlyenrichtheperspectivesof frastructurehassignificantlyimpactedmulti-modal individualpractitioners,butwillalsoenhancethe community’scollectiveknowledgeandpromotea gaming,robotics,healthcare,andlong-videounder- holistic view that is more inclusive of the wide- standing. Specifically, the development of multi- rangingchallengesfacedbyfutureagentAI. modalagentsingamingcouldleadtomoreimmer- siveandpersonalizedgamingexperiences,thereby 8 EmergentAbilities transformingthegamingindustry. Inrobotics,the developmentofadaptivesystemscouldrevolution- Despitethegrowingadoptionofinteractiveagent izeindustriesrangingfrommanufacturingtoagri- AIsystems,themajorityofproposedmethodsstill culture,potentiallyaddressinglaborshortagesand face a challenge in terms of their generalization improvingefficiency. Inhealthcare,theuseoflarge performance in unseen environments or scenar- foundation model as diagnostic agents or patient ios. Current modeling practices require develop- care assistants could lead to more accurate diag- ers to prepare large datasets for each domain to noses, improved patient care, and increased ac- finetune/pretrainmodels;however,thisprocessis cessibilitytomedicalservices,particularlyinun- costly and even impossible if the domain is new. derservedareas. Furthermore,theabilityofthese Toaddressthisissue,weproposebuildinginterac- models to interpret long-form videos could have tive agents that leverage the knowledge-memory far-reaching applications, from enhancing online ofgeneral-purposefoundationmodels(ChatGPT, learning to improving technical support services. Dall-E, GPT-4, etc.) for novel scenarios, specifi- Ingeneral,theAgentAIframeworkwillhavesig- callyforgeneratingacollaborativespacebetween nificant downstream effects on a wide range of humans and agents. We discover an emergent industriesandpeopleacrosstheworld. mechanism—whichwenameMixedRealitywith Wemustalsohighlightthediverseandcomplex KnowledgeInferenceInteraction—thatfacilitates challengesthatcomewithimplementingAIagents collaboration with humans to solve challenging across a wide variety of environments and situ- tasks in complex real-world environments and ations. For instance, there are many limitations enables the exploration of unseen environments andpotentialhazardslinkedtoAgenticAIsystems for adaptation to virtual reality. For this mecha- when they are developed for specialized sectors nism,theagentlearnsi)micro-reactionsincross- suchashealthcarediagnostics. Inthisdomain,is- modality: collectingrelevantindividualknowledge sueslikedangeroushallucinationsinAIbehavior for each interaction task (e.g., understanding un- canposesignificantrisks,highlightingthecritical seenscenes)fromtheexplicitwebsourceandby needformeticulousdesignandtesting. However, implicitly inferringfrom theoutput of pretrained these specific challenges may not be equally rel- models;ii)macro-behaviorinreality-agnostic: im- evant or noticeable when considering AI agents proving interactive dimensions and patterns in crafted for the gaming industry. In such recre- languageandmulti-modalitydomains,andmake ationalfields, developersmightinsteadprioritize changesbasedoncharacterizedroles,certaintarget tacklingdifferenthurdles,suchastheneedforAI variable,influenceddiversificationofcollaborative toperformmoreopen-endedgenerationandexhibit information in mixed-reality and LLMs. We in- creativity, adaptingdynamicallytounpredictable vestigatethetaskofknowledge-guidedinteractive gameplayscenariosandplayerinteractions. synergisticeffectstocollaboratedscenegeneration withcombiningvariousOpenAImodels,andshow 10 Conclusion promisingresultsofhowtheinteractiveagentsys- temcanfurtherboostthelargefoundationmodels OurproposedAgentAIfocusesonadvancedmulti- inoursetting. Itintegratesandimprovesthedepth modalsystemsthatinteracteffectivelywithinboth ofgeneralization,consciousandinterpretabilityof physicalandvirtualenvironmentsandfacilitateef- acomplexadaptiveAIsystems. fectiveinteractionwithhumans. Thispaperaimsto uniteresearcherstodeepenthediscourseonAgent 9 ImpactStatement AI,cuttingacrossvariousAIdisciplinesincluding agent paradigms, foundation models, infrastruc- Agent AI paradigm is to create general-purpose tures,andsystems. Ourgoalistoenrichthescien- agentsthatcanworkalongsidehumansinbothreal tific comprehension of Agent AI and explore the andvirtualenvironments. Thisparadigmtherefore potential of embodied agents within the realm of intendstohaveaverybroadimpact,possiblyaffect- holisticintelligenceresearch. Thisendeavorposi- ingall membersofsociety. AgentAIframework tionsustoleverageemergingfoundationalmodels emphasizestheintegrationofagentsintothewider effectively. environment across a variety of settings, such as EthicalConsideration Limitations The main thesis of our work is that the Agent AI AgentAIsystemshavemanyapplications. Inaddi- formulationhelpstobringthefieldofAIbackto tiontointeractiveAI,groundedmultimodalmodels itsrootsinholisticintelligence. However,thereare couldhelpingeneratingtrainingdatasetsforrobots stillmanyunknownswithintheAgentAIparadigm. and AI agents, and assist in productivity applica- Existingfoundationmodelsexhibitbiasesandhal- tions, helping to re-play or paraphrase scenario, lucinations,anditisunclearwhetherthesecanbe predict actions in novel scenarios, or synthesize resolvedthroughscalingupmodelanddatasetsizes 3Dor2Dscenes. Fundamentaladvancesinagent orifthesearefundamentallimitationsofAgentAI. AI help contribute towards these goals and many We also acknowledge that there are many ad- wouldbenefitfromagreaterunderstandingofhow ditional challenges in this field that we have not tomodelembodiedandempatheticbehaviorina coveredinSection7. Asagrowingfieldwithapo- simulated environment or the real world. There- tentialformajorimpact,webelievethatthedevel- fore,therearemanyapplicationsthathavepositive opmentofAgentAImustincludeadiverserange benefits. ofperspectivesacrossdisciplinestoensurethatit hasapositiveimpactonhumanity. However,thistechnologycouldalsobeusedby bad actors. Agent AI systems that generate con- tentcanbeusedtomanipulateordeceivepeople. References Therefore,itisveryimportantthatthistechnology Michael Ahn, Anthony Brohan, Noah Brown, Yev- is developed in accordance with responsible AI genChebotar,OmarCortes,ByronDavid,Chelsea guidelines. Forexample,explicitlycommunicating Finn,ChuyuanFu,KeerthanaGopalakrishnan,Karol tousersthatcontentisgeneratedbyanAIsystem Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, and providing the user with controls in order to Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, customizesuchasystem. ItispossibletheAgent Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jes- month, Nikhil Joshi, Ryan Julian, Dmitry Kalash- AIcouldbeusedtodevelopnewmethodstodetect nikov, Yuheng Kuang, Kuang-Huei Lee, Sergey manipulativecontent-partlybecauseitisrichwith Levine, Yao Lu, Linda Luu, Carolina Parada, Pe- hallucinationsthatemergefromlargefoundation terPastor, JornellQuiambao, KanishkaRao, Jarek models-andthushelpaddressanotherrealworld Rettinghouse,DiegoReyes,PierreSermanet,Nico- problem. lasSievers,ClaytonTan,AlexanderToshev,Vincent Vanhoucke,FeiXia,TedXiao,PengXu,SichunXu, Forexample,ethicaldeploymentoflargeagents Mengyuan Yan, and Andy Zeng. 2022a. Do as i foundationmodels,especiallyinsensitivedomains canandnotasisay: Groundinglanguageinrobotic like healthcare, is paramount. AI agents trained affordances. InarXivpreprintarXiv:2204.01691. onbiaseddatacouldpotentiallyworsenhealthdis- MichaelAhn,AnthonyBrohan,NoahBrown,Yevgen paritiesbyprovidinginaccuratediagnosesforun- Chebotar,OmarCortes,ByronDavid,ChelseaFinn, derrepresentedgroups. Moreover,thehandlingof Keerthana Gopalakrishnan, Karol Hausman, Alex sensitive patient data by AI agents raises signifi- Herzog, et al. 2022b. Do as i can, not as i say: cant privacy and confidentiality concerns. In the Grounding language in robotic affordances. arXiv preprintarXiv:2204.01691. gaming industry, AI agents could transform the roleofdevelopers,shiftingtheirfocusfromscript- Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, ing non-player characters to refining agent learn- Antoine Miech, Iain Barr, Yana Hasson, Karel ingprocesses. Similarly,adaptiveroboticsystems Lenc,ArthurMensch,KatherineMillican,Malcolm Reynolds,etal.2022. Flamingo: avisuallanguage couldredefinemanufacturingroles,necessitating model for few-shot learning. Advances in Neural newskillsetsratherthanreplacinghumanworkers. InformationProcessingSystems,35:23716–23736. Navigatingthesetransitionsresponsiblyisvitalto minimizepotentialsocio-economicdisruptions. AdamAllevato,ElaineSchaertlShort,MitchPryor,and Andrea Thomaz. 2020. Tunenet: One-shot resid- Furthermore, the agent AI focuses on learning ualtuningforsystemidentificationandsim-to-real collaborative policies in simulation and there is robottasktransfer. InConferenceonRobotLearning, some risk of directly applying the policy to the pages445–455.PMLR. real world due to the distribution shift. Robust AyeshaAmjad,PiotrKordel,andGabrielaFernandes. testing and continuous safety monitoring mecha- 2023. Areviewoninnovationinhealthcaresector nismsshouldbeputinplacetominimizerisksof (telehealth) through artificial intelligence. Sustain- unpredictablebehaviorsinreal-worldscenarios. ability,15(8):6655. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Bridgingzero-shotobjectnavigationandfoundation MarkJohnson,NikoSünderhauf,IanReid,Stephen modelsthroughpixel-guidednavigationskill. arXiv Gould, and Anton Van Den Hengel. 2018. Vision- preprintarXiv:2309.10309. and-language navigation: Interpreting visually- grounded navigation instructions in real environ- MicahCarroll,RohinShah,MarkKHo,TomGriffiths, ments. In Proceedings of the IEEE conference on SanjitSeshia,PieterAbbeel,andAncaDragan.2019. computervisionandpatternrecognition,pages3674– Ontheutilityoflearningabouthumansforhuman-ai 3683. coordination. Advancesinneuralinformationpro- cessingsystems,32. StanislawAntol,AishwaryaAgrawal,JiasenLu,Mar- garetMitchell,DhruvBatra,CLawrenceZitnick,and DevendraSinghChaplot,DhirajPrakashchandGandhi, DeviParikh.2015. Vqa: Visualquestionanswering. Abhinav Gupta, and Russ R Salakhutdinov. 2020a. InProceedingsoftheIEEEinternationalconference Objectgoalnavigationusinggoal-orientedsemantic oncomputervision,pages2425–2433. exploration. Advances in Neural Information Pro- cessingSystems,33:4247–4258. Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon DevendraSinghChaplot,RuslanSalakhutdinov,Abhi- Houghton, Raul Sampedro, and Jeff Clune. 2022. navGupta,andSaurabhGupta.2020b. Neuraltopo- Video pretraining (vpt): Learning to act by watch- logicalslamforvisualnavigation. InProceedingsof ing unlabeled online videos. Advances in Neural theIEEE/CVFConferenceonComputerVisionand InformationProcessingSystems,35:24639–24654. PatternRecognition,pages12875–12884. DhruvBatra, AaronGokaslan, AniruddhaKembhavi, Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Oleksandr Maksymets, Roozbeh Mottaghi, Mano- Yifei Huang, Junting Pan, Yi Wang, Yali Wang, lis Savva, Alexander Toshev, and Erik Wijmans. Yu Qiao, Tong Lu, and Limin Wang. 2023. Vide- 2020. Objectnav revisited: On evaluation of em- ollm: Modelingvideosequencewithlargelanguage bodiedagentsnavigatingtoobjects. arXivpreprint models. arXiv:2006.13171. KezhenChen,QiuyuanHuang,DanielMcDuff,Xiang KonstantinosBousmalis,GiuliaVezzani,DushyantRao, Gao,HamidPalangi,JianfengWang,KennethFor- Coline Devin, Alex X Lee, Maria Bauza, Todor bus, and Jianfeng Gao. 2021. Nice: Neural image Davchev,YuxiangZhou,AgrimGupta,AkhilRaju, commentingwithempathy. InEMNLP2021. et al. 2023. Robocat: A self-improving founda- tionagentforroboticmanipulation. arXivpreprint Kezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul arXiv:2306.11706. Smolensky, Kenneth D. Forbus, and Jianfeng Gao. 2020. Mappingnatural-languageproblemstoformal- AnthonyBrohan,NoahBrown,JusticeCarbajal,Yevgen languagesolutionsusingstructuredneuralrepresen- Chebotar,XiChen,KrzysztofChoromanski,Tianli tations. InICML2020. Ding,DannyDriess,AvinavaDubey,ChelseaFinn, et al. 2023. Rt-2: Vision-language-action models BrandonCui,AndreiLupu,SamuelSokota,Hengyuan transfer web knowledge to robotic control. arXiv Hu,DavidJWu,andJakobNicolausFoerster.2023. preprintarXiv:2307.15818. Adversarialdiversityinhanabi. InTheEleventhIn- ternationalConferenceonLearningRepresentations. AnthonyBrohan,NoahBrown,JusticeCarbajal,Yev- genChebotar,JosephDabis,ChelseaFinn,Keerthana Gautier Dagan, Frank Keller, and Alex Lascarides. Gopalakrishnan,KarolHausman,AlexHerzog,Jas- 2023. Dynamicplanningwithallm. arXivpreprint mineHsu, etal.2022. Rt-1: Roboticstransformer arXiv:2308.06391. for real-world control at scale. arXiv preprint MattDeitke,WinsonHan,AlvaroHerrasti,Aniruddha arXiv:2212.06817. Kembhavi,EricKolve,RoozbehMottaghi,JordiSal- Tom Brown, Benjamin Mann, Nick Ryder, Melanie vador, Dustin Schwenk, Eli VanderBilt, Matthew Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind Wallingford, et al. 2020. Robothor: An open Neelakantan,PranavShyam,GirishSastry,Amanda simulation-to-realembodiedaiplatform. InProceed- Askell,etal.2020. Languagemodelsarefew-shot ingsoftheIEEE/CVFconferenceoncomputervision learners. Advancesinneuralinformationprocessing andpatternrecognition,pages3164–3174. systems,33:1877–1901. Vishnu Sashank Dorbala, James F Mullen Jr, and Di- PatrickButlin,RobertLong,EricElmoznino,Yoshua neshManocha.2023. Cananembodiedagentfind Bengio, Jonathan Birch, Axel Constant, George your"cat-shapedmug"? llm-basedzero-shotobject Deane,StephenMFleming,ChrisFrith,XuJi,etal. navigation. arXivpreprintarXiv:2303.03480. 2023. Consciousness in artificial intelligence: in- sights from the science of consciousness. arXiv Vishnu Sashank Dorbala, Gunnar Sigurdsson, Robin- preprintarXiv:2308.08708. son Piramuthu, Jesse Thomason, and Gaurav S Sukhatme. 2022. Clip-nav: Using clip for zero- WenzheCai,SiyuanHuang,GuangranCheng,Yuxing shotvision-and-languagenavigation. arXivpreprint Long,PengGao,ChangyinSun,andHaoDong.2023. arXiv:2211.16649. DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch, RanGong,QiuyuanHuang,XiaojianMa,HoiVo,Zane AakankshaChowdhery,BrianIchter,AyzaanWahid, Durante, Yusuke Noda, Zilong Zheng, Song-Chun Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Zhu, Demetri Terzopoulos, Li Fei-Fei, et al. 2023. 2023. Palm-e: Anembodiedmultimodallanguage Mindagent: Emergent gaming interaction. arXiv model. arXivpreprintarXiv:2303.03378. preprintarXiv:2309.09971. Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, AnkitGoyal,JieXu,YijieGuo,ValtsBlukis,Yu-Wei AyzaanWahid,BrianIchter,PierreSermanet,Tianhe Chao,andDieterFox.2023. Rvt:Roboticviewtrans- Yu, Pieter Abbeel, Joshua B Tenenbaum, et al. former for 3d object manipulation. arXiv preprint 2023. Video language planning. arXiv preprint arXiv:2306.14896. arXiv:2310.10625. IsabelaGranic,AdamLobel,andRutgerCMEEngels. ZaneDurante,QiuyuanHuang,NaokiWake,RanGong, 2014. Thebenefitsofplayingvideogames. Ameri- JaeSungPark,BidiptaSarkar,RohanTaori,Yusuke canpsychologist,69(1):66. Noda, Demetri Terzopoulos, Yejin Choi, Katsushi Ikeuchi,HoiVo,LiFei-Fei,andJianfengGao.2024a. Liangke Gui, Qiuyuan Huang, Alex Hauptmann, Agentai: Surveyingthehorizonsofmultimodalin- YonatanBisk,andJianfengGao.2022a. Vlc: Train- teraction. arXivpreprintarXiv:2401.03568. ingvision-languagetransformersfromcaptions. Zane Durante, Bidipta Sarkar, Ran Gong, Rohan LiangkeGui,BoruiWang,QiuyuanHuang,AlexHaupt- Taori, Yusuke Noda, Paul Tang, Ehsan Adeli, mann,YonatanBisk,andJianfengGao.2022b. Kat: ShrinidhiKowshikaLakshmikanth,KevinSchulman, Aknowledgeaugmentedtransformerforvision-and- ArnoldMilstein,DemetriTerzopoulos,AdeFamoti, language. InNAACL2022.Longpaper,Oral. NoboruKuno,AshleyJ.Llorens,HoiVo,Katsushi Ikeuchi,Fei-FeiLi,JianfengGao,NaokiWake,and Rodrigo Longhi Guimarães, André Schneider QiuyuanHuang.2024b. Agentfoundationmodel. deOliveira,JoãoAlbertoFabro,ThiagoBecker,and Nouha Dziri, Andrea Madotto, Osmar Zaiane, and Vinícius Amilgar Brenner. 2016. Ros navigation: AvishekJoeyBose.2021. Neuralpathhunter: Re- Concepts and tutorial. Robot Operating System ducing hallucination in dialogue systems via path (ROS) The Complete Reference (Volume 1), pages grounding. arXivpreprintarXiv:2104.08455. 121–160. KianaEhsani,WinsonHan,AlvaroHerrasti,EliVan- KelvinGuu,KentonLee,ZoraTung,PanupongPasu- derBilt,LucaWeihs,EricKolve,AniruddhaKemb- pat,andMingweiChang.2020. Retrievalaugmented havi, andRoozbehMottaghi.2021. Manipulathor: languagemodelpre-training. InInternationalconfer- Aframeworkforvisualobjectmanipulation. InPro- enceonmachinelearning,pages3929–3938.PMLR. ceedingsoftheIEEE/CVFconferenceoncomputer visionandpatternrecognition,pages4497–4506. HuyHa,PeteFlorence,andShuranSong.2023. Scaling upanddistillingdown: Language-guidedrobotskill SamirYitzhakGadre, MitchellWortsman, GabrielIl- acquisition. arXivpreprintarXiv:2307.14535. harco, Ludwig Schmidt, and Shuran Song. 2023. Cows on pasture: Baselines and benchmarks for LauraBirkaHensel,NutchanonYongsatianchot,Parisa language-drivenzero-shotobjectnavigation. InPro- Torshizi,ElenaMinucci,andStacyMarsella.2023. ceedingsoftheIEEE/CVFConferenceonComputer Largelanguagemodelsintextualanalysisforgesture VisionandPatternRecognition,pages23171–23181. selection. InINTERNATIONALCONFERENCEON MULTIMODALINTERACTION,pages378–387. JianfengGao,BaolinPeng,ChunyuanLi,JinchaoLi, Shahin Shayandeh, Lars Liden, and Heung-Yeung DanielHo,KanishkaRao,ZhuoXu,EricJang,Mohi Shum.2020. Robustconversationalaiwithgrounded Khansari, and Yunfei Bai. 2021. Retinagan: An textgeneration. arXivpreprintarXiv:2009.03457. object-aware approach to sim-to-real transfer. In 2021 IEEE International Conference on Robotics Jianfeng Gao, Chenyan Xiong, Paul Bennett, and andAutomation(ICRA),pages10920–10926.IEEE. Nick Craswell. 2022. Neural approaches to con- versational information retrieval. arXiv preprint ChenguangHuang, OierMees, AndyZeng, andWol- arXiv:2201.05176. framBurgard.2023a. Visuallanguagemapsforrobot CaelanReedGarrett,RohanChitnis,RachelHolladay, navigation. In2023IEEEInternationalConference BeomjoonKim,TomSilver,LesliePackKaelbling, onRoboticsandAutomation(ICRA),pages10608– andTomásLozano-Pérez.2021. Integratedtaskand 10615.IEEE. motionplanning. Annualreviewofcontrol,robotics, andautonomoussystems,4:265–293. QiuyuanHuang,JaeSungPark,AbhinavGupta,Paul Bennett, Ran Gong, Subhojit Som, Baolin Peng, TheophileGervet,SoumithChintala,DhruvBatra,Jiten- Owais Khan Mohammed, Chris Pal, Yejin Choi, draMalik,andDevendraSinghChaplot.2023. Navi- et al. 2023b. Ark: Augmented reality with knowl- gatingtoobjectsintherealworld. ScienceRobotics, edge interactive emergent ability. arXiv preprint 8(79):eadf6991. arXiv:2305.00970. Qiuyuan Huang, Pengchuan Zhang, Oliver Wu, and RanjayKrishna,YukeZhu,OliverGroth,JustinJohn- LeiZhang.2018. Turbolearningforcaptionbotand son, Kenji Hata, Joshua Kravitz, Stephanie Chen, drawingbot. InNeurIPS2018. Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. 2016. Vi- Wenlong Huang, Pieter Abbeel, Deepak Pathak, and sual genome: Connecting language and vision us- Igor Mordatch. 2022a. Language models as zero- ing crowdsourced dense image annotations. In shotplanners: Extractingactionableknowledgefor arXiv:1602.07332. embodiedagents. InProceedingsofthe39thInter- nationalConferenceonMachineLearning,volume KNiranjanKumar, IrfanEssa, andSehoonHa.2023. 162ofProceedingsofMachineLearningResearch, Wordsintoaction: Learningdiversehumanoidrobot pages9118–9147.PMLR. behaviorsusinglanguageguidediterativemotionre- finement. arXivpreprintarXiv:2310.06226. WenlongHuang,FeiXia,TedXiao,HarrisChan,Jacky Liang, PeteFlorence, AndyZeng, JonathanTomp- PeterLee,SebastienBubeck,andJosephPetro.2023. son, Igor Mordatch, Yevgen Chebotar, Pierre Ser- Benefits, limits, andrisksofgpt-4asanaichatbot manet, Noah Brown, Tomas Jackson, Linda Luu, for medicine. New England Journal of Medicine, Sergey Levine, Karol Hausman, and Brian Ichter. 388(13):1233–1239. 2022b. Inner monologue: Embodied reasoning through planning with language models. In arXiv PatrickLewis,EthanPerez,AleksandraPiktus,Fabio preprintarXiv:2207.05608. Petroni,VladimirKarpukhin,NamanGoyal,Hein- richKüttler, MikeLewis, Wen-tauYih, TimRock- DFCIntelligence.2020. Globalvideogameaudience täschel,etal.2020. Retrieval-augmentedgeneration reaches3.7billion. https://www.dfcint.com/glo forknowledge-intensivenlptasks. InNeurIPS. bal-video-game-audience-reaches-3-7-bil lion/. Accessed: 2024-02-05. BoyiLi,PhilippWu,PieterAbbeel,andJitendraMa- lik.2023a. Interactivetaskplanningwithlanguage Stephen James and Andrew J Davison. 2022. Q- models. arXivpreprintarXiv:2310.10645. attention:Enablingefficientlearningforvision-based roboticmanipulation. IEEERoboticsandAutoma- ChengshuLi,FeiXia,RobertoMartín-Martín,Michael tionLetters,7(2):1612–1619. Lingelbach, Sanjana Srivastava, Bokui Shen, Kent Vainio, Cem Gokmen, Gokul Dharan, Tanish Jain, EricJang,AlexIrpan,MohiKhansari,DanielKappler, etal.2021. igibson2.0:Object-centricsimulationfor Frederik Ebert, Corey Lynch, Sergey Levine, and robotlearningofeverydayhouseholdtasks. arXiv ChelseaFinn.2022. Bc-z: Zero-shottaskgeneraliza- preprintarXiv:2108.03272. tionwithroboticimitationlearning. InConference onRobotLearning,pages991–1002.PMLR. Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. ZiweiJi,NayeonLee,RitaFrieske,TiezhengYu,Dan 2023b. Camel: Communicative agents for" mind" Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea exploration of large scale language model society. Madotto,andPascaleFung.2023. Surveyofhalluci- arXivpreprintarXiv:2303.17760. nationinnaturallanguagegeneration. ACMComput- ingSurveys,55(12):1–38. Jiachen Li, Qiaozi Gao, Michael Johnston, Xiaofeng Gao,XuehaiHe,SuhailaShakiah,HangjieShi,Reza Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Ghanadan, and William Yang Wang. 2023c. Mas- Wang,YongqiangDou,YanjunChen,LiFei-Fei,An- teringrobotmanipulationwithmultimodalprompts ima Anandkumar, Yuke Zhu, and Linxi Fan. 2022. throughpretrainingandmulti-taskfine-tuning. arXiv Vima: Generalrobotmanipulationwithmultimodal preprintarXiv:2310.09676. prompts. arXiv. AndrejKarpathy,ArmandJoulin,andLiFFei-Fei.2014. JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Deepfragmentembeddingsforbidirectionalimage 2023d. Blip-2: Bootstrappinglanguage-imagepre- sentencemapping. Advancesinneuralinformation training with frozen image encoders and large lan- processingsystems,27. guagemodels. arXivpreprintarXiv:2301.12597. PushkalKatara,ZhouXian,andKaterinaFragkiadaki. KunChang Li, Yinan He, Wang Yi, Yizhuo Li, Wen- 2023. Gen2sim: Scaling up robot learning in sim- haiWang,PingLuo,YaliWang,LiminWang,and ulation with generative models. arXiv preprint YuQiao.2023e. Videochat: Chat-centricvideoun- arXiv:2310.18308. derstanding. arXivpreprintarXiv:2305.06355. EricKolve,RoozbehMottaghi,WinsonHan,EliVan- Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun derBilt, LucaWeihs, AlvaroHerrasti, MattDeitke, Yu,JieXu,HongtaoWu,ChilamCheang,YaJing, KianaEhsani,DanielGordon,YukeZhu,etal.2017. Weinan Zhang, Huaping Liu, et al. 2023f. Vision- Ai2-thor: Aninteractive3denvironmentforvisualai. languagefoundationmodelsaseffectiverobotimita- arXivpreprintarXiv:1712.05474. tors. arXivpreprintarXiv:2311.01378. JackyLiang,WenlongHuang,FeiXia,PengXu,Karol HengyuanHu,etal.2022. Human-levelplayinthe Hausman, Brian Ichter, Pete Florence, and Andy gameofDiplomacybycombininglanguagemodels Zeng. 2022. Code as policies: Language model withstrategicreasoning. Science,378(6624):1067– programs for embodied control. In arXiv preprint 1074. arXiv:2209.07753. Mayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu, XiwenLiang,LiangMa,ShanshanGuo,JianhuaHan, Nikita Rudin, David Hoeller, Jia Lin Yuan, Ritvik Hang Xu, Shikui Ma, and Xiaodan Liang. 2023. Singh,YunrongGuo,HammadMazhar,etal.2023. Mo-vln: Amulti-taskbenchmarkforopen-setzero- Orbit:Aunifiedsimulationframeworkforinteractive shotvision-and-languagenavigation. arXivpreprint robot learning environments. IEEE Robotics and arXiv:2306.10322. AutomationLetters. Tsung-YiLin,MichaelMaire,SergeBelongie,Lubomir Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Bourdev,RossGirshick,JamesHays,PietroPerona, Andrei A Rusu, Joel Veness, Marc G Bellemare, DevaRamanan,C.LawrenceZitnick,andPiotrDol- Alex Graves, Martin Riedmiller, Andreas K Fidje- lár.2014. Microsoftcoco: Commonobjectsincon- land, Georg Ostrovski, et al. 2015. Human-level text. ProceedingsofECCV. controlthroughdeepreinforcementlearning. nature, 518(7540):529–533. CKarenLiuandDanNegrut.2021. Theroleofphysics- basedsimulatorsinrobotics. AnnualReviewofCon- Matthias Müller, Vincent Casser, Jean Lahoud, Neil trol,Robotics,andAutonomousSystems,4:35–58. Smith, and Bernard Ghanem. 2018. Sim4cv: A photo-realisticsimulatorforcomputervisionappli- YechengJasonMa,WilliamLiang,GuanzhiWang,De- cations. InternationalJournalofComputerVision, AnHuang,OsbertBastani,DineshJayaraman,Yuke 126:902–919. Zhu,LinxiFan,andAnimaAnandkumar.2023. Eu- reka: Human-level reward design via coding large ZheNi, Xiao-XinDeng, CongTai, Xin-YueZhu, Xi- languagemodels. arXivpreprintarXiv:2310.12931. angWu,Yong-JinLiu,andLongZeng.2023. Grid: Scene-graph-based instruction-driven robotic task Muhammad Maaz, Hanoona Rasheed, Salman Khan, planning. arXivpreprintarXiv:2309.07726. andFahadShahbazKhan.2023. Video-chatgpt: To- wardsdetailedvideounderstandingvialargevision OpenAI.2023. GPT-4technicalreport. Technicalre- andlanguagemodels. port,OpenAI. RuiMao,QianLiu,KaiHe,WeiLi,andErikCambria. LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida, 2022. The biases of pre-trained language models: CarrollWainwright,PamelaMishkin,ChongZhang, Anempiricalstudyonprompt-basedsentimentanal- SandhiniAgarwal,KatarinaSlama,AlexRay,etal. ysisandemotiondetection. IEEETransactionson 2022. Training languagemodelsto followinstruc- AffectiveComputing. tions with human feedback. Advances in Neural InformationProcessingSystems,35:27730–27744. Gary Marcus and Ernest Davis. 2019. Rebooting AI: Building artificial intelligence we can trust. Pan- MeenalParakh,AlishaFong,AnthonySimeonov,Ab- theon. hishekGupta,TaoChen,andPulkitAgrawal.2023. Pablo Martinez-Gonzalez, Sergiu Oprea, Alberto Human-assistedcontinualrobotlearningwithfoun- Garcia-Garcia, Alvaro Jover-Alvarez, Sergio Orts- dationmodels. arXivpreprintarXiv:2309.14321. Escolano, and Jose Garcia-Rodriguez. 2020. Un- JaeSungPark,JackHessel,KhyathiChandu,PaulPu realrox: an extremely photorealistic virtual reality Liang,XimingLu,PeterWest,QiuyuanHuang,Jian- environmentforroboticssimulationsandsynthetic fengGao,AliFarhadi,andYejinChoi.2023a. Multi- datageneration. VirtualReality,24:271–288. modalagent–localizedsymbolicknowledgedistil- Joshua Maynez, Shashi Narayan, Bernd Bohnet, and lationforvisualcommonsensemodels. InNeurIPS Ryan McDonald. 2020. On faithfulness and factu- 2023. alityinabstractivesummarization. InProceedings of the 58th Annual Meeting of the Association for JaeSungPark,JackHessel,KhyathiChandu,PaulPu Computational Linguistics, pages 1906–1919, On- Liang, Ximing Lu, Peter West, Youngjae Yu, Qi- line.AssociationforComputationalLinguistics. uyuanHuang,JianfengGao,AliFarhadi,andYejin Choi.2023b. Localizedsymbolicknowledgedistil- Oier Mees, Lukas Hermann, Erick Rosete-Beas, and lation for visual commonsense models. In Thirty- Wolfram Burgard. 2022. Calvin: A benchmark seventhConferenceonNeuralInformationProcess- for language-conditioned policy learning for long- ingSystems. horizon robot manipulation tasks. IEEE Robotics andAutomationLetters,7(3):7327–7334. JoonSungPark,JosephCO’Brien,CarrieJCai,Mered- ith Ringel Morris, Percy Liang, and Michael S MetaFundamentalAIResearch,AntonBakhtin,Noam Bernstein. 2023c. Generative agents: Interac- Brown, Emily Dinan, Gabriele Farina, Colin Fla- tive simulacra of human behavior. arXiv preprint herty, Daniel Fried, Andrew Goff, Jonathan Gray, arXiv:2304.03442. BaolinPeng,MichelGalley,PengchengHe,HaoCheng, Daichi Saito, Kazuhiro Sasabuchi, Naoki Wake, Jun YujiaXie,YuHu,QiuyuanHuang,LarsLiden,Zhou Takamatsu, Hideki Koike, and Katsushi Ikeuchi. Yu,WeizhuChen,etal.2023. Checkyourfactsand 2022. Task-grasping from a demonstrated human try again: Improving large language models with strategy. In2022IEEE-RAS21stInternationalCon- externalknowledgeandautomatedfeedback. arXiv ference on Humanoid Robots (Humanoids), pages preprintarXiv:2302.12813. 880–887. Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, BidiptaSarkar,AndyShih,andDorsaSadigh.2023. Di- Tingwu Wang, Sanja Fidler, and Antonio Torralba. verse conventions for human-AI collaboration. In 2018. Virtualhome: Simulatinghouseholdactivities Thirty-seventh Conference on Neural Information viaprograms. In2018IEEEInternationalConfer- ProcessingSystems. ence on Computer Vision and Pattern Recognition (CVPR),pages8494–8502. Kazuhiro Sasabuchi, Daichi Saito, Atsushi Kanehira, NaokiWake,JunTakamatsu,andKatsushiIkeuchi. Xavier Puig, Eric Undersander, Andrew Szot, 2023. Task-sequencing simulator: Integrated ma- MikaelDallaireCote,Tsung-YenYang,RuslanPart- chinelearningtoexecutionsimulationforrobotma- sey,RutaDesai,AlexanderWilliamClegg,Michal nipulation. arXivpreprintarXiv:2301.01382. Hlavac, So Yeon Min, et al. 2023. Habitat 3.0: A co-habitat for humans, avatars and robots. arXiv preprintarXiv:2310.13724. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana SanthoshKumarRamakrishnan,DevendraSinghChap- Jain,JulianStraub,JiaLiu,VladlenKoltun,Jitendra lot,ZiadAl-Halah,JitendraMalik,andKristenGrau- Malik,etal.2019. Habitat: Aplatformforembodied man.2022. Poni: Potentialfunctionsforobjectgoal ai research. In Proceedings of the IEEE/CVF navigation with interaction-free learning. In Pro- internationalconferenceoncomputervision,pages ceedingsoftheIEEE/CVFConferenceonComputer 9339–9347. VisionandPatternRecognition,pages18890–18900. JohnSchulman,FilipWolski,PrafullaDhariwal,Alec ShreyasSundaraRaman,VanyaCohen,DavidPaulius, Radford,andOlegKlimov.2017. Proximalpolicy IfrahIdrees,EricRosen,RayMooney,andStefanie optimizationalgorithms. Tellex.2023. Cape: Correctiveactionsfromprecon- dition errors using large language models. In 2nd BrennanShacklett,LucGuyRosenzweig,ZhiqiangXie, Workshop on Language and Robot Learning: Lan- BidiptaSarkar,AndrewSzot,ErikWijmans,Vladlen guageasGrounding. Koltun,DhruvBatra,andKayvonFatahalian.2023. An extensible, data-oriented architecture for high- KanishkaRao,ChrisHarris,AlexIrpan,SergeyLevine, performance,many-worldsimulation. ACMTrans. JulianIbarz,andMohiKhansari.2020. Rl-cyclegan: Graph.,42(4). Reinforcementlearningawaresimulation-to-real. In ProceedingsoftheIEEE/CVFConferenceonCom- DhruvShah,Błaz˙ejOsin´ski,SergeyLevine,etal.2023a. puterVisionandPatternRecognition,pages11157– Lm-nav: Roboticnavigationwithlargepre-trained 11166. modelsoflanguage, vision, andaction. InConfer- Vikas Raunak, Arul Menezes, and Marcin Junczys- enceonRobotLearning,pages492–504.PMLR. Dowmunt. 2021. The curious case of hallucina- tionsinneuralmachinetranslation. arXivpreprint Rutav Shah, Roberto Martín-Martín, and Yuke Zhu. arXiv:2104.06683. 2023b. Mutex: Learning unified policies from multimodal task specifications. arXiv preprint Mengye Ren, Ryan Kiros, and Richard Zemel. 2015. arXiv:2309.14320. Exploring models and data for image question an- swering. Advancesinneuralinformationprocessing ShitalShah,DebadeeptaDey,ChrisLovett,andAshish systems,28. Kapoor.2018. Airsim: High-fidelityvisualandphys- icalsimulationforautonomousvehicles. InFieldand AnnaRohrbach,LisaAnneHendricks,KayleeBurns, ServiceRobotics: Resultsofthe11thInternational Trevor Darrell, and Kate Saenko. 2018. Object Conference,pages621–635.Springer. hallucination in image captioning. arXiv preprint arXiv:1809.02156. Piyush Sharma, Nan Ding, Sebastian Goodman, and RaduSoricut.2018. Conceptualcaptions: Acleaned, Antoni Rosinol, John J Leonard, and Luca Carlone. hypernymed,imagealt-textdatasetforautomaticim- 2022. Nerf-slam: Real-time dense monocular agecaptioning. Proceedingsofthe56thAnnualMeet- slam with neural radiance fields. arXiv preprint ingoftheAssociationforComputationalLinguistics. arXiv:2210.13641. Daichi Saito, Kazuhiro Sasabuchi, Naoki Wake, At- MohitShridhar,LucasManuelli,andDieterFox.2023. sushiKanehira,JunTakamatsu,HidekiKoike,and Perceiver-actor: Amulti-tasktransformerforrobotic KatsushiIkeuchi.2023. Constraint-awarepolicyfor manipulation. In Conference on Robot Learning, compliantmanipulation. pages785–799.PMLR. KurtShuster,SpencerPoff,MoyaChen,DouweKiela, Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, and Jason Weston. 2021. Retrieval augmentation JunTakamatsu,andKatsushiIkeuchi.2023a. Bias reduceshallucinationinconversation. arXivpreprint inemotionrecognitionwithchatgpt. arXivpreprint arXiv:2104.07567. arXiv:2310.11753. Amanpreet Singh, Vivek Natarajan, Meet Shah, Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, JunTakamatsu,andKatsushiIkeuchi.2023b. Chat- andMarcusRohrbach.2019. Towardsvqamodels gpt empowered long-step robot control in various thatcanread. InProceedingsoftheIEEE/CVFcon- environments: A case application. IEEE Access, ferenceoncomputervisionandpatternrecognition, 11:95060–95078. pages8317–8326. Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Sumedh A Sontakke, Jesse Zhang, Sébastien MR JunTakamatsu, andKatsushiIkeuchi.2023c. Gpt- Arnold, Karl Pertsch, Erdem Bıyık, Dorsa Sadigh, 4v(ision) for robotics: Multimodal task plan- Chelsea Finn, and Laurent Itti. 2023. Roboclip: ning from human demonstration. arXiv preprint Onedemonstrationisenoughtolearnrobotpolicies. arXiv:2311.12015. arXivpreprintarXiv:2310.07899. SanjanaSrivastava,ChengshuLi,MichaelLingelbach, Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, RobertoMartín-Martín,FeiXia,KentElliottVainio, Jun Takamatsu, and Katsushi Ikeuchi. 2023d. Gpt Zheng Lian, Cem Gokmen, Shyamal Buch, Karen modelsmeetroboticapplications: Co-speechgestur- Liu, et al. 2022. Behavior: Benchmark for every- ingchatsystem. arXivpreprintarXiv:2306.01741. dayhouseholdactivitiesinvirtual, interactive, and ecological environments. In Conference on Robot BoruiWang,QiuyuanHuang,BudhadityaDeb,AaronL. Learning,pages477–490.PMLR. Halfaker, Liqun Shao, Daniel McDuff, Ahmed Awadallah, Dragomir Radev, and Jianfeng Gao. Andrew Szot, Alex Clegg, Eric Undersander, Erik 2023a. Logicaltransformers: Infusinglogicalstruc- Wijmans, Yili Zhao, John Turner, Noah Maestre, turesintopre-trainedlanguagemodels. InProceed- Mustafa Mukadam, Devendra Chaplot, Oleksandr ingsofACL2023. Maksymets, Aaron Gokaslan, Vladimir Vondrus, SameerDharur,FranziskaMeier,WojciechGaluba, Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man- AngelChang,ZsoltKira,VladlenKoltun,Jitendra dlekar,ChaoweiXiao,YukeZhu,LinxiFan,andAn- Malik,ManolisSavva,andDhruvBatra.2021. Habi- imaAnandkumar.2023b. Voyager: Anopen-ended tat2.0: Traininghomeassistantstorearrangetheir embodiedagentwithlargelanguagemodels. arXiv habitat. InAdvancesinNeuralInformationProcess- preprintarXiv:2305.16291. ingSystems(NeurIPS). Hitoshi Teshima, Naoki Wake, Diego Thomas, Yuta XinWang,QiuyuanHuang,AsliCelikyilmaz,Jianfeng Nakashima,HiroshiKawasaki,andKatsushiIkeuchi. Gao,DinghanShen,Yuan-FangWeng,WilliamYang 2022. Deepgesturegenerationforsocialrobotsusing Wang,andLeiZhang.2019. Reinforcedcross-modal type-specific libraries. In 2022 IEEE/RSJ Interna- matchingandself-supervisedimitationlearningfor tionalConferenceonIntelligentRobotsandSystems vision-languagenavigation. InCVPR2019. (IROS),pages8286–8291.IEEE. YiWang,YinanHe,YizhuoLi,KunchangLi,Jiashuo JoshTobin, RachelFong, AlexRay, JonasSchneider, Yu,XinMa,XinyuanChen,YaohuiWang,PingLuo, Wojciech Zaremba, and Pieter Abbeel. 2017. Do- Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. mainrandomizationfortransferringdeepneuralnet- 2023c. Internvid: Alarge-scalevideo-textdatasetfor works from simulation to the real world. In 2017 multimodalunderstandingandgeneration. IEEE/RSJ international conference on intelligent robotsandsystems(IROS),pages23–30.IEEE. YufeiWang,ZhouXian,FengChen,Tsun-HsuanWang, YianWang,KaterinaFragkiadaki,ZackoryErickson, Nathan Tsoi, Alec Xiang, Peter Yu, Samuel S Sohn, David Held, and Chuang Gan. 2023d. Robogen: Greg Schwartz, Subashri Ramesh, Mohamed Hus- Towardsunleashinginfinitedataforautomatedrobot sein, Anjali W Gupta, Mubbasir Kapadia, and learning via generative simulation. arXiv preprint Marynel Vázquez. 2022. Sean 2.0: Formalizing arXiv:2311.01455. andgeneratingsocialsituationsforrobotnavigation. IEEERoboticsandAutomationLetters,7(4):11047– ZihaoWang,ShaofeiCai,AnjiLiu,XiaojianMa,and 11054. YitaoLiang.2023e. Describe,explain,planandse- Naoki Wake, Riku Arakawa, Iori Yanokura, Takuya lect: Interactiveplanningwithlargelanguagemodels Kiyokawa,KazuhiroSasabuchi,JunTakamatsu,and enablesopen-worldmulti-taskagents. arXivpreprint KatsushiIkeuchi.2021. Alearning-from-observation arXiv:2302.01560. framework: One-shot robot teaching for grasp- manipulation-releasehouseholdoperations. In2021 P. H. Winston. 1972. The m.i.t. robot. In D. Michie, IEEE/SICEInternationalSymposiumonSystemInte- editor,MachineIntelligence7.EdinburghUniversity gration(SII).IEEE. Press,Edinburgh,Scotland. Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Gengze Zhou, Yicong Hong, and Qi Wu. 2023a. Ziyue Wang, Chencheng Jiang, Haoran Tan, Ji- Navgpt: Explicitreasoninginvision-and-language amu Kang, Yuanhan Zhang, Kaiyang Zhou, et al. navigation with large language models. arXiv 2023a. Octopus: Embodied vision-language pro- preprintarXiv:2305.16986. grammer from environmental feedback. arXiv preprintarXiv:2310.08588. HaoyuZhou,MingyuDing,WeikunPeng,Masayoshi Tomizuka,LinShao,andChuangGan.2023b. Gen- KailaiYang,ShaoxiongJi,TianlinZhang,QianqianXie, eralizablelong-horizonmanipulationswithlargelan- andSophiaAnaniadou.2023b. Ontheevaluationsof guagemodels. arXivpreprintarXiv:2310.02264. chatgptandemotion-enhancedpromptingformental healthanalysis. arXivpreprintarXiv:2304.03347. Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang,ZhunDeng,ChelseaFinn,MohitBansal,and Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak HuaxiuYao.2023c. Analyzingandmitigatingobject Shafran,KarthikNarasimhan,andYuanCao.2023. hallucinationinlargevision-languagemodels. arXiv React: Synergizingreasoningandactinginlanguage preprintarXiv:2310.00754. models. DeyaoZhu, JunChen, XiaoqianShen, XiangLi, and Yang Ye, Hengxu You, and Jing Du. 2023. Im- MohamedElhoseiny.2023. Minigpt-4: Enhancing provedtrustinhuman-robotcollaborationwithchat- vision-languageunderstandingwithadvancedlarge gpt. IEEEAccess. languagemodels. Jun-YanZhu,TaesungPark,PhillipIsola,andAlexeiA PeterYoung,AliceLai,MicahHodosh,andJuliaHock- Efros.2017a. Unpairedimage-to-imagetranslation enmaier. 2014. From image descriptions to visual usingcycle-consistentadversarialnetworks. InPro- denotations: Newsimilaritymetricsforsemanticin- ceedings of the IEEE international conference on ferenceovereventdescriptions. Proceedingsofthe computervision,pages2223–2232. AnnualMeetingoftheAssociationforComputational Linguistics. ShaojunZhu,AndrewKimmel,KostasEBekris,and Abdeslam Boularias. 2017b. Fast model identifi- LichengYu,PatrickPoirson,ShanYang,AlexanderC cation via physics engines for data-efficient policy Berg,andTamaraLBerg.2016. Modelingcontext search. arXivpreprintarXiv:1710.08893. inreferringexpressions. InComputerVision–ECCV 2016: 14thEuropeanConference,Amsterdam,The Netherlands,October11-14,2016,Proceedings,Part II14,pages69–85.Springer. Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kir- mani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-TienLewisChiang,TomErez,LeonardHasen- clever, Jan Humplik, et al. 2023. Language to re- wards for robotic skill synthesis. arXiv preprint arXiv:2306.08647. AndyZeng,PeteFlorence,JonathanTompson,Stefan Welker,JonathanChien,MariaAttarian,TravisArm- strong,IvanKrasin,DanDuong,VikasSindhwani, etal.2021. Transporternetworks: Rearrangingthe visualworldforroboticmanipulation. InConference onRobotLearning,pages726–747.PMLR. Sixian Zhang, Xinhang Song, Yubing Bai, Weijie Li, YakuiChu,andShuqiangJiang.2021. Hierarchical object-to-zonegraphforobjectnavigation. InPro- ceedingsoftheIEEE/CVFinternationalconference oncomputervision,pages15130–15140. WeixiangZhao,YanyanZhao,XinLu,ShilongWang, Yanpeng Tong, and Bing Qin. 2023. Is chat- gptequippedwithemotionaldialoguecapabilities? arXivpreprintarXiv:2304.09582. ZhideZhong,JiakaiCao,SongenGu,SiruiXie,Weibo Gao, Liyi Luo, Zike Yan, Hao Zhao, and Guyue Zhou.2023. Assist: Interactivescenenodesforscal- ableandrealisticindoorsimulation. arXivpreprint arXiv:2311.06211. Appendices for Agent AI Towards a Holistic Intelligence A IntentionInformationand largeactionmodelsforGeneralPurposeRobotics. ManipulationforEmbodiedAction Optimizeandvalidatethealgorithmsinreal-world scenarioswithlargeactionmodelsonPhoenix. Ex- LanguageConditionedinternetactioninstruction ploreapplicationsindiversedomains,ensuringro- entails the ability of a robotic system to inter- bustnessandscalability. Refinealgorithmsbased pretandexecutetasksbasedonlanguageinstruc- on real-world evaluation feedback and scale for tions. Thisaspectisparticularlycrucialforcreating broaderclouddeploymentintheembodiedsystem. intuitive and user-friendly interfaces for human- robotinteraction. Throughnaturallanguagecom- B AgentforCross-modalityand mands,userscanspecifygoalsandtaskstorobots Mix-reality in a manner similar to human-human communi- B.1 AgentsforCross-modalUnderstanding cation (Wang et al., 2019), thereby lowering the barrier to operating robotic systems. In a practi- Multi-modal understanding is a significant chal- cal scenario, for instance, a user could instruct a lengeforcreatinggeneralistAIagentsduetothe service robot to “pick up the red apple from the lackoflarge-scaledatasetsthatcontainvision,lan- table,”andtherobotwouldparsethisinstruction, guage,andagentbehavior. Moregenerally,training identifythereferredobjectandexecutethetaskof dataforAIagentsisoftenmodalityspecific. This pickingitup(Wakeetal.,2023b). Thecorechal- resultsinmostmodernmulti-modalsystemsusing lenge lies in developing robust natural language a combination of frozen submodules. Some no- processingandunderstandingalgorithmsthatcan tableexamplesareFlamingo(Alayracetal.,2022), accurately interpret a wide array of instructions, BLIP-2(Lietal.,2023d),VLC(Guietal.,2022a) ranging from direct commands to more abstract andArK(Huangetal.,2023b),allofwhichutilize directives, and enable the robot to convert these a frozen LLM and frozen visual encoder. These instructionsintoactionabletasks. Furthermore,en- submodules are trained individually on separate suringthatrobotscangeneralizetheseinstructions datasets,andthenadaptationlayersaretrainedto across diverse tasks and environments is critical encode the visual encoder into the LLM embed- for enhancing their versatility and utility in real- dingspace. Inordertomakefurtherprogressfor world applications. The use of language input to cross-modalunderstandingforAIagents,itislikely guiderobot’staskplanninghasgainedattentionin thatthestrategyofusingfrozenLLMsandvisual thecontextofarobotframeworkcalledTaskand encoders will need to change. Indeed, RT-2, a MotionPlanning(Garrettetal.,2021). recentvisual-languagemodelthatiscapableoftak- Inaddition, (Duranteetal.,2024b)learnabout ingactionswithinthedomainofroboticsshowed theintricatechallengesoflargeactionmodelsfor significantlyimprovedperformancewhenjointly embodied systems e.g., robotic. It begin with a tuning the visual encoder and LLM for robotics low-level action manipulation foundational mod- andvisual-languagetasks(Brohanetal.,2023). els, it explore solutions to issues such as action B.2 AgentsforCross-domainUnderstanding resignation,adaptabilitytodynamicenvironments, andtheefficientmanagementofhigh-dimensional Akeychallengeforcreatinggeneralistagentsisthe actionspaces. Whentransfertonextphase,weim- distinctivevisualappearanceanddisparateaction plementandrefinealgorithms,ensuringscalability spacesacrossdifferentdomains. Humanspossess andeffectivenessinsimulationsonourserver. De- thecapabilitytointerpretimagesandvideosfrom velopandimplementfoundationalalgorithmsfor various sources, including the real world, video large action models, emphasizing efficiency and games, and specialized domains such as robotics scalability. Focus on addressing issues related to andhealthcare (Duranteetal.,2024a),oncethey pre-training,fine-tuning,andmodeloptimization. becomefamiliarwiththespecificdetailsofthese Conduct initial simulations on Azure to validate areas. However, existing LLMs and VLMs often algorithmicconcepts. Theultimateobjectiveinthe demonstrate significant differences between the thirdphase,istooptimizeandvalidatethesealgo- datatheyweretrainedonandthevarieddomains rithms in real-world scenarios, exploring diverse in which they are applied. And notably, training applications and contributing to the evolution of agentmodelstopredictspecificactionspresentsa considerable challenge when trying to develop a agentthatisrespectfulandaccessibletoallusers, singlepolicythatcaneffectivelylearnmultiplecon- regardlessoftheirbackgroundoridentity. trolsystemsacrossdomains(Huangetal.,2023b). D Hallucinations Generally,theapproachmostmodernworkstake whenapplyingsystemswithinspecificdomainsis Agentsthatgeneratetextareoftenpronetohalluci- to start from a pretrained foundation model and nations, whichareinstanceswherethegenerated then finetune a separate model for each specific text is nonsensical or unfaithful to the provided domain. This fails to capture any commonalities sourcecontent(Raunaketal.,2021;Maynezetal., betweendomainsandresultsinasmallertotalset 2020). Hallucinations can be split into two cate- ofdatausedfortraininginsteadofleveragingeach gories,intrinsicandextrinsic(Jietal.,2023). In- domain’sdata. trinsic hallucinations are hallucinations that are contradictory to the source material, whereas ex- B.3 Interactiveagentforcross-modalityand trinsichallucinationsarewhenthegeneratedtext cross-reality containsadditionalinformationthatwasnotorigi- DevelopingAIagentsthatcansuccessfullyunder- nallyincludedinthesourcematerial. stand and perform tasks across different realities Somepromisingroutesforreducingtherateof isanon-goingchallengethathasseensomerecent hallucination in language generation involve us- success for image and scene generation (Huang ing retrieval-augmented generation (Lewis et al., et al., 2023b). In particular, it is challenging for 2020; Shuster et al., 2021) or other methods for agents to simultaneously understand real-world grounding natural language outputs via external andvirtualrealityenvironmentsduetotheirvisual knowledgeretrieval(Dzirietal.,2021;Pengetal., dissimilarities and separate environment physics. 2023). Generally,thesemethodsseektoaugment Within the context of cross-reality, Sim to Real languagegenerationbyretrievingadditionalsource transferisaparticularlyimportantproblemwhen materialandbyprovidingmechanismstocheckfor using simulation-trained policies for real-world contradictionsbetweenthegeneratedresponseand data,whichwediscussinthenextsection. thesourcematerial. Withinthecontextofmulti-modalagentsystems, C Bias have multimodality been shown to hallucinate as well (Zhou et al., 2023c). One common cause of AIagentsbasedonLLMsorLMMs(largemulti- hallucinationforvision-basedlanguage-generation modalmodels)havebiasesduetoseveralfactors isduetotheover-relianceonco-occurrenceofob- inherentintheirdesignandtrainingprocess. When jectsandvisualcuesinthetrainingdata(Rohrbach designingtheseAIagents,wemustbemindfulof etal.,2018). AIagentsthatexclusivelyrelyupon beinginclusiveandawareoftheneedsofallend pretrained large foundation models and use lim- usersandstakeholders. InthecontextofAIagents, itedenvironment-specificfinetuningcanbepartic- inclusivity refers to the measures and principles ularlyvulnerabletohallucinationssincetheyrely employedtoensurethattheagent’sresponsesand upontheinternalknowledge-baseofthepretrained interactionsareinclusive,respectful,andsensitive models for generating actions and may not accu- toawiderangeofusersfromdiversebackgrounds. ratelyunderstandthedynamicsoftheworldstate Despitethesemeasures,AIagentsstillexhibitbi- inwhichtheyaredeployed. ases. Ongoing efforts in agent AI research and developmentarefocusedonfurtherreducingthese biases and enhancing the inclusivity and fairness of agent AI systems. Despite these measures, AI agentsstillexhibitbiases. Ongoingeffortsinagent AIresearchanddevelopmentarefocusedonfurther reducingthesebiasesandenhancingtheinclusivity and fairness of agent AI systems. Despite these efforts,it’simportanttobeawareofthepotential forbiasesinresponsesandtointerpretthemwith criticalthinking. ContinuousimprovementsinAI agent technology and ethical practices aim to re- ducethesebiasesovertime. Oneoftheoverarch- inggoalsforinclusivityinagentAIistocreatean